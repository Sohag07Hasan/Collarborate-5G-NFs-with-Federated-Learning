{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424c0338-ba52-45c7-b636-7c05ff16352d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "from dataloader import get_evaluation_datasets_by_client  # Assuming this function gets local client datasets\n",
    "from model import Net\n",
    "from collections import OrderedDict\n",
    "from config import NUM_CLASSES, NUM_CLIENTS, GLOBAL_MODEL_PATH, BATCH_SIZE, NUM_FEATURES\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import to_tensor\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef994e1c-6907-4e54-8e2a-e623db313884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b96ad0a5-4b91-474f-97ef-a0f36ceacc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sources = {\n",
    "    'components': [12, 13, 14, 15, 16],\n",
    "    'folds': [1, 2, 3, 4, 5],\n",
    "    #'folds': [1, 2],\n",
    "    'marker': ['o', '-', '^' 'x', '-o-'],\n",
    "    'clients': [1, 2, 3, 4],\n",
    "    'path': './results/original_{0}_fold_{1}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd56603-bffb-4dee-8e02-0c1b05e790ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5de8f59b-1122-4aab-aec3-84239c5baf44",
   "metadata": {},
   "source": [
    "## 1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a5a2eaf-1391-4c5e-a4cb-ebc40517757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the global model from the saved path\n",
    "def load_model(model_path=GLOBAL_MODEL_PATH, input_size=NUM_FEATURES, num_classes=NUM_CLASSES):\n",
    "    model = Net(input_size=input_size, num_classes=num_classes)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e4f19d-02cd-42b4-a9ca-97dadfb76ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on a client's dataset\n",
    "def run_inference(model, dataloader, device):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Get the total number of samples from the DataLoader\n",
    "    total_samples = len(dataloader.dataset)\n",
    "    # Start the timer before the loop\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch[0].to(device), batch[1].to(device)\n",
    "            outputs = model(features)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # End the timer after the loop\n",
    "    end_time = time.time()    \n",
    "    # Calculate the total inference time\n",
    "    total_inference_time = end_time - start_time    \n",
    "    # Calculate average inference time per sample\n",
    "    #inference_time_per_sample =  total_inference_time * 1000\n",
    "    inference_time_per_sample =  total_inference_time * 1000000 / total_samples\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels), f'{inference_time_per_sample:.4f} us'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "106d4a49-75bd-4cc1-b995-0fdc5e26b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, title, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax)  # Pass ax here directly\n",
    "    ax.set_title(title)  # Optional: Set a title for each subplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59336a77-e75c-4299-9b8b-d9be081b3510",
   "metadata": {},
   "source": [
    "## 2. Performance/History of Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf58d5-b6f7-4295-8907-44d74e8bdaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b5d34-d75b-48b9-a7d2-52a86a0cc712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1199c7df-f815-4c52-93f6-f3854ebd9497",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will read pickle file and return the metrics\n",
    "def parse_history(history_path):\n",
    "    with open(history_path, 'rb') as file:\n",
    "        history = pickle.load(file)\n",
    "        # Extract distributed and centralized losses\n",
    "        loss_distributed = [loss for _, loss in history.losses_distributed]\n",
    "        loss_centralized = [loss for _, loss in history.losses_centralized]\n",
    "        \n",
    "        # Extract accuracy for distributed and centralized evaluation\n",
    "        accuracy_distributed = [acc for _, acc in history.metrics_distributed['accuracy']]\n",
    "        accuracy_centralized = [acc for _, acc in history.metrics_centralized['accuracy']]\n",
    "\n",
    "        return loss_distributed, loss_centralized, accuracy_distributed, accuracy_centralized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09acce9a-cf00-4638-9b90-b6c8ab25a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Parse the Training Time\n",
    "def parse_training_time(time_path):\n",
    "    # Open the text file and read the value\n",
    "    with open(time_path, 'r') as file:\n",
    "    # Read the content of the file and strip any extra spaces or newlines\n",
    "        content = file.read().strip()\n",
    "    # Convert the content to a float, round it, and cast it to an integer\n",
    "    rounded_value = round(float(content))\n",
    "    return int(rounded_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55aeb134-0bf6-4f4f-8018-6cfc65266b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(y_true, y_pred, classes, title, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax)  # Pass ax here directly\n",
    "    ax.set_title(title)  # Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc834c8-24d6-466f-a742-4fbba263fb18",
   "metadata": {},
   "source": [
    "### 2.1 Accuracy/Loss vs Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f53caf3-4641-4fb3-a80e-5e6a70b42745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 01:08:10,149\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './results/original_18_fold_2/history.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold \u001b[38;5;129;01min\u001b[39;00m result_sources\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfolds\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      8\u001b[0m     history_path \u001b[38;5;241m=\u001b[39m result_sources\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(component, fold) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/history.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m        \n\u001b[0;32m----> 9\u001b[0m     l_d, l_c, a_d, a_c \u001b[38;5;241m=\u001b[39m \u001b[43mparse_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory_path\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     10\u001b[0m     loss_distributed\u001b[38;5;241m.\u001b[39mappend(l_d)\n\u001b[1;32m     11\u001b[0m     loss_centralized\u001b[38;5;241m.\u001b[39mappend(l_c)\n",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m, in \u001b[0;36mparse_history\u001b[0;34m(history_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_history\u001b[39m(history_path):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhistory_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      4\u001b[0m         history \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m# Extract distributed and centralized losses\u001b[39;00m\n",
      "File \u001b[0;32m/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './results/original_18_fold_2/history.pkl'"
     ]
    }
   ],
   "source": [
    "for component in result_sources.get('components'):    \n",
    "    loss_distributed = [] \n",
    "    loss_centralized = [] \n",
    "    accuracy_distributed =[] \n",
    "    accuracy_centralized = []\n",
    "   \n",
    "    for fold in result_sources.get('folds'):\n",
    "        history_path = result_sources.get('path').format(component, fold) + '/history.pkl'        \n",
    "        l_d, l_c, a_d, a_c = parse_history(history_path)        \n",
    "        loss_distributed.append(l_d)\n",
    "        loss_centralized.append(l_c)\n",
    "        accuracy_distributed.append(a_d)\n",
    "        accuracy_centralized.append(a_c)\n",
    "\n",
    "    history_plots = [\n",
    "        {\n",
    "            'type': 'distributed_loss',\n",
    "            'plot_name': 'Distributed Loss {}',\n",
    "            'x': 'Rounds',\n",
    "            'y': 'Loss',\n",
    "            'plot_position': [0, 0],\n",
    "            'data': loss_distributed,\n",
    "            'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "        },\n",
    "        {\n",
    "            'type': 'accuracy_distributed',\n",
    "            'plot_name': 'Distributed Accuracy {}',\n",
    "            'x': 'Rounds',\n",
    "            'y': 'Accuracy',\n",
    "            'plot_position': [0, 1],\n",
    "            'data': accuracy_distributed,\n",
    "            'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "        },\n",
    "         {\n",
    "            'type': 'centralized_loss',\n",
    "            'plot_name': 'Centralized Loss {}',\n",
    "            'x': 'Rounds',\n",
    "            'y': 'Loss',\n",
    "            'plot_position': [1, 0],\n",
    "            'data': loss_centralized,\n",
    "            'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "        },\n",
    "        {\n",
    "            'type': 'centralized_accuracy',\n",
    "            'plot_name': 'Centralized Accuracy {}',\n",
    "            'x': 'Rounds',\n",
    "            'y': 'Accuracy',\n",
    "            'plot_position': [1, 1],\n",
    "            'data': accuracy_centralized,\n",
    "            'colors': ['red', 'brown', 'blue', 'purple', 'green']\n",
    "        },     \n",
    "      \n",
    "    ]\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(16, 9))  # Adjust the figsize as needed\n",
    "    for plot in  history_plots:\n",
    "        position = plot.get('plot_position')\n",
    "        all_fold_data = plot.get('data')\n",
    "        #print(len(all_fold_data))\n",
    "        for i, data in enumerate(all_fold_data):\n",
    "            rounds = list(range(1, len(data)+1))\n",
    "            ax[position[0], position[1]].plot(rounds, data, label=f'Fold_{i+1}', marker='o', color=plot.get('colors')[i])\n",
    "            ax[position[0], position[1]].set_title(plot.get('plot_name').format(component))\n",
    "            ax[position[0], position[1]].set_xlabel(plot.get('x'))\n",
    "            ax[position[0], position[1]].set_ylabel(plot.get('y'))\n",
    "            ax[position[0], position[1]].legend()\n",
    "            ax[position[0], position[1]].grid(True)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331ad99-9689-4909-824a-f4921522fe14",
   "metadata": {},
   "source": [
    "### 2.2 Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2107e6-fb98-4588-8ffa-d206223fe877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Plotting the Time charts\n",
    "# for component in result_sources.get('components'):\n",
    "#     training_time = []\n",
    "#     for fold in result_sources.get('folds'):\n",
    "#         ##Parsign the training time\n",
    "#         training_time_path = result_sources.get('path').format(component, fold) + '/training_time.txt'\n",
    "#         training_time = parse_training_time(training_time_path)\n",
    "#         print(f'[Component {component} Fold {fold}]: {training_time} Seconds')\n",
    "\n",
    "\n",
    "for component in result_sources.get('components'):\n",
    "    training_time = []\n",
    "    for fold in result_sources.get('folds'):\n",
    "        ##Parsign the training time\n",
    "        training_time_path = result_sources.get('path').format(component, fold) + '/training_time.txt'\n",
    "        training_time.append(parse_training_time(training_time_path))\n",
    "        #print(f'[Component {component} Fold {fold}]: {training_time} Seconds')\n",
    "\n",
    "    training_time_to_string = \", \".join(map(str, training_time))\n",
    "    print(f'Component {component}: {training_time_to_string}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b5db2-b662-4581-84fd-87faceaab836",
   "metadata": {},
   "source": [
    "## 3. Accumulate Results\n",
    "- Accumulate all thre results and save in csv file\n",
    "- It also stores values reauired for confusion matrix in a varialbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf6381e-b55e-4af1-ad87-c274d9c818d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "client_metrics = {\n",
    "    'Component': [],\n",
    "    'Fold': [],\n",
    "    'Client': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1_Score': [],\n",
    "    'Sample_Number': [],\n",
    "    'Inference_Time_Per_Sample': []\n",
    "}\n",
    "\n",
    "classes = np.arange(NUM_CLASSES)  # Define or import this variable\n",
    "\n",
    "def accumulate_results(results, confusion_matrix_data, model_to_use):\n",
    "    components = results.get('components')\n",
    "    folds = results.get('folds')\n",
    "    path = results.get('path')\n",
    "    clients = results.get('clients')\n",
    "    for component in components:  \n",
    "        for fold in folds:\n",
    "            global_model = path.format(component, fold) + '/' + model_to_use\n",
    "            history = path.format(component, fold) + '/' + 'history.pkl'\n",
    "            #training_time = path.format(component, fold) + '/' + 'training_time.txt'\n",
    "            \n",
    "            model = load_model(model_path=global_model, input_size=component, num_classes=2)\n",
    "            model.to(device)    \n",
    "            num_clients = NUM_CLIENTS  # Define or import this variable    \n",
    "            \n",
    "            for client in clients:                                \n",
    "                testset = get_evaluation_datasets_by_client(client, fold=fold, feature_count=component) \n",
    "                #print(testset.shape())\n",
    "                testloader = DataLoader(to_tensor(testset), batch_size=BATCH_SIZE)\n",
    "                preds, labels, inference_time_per_sample = run_inference(model, testloader, device)\n",
    "                client_metrics['Component'].append(component)\n",
    "                client_metrics['Fold'].append(fold)\n",
    "                client_metrics['Client'].append(client)\n",
    "                client_metrics['Accuracy'].append(accuracy_score(labels, preds))\n",
    "                client_metrics['Precision'].append(precision_score(labels, preds))\n",
    "                client_metrics['Recall'].append(recall_score(labels, preds))\n",
    "                client_metrics['F1_Score'].append(f1_score(labels, preds))\n",
    "                client_metrics['Sample_Number'].append(len(labels)),\n",
    "                client_metrics['Inference_Time_Per_Sample'].append(inference_time_per_sample)\n",
    "\n",
    "                #Saving info for confusion matrix\n",
    "                key = f'{component}_{fold}_{client}'\n",
    "                confusion_matrix_data[key] = {\n",
    "                    'preds': preds,\n",
    "                    'labels': labels,\n",
    "                    'classes': np.arange(NUM_CLASSES)\n",
    "                }   \n",
    "\n",
    "    ##Converting into datafram for better visualization\n",
    "    df = pd.DataFrame(client_metrics)\n",
    "    print(df.to_string(index=False))\n",
    "    return df, confusion_matrix_data           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c6cb12-f99f-4019-9a45-c9c814003fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_data = {}\n",
    "result_df, store_results_df = accumulate_results(result_sources, confusion_matrix_data, 'best_global_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcee64a5-4404-4e51-aed9-cbcc52e1d352",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix (per client per Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369edecd-71ed-4395-9f49-15e6138f84fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860421bd-e59e-4d79-be72-7bc5aa089873",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = [\n",
    "    {\n",
    "        'client_id': 1,\n",
    "        'plot_name': 'Client 1',\n",
    "        'plot_position': [0, 0]\n",
    "    },\n",
    "    {\n",
    "        'client_id': 2,\n",
    "        'plot_name': 'Client 2',\n",
    "        'plot_position': [0, 1]\n",
    "    },\n",
    "    {\n",
    "        'client_id': 3,\n",
    "        'plot_name': 'Client 3',\n",
    "        'plot_position': [1, 0]\n",
    "    },\n",
    "    {\n",
    "        'client_id': 4,\n",
    "        'plot_name': 'Client 4',\n",
    "        'plot_position': [1, 1]\n",
    "    }   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1b01cb-ef88-4e38-a688-ae3c2e7030ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for component in result_sources.get('components'):\n",
    "#     for fold in result_sources.get('folds'):\n",
    "#         #print(f\" Plots for Components: {component}, and Folds: {fold}\")\n",
    "#         fig, ax = plt.subplots(2, 2, figsize=(12, 10))  # Adjust the figsize as needed\n",
    "#         for plot in plots:\n",
    "#             client = plot.get('client_id')\n",
    "#             key = f'{component}_{fold}_{client}'\n",
    "#             data = confusion_matrix_data[key]\n",
    "#             title = \"{0} ({1}_{2})\".format(plot['plot_name'], component, fold)\n",
    "#             plot_confusion_matrix(data['labels'], data['preds'], data['classes'], title, ax = ax[plot['plot_position'][0], plot['plot_position'][1]])\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "#         print(\"---------------------------------------------------------------------------------------------------------------------------------\")\n",
    "#         print(\"---------------------------------------------------------------------------------------------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f6a38-c15a-428b-ba55-d32bb4e572dd",
   "metadata": {},
   "source": [
    "## 5. Accumulate All the Training/Validaiton Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b666a0-03c7-4db7-835b-f24f837392d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "components =  result_sources.get('components')\n",
    "folds =  result_sources.get('folds')\n",
    "path =  result_sources.get('path')\n",
    "#clients = results.get('clients')\n",
    "\n",
    "metric_dfs = []\n",
    "for component in components:  \n",
    "    for fold in folds:\n",
    "        metric_path = path.format(component, fold) + '/' + 'metrics.csv'\n",
    "        metric_df = pd.read_csv(metric_path)\n",
    "        metric_df['fold'] = fold\n",
    "        metric_dfs.append(metric_df)\n",
    "\n",
    "#merging\n",
    "merged_df = pd.concat(metric_dfs, ignore_index=True)\n",
    "merged_df.to_csv(f\"./results/accumulted_metrics_{components[0]}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0f0ba-a3e3-4350-9dcf-599e93cd939a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "284d1734-2118-44da-86f6-9fe9f5369df1",
   "metadata": {},
   "source": [
    "## 5. Accumulate All Local Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade5bf1a-4da9-49ba-a0dc-5548a229d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "components =  result_sources.get('components')\n",
    "folds =  result_sources.get('folds')\n",
    "path =  result_sources.get('path')\n",
    "clients = result_sources.get('clients')\n",
    "\n",
    "local_training_dfs = []\n",
    "for component in components:\n",
    "    for fold in folds:\n",
    "        for client in clients:\n",
    "            local_training_history_path = path.format(component, fold) + f\"/local_train_history_{client}.csv\"\n",
    "            local_training_df = pd.read_csv(local_training_history_path)\n",
    "            local_training_df['fold'] = fold\n",
    "            local_training_dfs.append(local_training_df)\n",
    "\n",
    "#merging\n",
    "local_training_dfs_merged = pd.concat(local_training_dfs, ignore_index=True)\n",
    "local_training_dfs_merged.to_csv(f\"./results/local_training_metrics_{components[0]}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fba70f-69e7-41c9-8a46-072770da0ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
