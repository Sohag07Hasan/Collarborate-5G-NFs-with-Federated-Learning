{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overall DataSet Preparation from Row data\n",
    "- Read all the row feature files from the directory specified\n",
    "- Sanitize all the inputs\n",
    "- Take sample count based on attack and benign csv files (technically it's class)\n",
    "- Save the dataset locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_csv_files(directory_path, features=None):\n",
    "    # Initialize an empty list to store dataframes from CSV files\n",
    "    dataframes = []\n",
    "\n",
    "    # Get a list of all files in the directory\n",
    "    file_list = os.listdir(directory_path)\n",
    "\n",
    "    # Loop through each file and check if it's a CSV file\n",
    "    for file_number, file_name in enumerate(file_list):\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Get the full file path\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            # Read the CSV file into a pandas DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Remove leading and trailing spaces from column names\n",
    "            df.columns = df.columns.str.strip()\n",
    "            # Append the DataFrame to the list\n",
    "            df['CSV_File_Number'] = file_number\n",
    "            #dataframes.append(df[features])\n",
    "            dataframes.append(df) #as it is feature analysis, we are taking all the features\n",
    "\n",
    "    # Merge all DataFrames into a single DataFrame\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_data_frames_updated(dataframe, remove_infinity=True, remove_null=True):\n",
    "\n",
    "    if remove_infinity:\n",
    "        numeric_cols = dataframe.select_dtypes(include=[np.number]).columns\n",
    "        infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n",
    "        for col, count in infinite_counts.items():\n",
    "            if count != 0:\n",
    "                dataframe = dataframe[~np.isinf(dataframe[col])]\n",
    "\n",
    "    if remove_null:\n",
    "        null_counts = dataframe.isnull().sum()\n",
    "        for col, count in null_counts.items():\n",
    "            if count != 0:\n",
    "                    dataframe = dataframe.dropna(subset=[col])\n",
    "    print(\"Sanitized Row Count:\", dataframe.shape[0])    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all dataset sources to make iterate to read csv files\n",
    "dataset_sources = {\n",
    " \n",
    "    'client_1': {\n",
    "        'benign': '../row_data/client_1/benign',\n",
    "        'attack': '../row_data/client_1/attack',\n",
    "    },\n",
    "    'client_2': {\n",
    "        'benign': '../row_data/client_2/benign',\n",
    "        'attack': '../row_data/client_2/attack',\n",
    "    },\n",
    "    'client_3': {\n",
    "        'benign': '../row_data/client_3/benign',\n",
    "        'attack': '../row_data/client_3/attack',\n",
    "    },\n",
    "    'client_4': {\n",
    "        'benign': '../row_data/client_4/benign',\n",
    "        'attack': '../row_data/client_4/attack',\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Uniform Sample\n",
    "def get_uniform_sample(df, group_col=[], sample_size=420000):\n",
    "    \n",
    "    #Separate the labels\n",
    "    label_0 = df[df['Label'] == 0]\n",
    "    label_1 = df[df['Label'] == 1]\n",
    "\n",
    "    count_0 = count_1 = 0 # Initialize with 0\n",
    "\n",
    "    if len(label_0) >= sample_size//2 and len(label_1) >= sample_size//2:\n",
    "        count_0 = count_1 = sample_size//2\n",
    "    elif len(label_0) < sample_size//2 and len(label_1) > sample_size//2:\n",
    "        count_0 = len(label_0)\n",
    "        count_1 = sample_size - count_0\n",
    "    elif len(label_0) > sample_size//2 and len(label_1) < sample_size//2:\n",
    "        count_1 = len(label_1)\n",
    "        count_0 = sample_size - count_1\n",
    "    else:\n",
    "        count_0 = len(label_0)\n",
    "        count_1 = len(label_1)\n",
    "\n",
    "    print(f\"count_0: {count_0}; count_1: {count_1}\")\n",
    "    print(f\"label_0: {len(label_0)}; label_1: {len(label_1)}\")\n",
    "\n",
    "    train_size_0 = 1.0 if count_0 >= len(label_0) else float(count_0/len(label_0))\n",
    "    train_size_1 = 1.0 if count_1 >= len(label_1) else float(count_1/len(label_1))\n",
    "\n",
    "    print(f\"train_size_0: {train_size_0}; train_size_1: {train_size_1}\")\n",
    "    \n",
    "# Handle edge cases where train_size is 1.0 and stratification might fail\n",
    "    if train_size_0 == 1.0:\n",
    "        sample_0 = label_0.copy()  # Use the whole subset without train_test_split\n",
    "    else:\n",
    "        sample_0, _ = train_test_split(\n",
    "            label_0,\n",
    "            train_size=train_size_0,\n",
    "            stratify=label_0['CSV_File_Number'] if group_col else None,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if train_size_1 == 1.0:\n",
    "        sample_1 = label_1.copy()  # Use the whole subset without train_test_split\n",
    "    else:\n",
    "        sample_1, _ = train_test_split(\n",
    "            label_1,\n",
    "            train_size=train_size_1,\n",
    "            stratify=label_1['CSV_File_Number'] if group_col else None,\n",
    "            random_state=42\n",
    "        )   \n",
    "    \n",
    "    # Concatenate and shuffle the samples\n",
    "    combined_sample = pd.concat([sample_0, sample_1])\n",
    "    shuffled_sample = combined_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    return shuffled_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory if it doesn't exist\n",
    "# THis will create subdirectoy as well\n",
    "def ensure_directory_exists(filepath):\n",
    "    directory = os.path.dirname(filepath)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 40136.88it/s]\n",
      "  0%|                                                                   | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_1, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2309040/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 202821\n",
      " Loading....Clinet = client_1, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2309040/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 2671524\n",
      "count_0: 202821; count_1: 217179\n",
      "label_0: 202821; label_1: 2671524\n",
      "train_size_0: 1.0; train_size_1: 0.08129404789176516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████▌                                           | 1/4 [03:57<11:51, 237.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_2, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2309040/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 185943\n",
      " Loading....Clinet = client_2, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2309040/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 2670872\n",
      "count_0: 185943; count_1: 234057\n",
      "label_0: 185943; label_1: 2670872\n",
      "train_size_0: 1.0; train_size_1: 0.08763317747911543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████                             | 2/4 [07:51<07:50, 235.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_3, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2309040/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 223200\n",
      " Loading....Clinet = client_3, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2309040/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 2597584\n",
      "count_0: 210000; count_1: 210000\n",
      "label_0: 223200; label_1: 2597584\n",
      "train_size_0: 0.9408602150537635; train_size_1: 0.0808443538303285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████▌              | 3/4 [11:44<03:54, 234.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_4, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2309040/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 246797\n",
      " Loading....Clinet = client_4, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2309040/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 2495085\n",
      "count_0: 210000; count_1: 210000\n",
      "label_0: 246797; label_1: 2495085\n",
      "train_size_0: 0.850901753262803; train_size_1: 0.0841654693126687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 4/4 [15:30<00:00, 232.57s/it]\n"
     ]
    }
   ],
   "source": [
    "##Reading all the data\n",
    "features = None\n",
    "\n",
    "\n",
    "for client, data in tqdm(dataset_sources.items(), total=len(dataset_sources)):\n",
    "    locals()[client] = {}\n",
    "\n",
    "for client, data in tqdm(dataset_sources.items(), total=len(dataset_sources)):\n",
    "    client_dataframe = []\n",
    "    for type, path in data.items():\n",
    "        print(f' Loading....Clinet = {client}, type = {type}')\n",
    "        dataframe = read_all_csv_files(path, features)\n",
    "        dataframe = sanitize_data_frames_updated(dataframe)\n",
    "        if type == 'benign':\n",
    "            dataframe['Label'] = 0\n",
    "        else:\n",
    "             dataframe['Label'] = 1\n",
    "        client_dataframe.append(dataframe)\n",
    "        #locals()[client][type] = dataframe\n",
    "    client_merged_df = get_uniform_sample(pd.concat(client_dataframe, ignore_index=True), ['Label', 'CSV_File_Number'], 420000 )\n",
    "    client_merged_df.to_csv(ensure_directory_exists(f'./dataset/{client}/{client}_original_dataset.csv'), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Train and Test Set Segregation\n",
    "- Read datasets saved in Step 1\n",
    "- Segregate Train and Test set and save locally. As we need to take decission based on csv file number and label, we will combine both column to stratify\n",
    "- Sacling Training data save the scaled data and save the scalers as well\n",
    "- Use saved scalers to scale the test data and save the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_exclude = ['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Timestamp', 'CSV_File_Number', 'Label', 'Stratify']\n",
    "output_features = ['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iteration Friendly dictionary\n",
    "clients = {\n",
    " \n",
    "    'client_1': {\n",
    "        'combined': './dataset/client_1/client_1_original_dataset.csv',\n",
    "        'train': './dataset/client_1/fold_{}/client_1_train_dataset.csv',\n",
    "        'test': './dataset/client_1/fold_{}/client_1_test_dataset.csv',\n",
    "        'scaler': './dataset/client_1/fold_{}/client_1_train_scaler.pkl',\n",
    "        'scaled_train': './dataset/client_1/fold_{}/client_1_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_1/fold_{}/client_1_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_2': {\n",
    "        'combined': './dataset/client_2/client_2_original_dataset.csv',\n",
    "        'train': './dataset/client_2/fold_{}/client_2_train_dataset.csv',\n",
    "        'test': './dataset/client_2/fold_{}/client_2_test_dataset.csv',\n",
    "        'scaler': './dataset/client_2/fold_{}/client_2_train_scaler.pkl',\n",
    "        'scaled_train': './dataset/client_2/fold_{}/client_2_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_2/fold_{}/client_2_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_3': {\n",
    "        'combined': './dataset/client_3/client_3_original_dataset.csv',\n",
    "        'train': './dataset/client_3/fold_{}/client_3_train_dataset.csv',\n",
    "        'test': './dataset/client_3/fold_{}/client_3_test_dataset.csv',\n",
    "        'scaler': './dataset/client_3/fold_{}/client_3_train_scaler.pkl',\n",
    "        'scaled_train': './dataset/client_3/fold_{}/client_3_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_3/fold_{}/client_3_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_4': {\n",
    "        'combined': './dataset/client_4/client_4_original_dataset.csv',\n",
    "        'train': './dataset/client_4/fold_{}/client_4_train_dataset.csv',\n",
    "        'test': './dataset/client_4/fold_{}/client_4_test_dataset.csv',\n",
    "        'scaler': './dataset/client_4/fold_{}/client_4_train_scaler.pkl',\n",
    "        'scaled_train': './dataset/client_4/fold_{}/client_4_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_4/fold_{}/client_4_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    },    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_remove_unwanted_features(dataframe, feature_to_exclude, out_features, scaler_path, type='train'):\n",
    "\n",
    "    features = dataframe.columns.values.tolist()\n",
    "    input_features = [feature for feature in features if feature not in feature_to_exclude] \n",
    "    output_df = dataframe[out_features]\n",
    "    input_df = dataframe[input_features]\n",
    "\n",
    "    print(f\"Input features shape: {input_df.shape}\")  # Debugging\n",
    "    print(f\"Output (label) features shape: {output_df.shape}\")  # Debugging\n",
    "\n",
    "    # Reset index for both input and output DataFrames to ensure correct alignment\n",
    "    input_df = input_df.reset_index(drop=True)\n",
    "    output_df = output_df.reset_index(drop=True)\n",
    "    \n",
    "    if type == 'train':\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(input_df)\n",
    "        scaled_df = pd.DataFrame(scaled_data, columns=input_df.columns)\n",
    "        print(f\"Scaled features shape: {scaled_df.shape}\")  # Debugging\n",
    "        \n",
    "        merged_df = pd.concat([scaled_df, output_df], axis=1)\n",
    "        print(f\"Merged DataFrame shape: {merged_df.shape}\")  # Debugging\n",
    "\n",
    "        # Save the scaler to a file to be used for test set\n",
    "        with open(scaler_path, \"wb\") as file:\n",
    "            pickle.dump(scaler, file)\n",
    "    else:\n",
    "        with open(scaler_path, \"rb\") as file:\n",
    "            scaler = pickle.load(file)\n",
    "            scaled_data = scaler.transform(input_df)  # Changed from fit_transform to transform\n",
    "            scaled_df = pd.DataFrame(scaled_data, columns=input_df.columns)\n",
    "            print(f\"Scaled features shape: {scaled_df.shape}\")  # Debugging\n",
    "            merged_df = pd.concat([scaled_df, output_df], axis=1)\n",
    "            print(f\"Merged DataFrame shape: {merged_df.shape}\")  # Debugging\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                   | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "Started for Client: client_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started for Split: 1\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_1\n",
      "Started for Split: 2\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_1\n",
      "Started for Split: 3\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_1\n",
      "Started for Split: 4\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_1\n",
      "Started for Split: 5\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████▌                                           | 1/4 [04:22<13:06, 262.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End for Client: client_1\n",
      "------------------------------------------------------------------------------\n",
      "Started for Client: client_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started for Split: 1\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_2\n",
      "Started for Split: 2\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_2\n",
      "Started for Split: 3\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_2\n",
      "Started for Split: 4\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_2\n",
      "Started for Split: 5\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████                             | 2/4 [08:40<08:40, 260.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End for Client: client_2\n",
      "------------------------------------------------------------------------------\n",
      "Started for Client: client_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started for Split: 1\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_3\n",
      "Started for Split: 2\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_3\n",
      "Started for Split: 3\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_3\n",
      "Started for Split: 4\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_3\n",
      "Started for Split: 5\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████▌              | 3/4 [12:45<04:13, 253.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End for Client: client_3\n",
      "------------------------------------------------------------------------------\n",
      "Started for Client: client_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started for Split: 1\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_4\n",
      "Started for Split: 2\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_4\n",
      "Started for Split: 3\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_4\n",
      "Started for Split: 4\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_4\n",
      "Started for Split: 5\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 4/4 [17:24<00:00, 261.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End for Client: client_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for client, info in tqdm(clients.items(), total=len(clients)):\n",
    "    print('------------------------------------------------------------------------------')\n",
    "    print(f\"Started for Client: {client}\")\n",
    "    df = pd.read_csv(info.get('combined'))\n",
    "    #combinding label and csv file number to stratify\n",
    "    df['Stratify'] = df['Label'].astype(str) + '_' + df['CSV_File_Number'].astype(str)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    #Loop through the splitted items:\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(df, df['Stratify']), 1):\n",
    "        print(f\"Started for Split: {fold}\")\n",
    "        # Create training and test sets for the current fold\n",
    "        train_df = df.iloc[train_index].reset_index(drop=True)\n",
    "        test_df = df.iloc[test_index].reset_index(drop=True)    \n",
    "\n",
    "        #checking if fold path exists. if not it will create one\n",
    "        #ensure_directory_exists(info.get('train').format(fold))\n",
    "        \n",
    "        train_df.to_csv(ensure_directory_exists(info.get('train').format(fold)), index=False) ##It will contain all column with additional column\n",
    "        test_df.to_csv(ensure_directory_exists(info.get('test').format(fold)), index=False) ##It will contain all column with additional column\n",
    "    \n",
    "        scaled_train_df = scale_and_remove_unwanted_features(train_df, feature_to_exclude, output_features, info.get('scaler').format(fold), 'train')\n",
    "        #scaled_train_df.describe()\n",
    "        scaled_train_df.to_csv(ensure_directory_exists(info.get('scaled_train')).format(fold), index=False)\n",
    "    \n",
    "        scaled_test_df = scale_and_remove_unwanted_features(test_df, feature_to_exclude, output_features, info.get('scaler').format(fold), 'test')\n",
    "        #scaled_test_df.describe()\n",
    "        scaled_test_df.to_csv(ensure_directory_exists(info.get('scaled_test').format(fold)), index=False)\n",
    "        print(f\"End for Client: {client}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Analysis of Prepared Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client: client_1\n",
      "Client: client_2\n",
      "Client: client_3\n",
      "Client: client_4\n"
     ]
    }
   ],
   "source": [
    "#pd.concat(client_dataframe, ignore_index=True)\n",
    "intersted_features = ['Label', 'CSV_File_Number', 'Stratify']\n",
    "train_df_all_clients = []\n",
    "test_df_all_clients = []\n",
    "for client, info in clients.items():\n",
    "    print(f'Client: {client}')\n",
    "    for fold in range(1, 6):\n",
    "        train_df = pd.read_csv(info.get('train').format(fold))\n",
    "        test_df = pd.read_csv(info.get('test').format(fold))\n",
    "\n",
    "        train_df = train_df[intersted_features]\n",
    "        test_df = test_df[intersted_features]\n",
    "        \n",
    "        train_df['client'] = client\n",
    "        train_df['fold'] = fold\n",
    "        \n",
    "        test_df['client'] = client\n",
    "        test_df['fold'] = fold\n",
    "\n",
    "        train_df_all_clients.append(train_df)\n",
    "        test_df_all_clients.append(test_df)\n",
    "        \n",
    "merged_train_df = pd.concat(train_df_all_clients, ignore_index=True)\n",
    "merged_test_df = pd.concat(test_df_all_clients, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Client Stratify   Fold_1   Fold_2   Fold_3   Fold_4   Fold_5\n",
      "0   client_1      0_0  82051.0  82051.0  82051.0  82051.0  82052.0\n",
      "1   client_1      0_1  41618.0  41618.0  41618.0  41619.0  41619.0\n",
      "2   client_1      0_2      3.0      3.0      2.0      2.0      2.0\n",
      "3   client_1      0_3  38585.0  38585.0  38585.0  38584.0  38585.0\n",
      "4   client_1      1_0  17049.0  17049.0  17049.0  17049.0  17048.0\n",
      "5   client_1      1_1  17048.0  17049.0  17049.0  17049.0  17049.0\n",
      "6   client_1      1_2  17049.0  17049.0  17049.0  17049.0  17048.0\n",
      "7   client_1      1_3  17049.0  17048.0  17049.0  17049.0  17049.0\n",
      "8   client_1      1_4  37354.0  37354.0  37354.0  37355.0  37355.0\n",
      "9   client_1      1_5  17048.0  17048.0  17048.0  17048.0  17048.0\n",
      "10  client_1      1_6  17049.0  17049.0  17049.0  17048.0  17049.0\n",
      "11  client_1      1_7  17049.0  17049.0  17049.0  17049.0  17048.0\n",
      "12  client_1      1_8  17048.0  17048.0  17048.0  17048.0  17048.0\n",
      "13  client_2      0_0  46683.0  46682.0  46682.0  46682.0  46683.0\n",
      "14  client_2      0_1      3.0      3.0      2.0      2.0      2.0\n",
      "15  client_2      0_2  83134.0  83134.0  83134.0  83135.0  83135.0\n",
      "16  client_2      0_3  18935.0  18935.0  18936.0  18935.0  18935.0\n",
      "17  client_2      1_0  18379.0  18379.0  18378.0  18378.0  18378.0\n",
      "18  client_2      1_1  18378.0  18378.0  18379.0  18379.0  18378.0\n",
      "19  client_2      1_2  18377.0  18378.0  18378.0  18378.0  18377.0\n",
      "20  client_2      1_3  18378.0  18378.0  18378.0  18377.0  18377.0\n",
      "21  client_2      1_4  18377.0  18377.0  18378.0  18378.0  18378.0\n",
      "22  client_2      1_5  18378.0  18378.0  18377.0  18377.0  18378.0\n",
      "23  client_2      1_6  18378.0  18378.0  18378.0  18379.0  18379.0\n",
      "24  client_2      1_7  18378.0  18378.0  18379.0  18379.0  18378.0\n",
      "25  client_2      1_8  40222.0  40222.0  40221.0  40221.0  40222.0\n",
      "26  client_3      0_0  37895.0  37895.0  37895.0  37895.0  37896.0\n",
      "27  client_3      0_1  41041.0  41041.0  41041.0  41041.0  41040.0\n",
      "28  client_3      0_2     17.0     17.0     18.0     18.0     18.0\n",
      "29  client_3      0_3      9.0      9.0      9.0      9.0      8.0\n",
      "30  client_3      0_4  14656.0  14657.0  14657.0  14657.0  14657.0\n",
      "31  client_3      0_5     19.0     19.0     18.0     18.0     18.0\n",
      "32  client_3      0_6   3043.0   3043.0   3042.0   3042.0   3042.0\n",
      "33  client_3      0_7      3.0      3.0      3.0      3.0      4.0\n",
      "34  client_3      0_8  71317.0  71316.0  71317.0  71317.0  71317.0\n",
      "35  client_3      1_0  33208.0  33208.0  33208.0  33208.0  33208.0\n",
      "36  client_3      1_1  16954.0  16954.0  16954.0  16955.0  16955.0\n",
      "37  client_3      1_2  16955.0  16955.0  16954.0  16954.0  16954.0\n",
      "38  client_3      1_3  16112.0  16112.0  16112.0  16112.0  16112.0\n",
      "39  client_3      1_4  16954.0  16954.0  16955.0  16955.0  16954.0\n",
      "40  client_3      1_5  16955.0  16954.0  16954.0  16954.0  16955.0\n",
      "41  client_3      1_6  16955.0  16954.0  16954.0  16954.0  16955.0\n",
      "42  client_3      1_7  16954.0  16955.0  16955.0  16954.0  16954.0\n",
      "43  client_3      1_8  16953.0  16954.0  16954.0  16954.0  16953.0\n",
      "44  client_4      0_0     11.0     12.0     11.0     11.0     11.0\n",
      "45  client_4      0_1  53876.0  53876.0  53876.0  53876.0  53876.0\n",
      "46  client_4      0_2      2.0      2.0      3.0      3.0      2.0\n",
      "47  client_4      0_3  23495.0  23495.0  23494.0  23494.0  23494.0\n",
      "48  client_4      0_4      9.0      9.0      9.0      8.0      9.0\n",
      "49  client_4      0_5      4.0      3.0      3.0      3.0      3.0\n",
      "50  client_4      0_6  24611.0  24611.0  24611.0  24612.0  24611.0\n",
      "51  client_4      0_7  63914.0  63914.0  63914.0  63915.0  63915.0\n",
      "52  client_4      0_8   2078.0   2078.0   2078.0   2079.0   2079.0\n",
      "53  client_4      1_0  17651.0  17651.0  17651.0  17652.0  17651.0\n",
      "54  client_4      1_1  17651.0  17651.0  17651.0  17652.0  17651.0\n",
      "55  client_4      1_2  17651.0  17650.0  17650.0  17650.0  17651.0\n",
      "56  client_4      1_3  17650.0  17651.0  17651.0  17650.0  17650.0\n",
      "57  client_4      1_4  17650.0  17651.0  17651.0  17650.0  17650.0\n",
      "58  client_4      1_5  17651.0  17651.0  17652.0  17651.0  17651.0\n",
      "59  client_4      1_6  16234.0  16235.0  16235.0  16234.0  16234.0\n",
      "60  client_4      1_7  17651.0  17650.0  17650.0  17650.0  17651.0\n",
      "61  client_4      1_8  28211.0  28210.0  28210.0  28210.0  28211.0\n"
     ]
    }
   ],
   "source": [
    "#print(merged_train_df.groupby(['client', 'fold', 'Stratify']).count())\n",
    "grouped_counts = merged_train_df.groupby(['client', 'fold', 'Stratify']).size().reset_index(name='count')\n",
    "# Pivoting the 'fold' column to make it into a separate column\n",
    "pivot_df = grouped_counts.pivot_table(index=['client', 'Stratify'], columns='fold', values='count', fill_value=0).reset_index()\n",
    "pivot_df.columns = ['Client', 'Stratify'] + [f'Fold_{int(col)}' for col in pivot_df.columns if isinstance(col, int)]\n",
    "pivot_df.to_csv(ensure_directory_exists(\"./dataset/dataset_summary.csv\"), index=False)\n",
    "print(pivot_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>CSV_File_Number</th>\n",
       "      <th>Stratify</th>\n",
       "      <th>client</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0</td>\n",
       "      <td>client_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1_9</td>\n",
       "      <td>client_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0_1</td>\n",
       "      <td>client_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1_8</td>\n",
       "      <td>client_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>client_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  CSV_File_Number Stratify    client  fold\n",
       "0      0                0      0_0  client_1     1\n",
       "1      1                9      1_9  client_1     1\n",
       "2      0                1      0_1  client_1     1\n",
       "3      1                8      1_8  client_1     1\n",
       "4      1                1      1_1  client_1     1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA dataset Preparation\n",
    "- Choose component of 30, 33, 35 and generate datasets accordingly for training dataset and store locally\n",
    "- Use PCA matrix to convert test dataset and store locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iteration Friendly dictionary\n",
    "clients = {\n",
    " \n",
    "    'client_1': {\n",
    "        'scaled_train': './dataset/client_1/fold_{}/client_1_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_1/fold_{}/client_1_sclaed_test_dataset.csv', #unwanted features removed\n",
    "        'pca_path': './dataset/client_1/fold_{}/pca/client_1_pca.pkl', #Saved for later use\n",
    "        'pca_train': './dataset/client_1/fold_{}/pca/client_1_pca_train_dataset.csv', #unwanted features removed\n",
    "        'pca_test': './dataset/client_1/fold_{}/pca/client_1_pca_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_2': {\n",
    "        'scaled_train': './dataset/client_2/fold_{}/client_2_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_2/fold_{}/client_2_sclaed_test_dataset.csv', #unwanted features removed\n",
    "        'pca_path': './dataset/client_2/fold_{}/pca/client_2_pca.pkl', #Saved for later use\n",
    "        'pca_train': './dataset/client_2/fold_{}/pca/client_2_pca_train_dataset.csv', #unwanted features removed\n",
    "        'pca_test': './dataset/client_2/fold_{}/pca/client_2_pca_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_3': {\n",
    "        'scaled_train': './dataset/client_3/fold_{}/client_3_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_3/fold_{}/client_3_sclaed_test_dataset.csv', #unwanted features removed\n",
    "        'pca_path': './dataset/client_3/fold_{}/pca/client_3_pca.pkl', #Saved for later use\n",
    "        'pca_train': './dataset/client_3/fold_{}/pca/client_3_pca_train_dataset.csv', #unwanted features removed\n",
    "        'pca_test': './dataset/client_3/fold_{}/pca/client_3_pca_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_4': {\n",
    "        'scaled_train': './dataset/client_4/fold_{}/client_4_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_4/fold_{}/client_4_sclaed_test_dataset.csv', #unwanted features removed\n",
    "        'pca_path': './dataset/client_4/fold_{}/pca/client_4_pca.pkl', #Saved for later use\n",
    "        'pca_train': './dataset/client_4/fold_{}/pca/client_4_pca_train_dataset.csv', #unwanted features removed\n",
    "        'pca_test': './dataset/client_4/fold_{}/pca/client_4_pca_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Function to perform PCA with a maximum number of components and save the PCA object\n",
    "def perform_pca_and_save_with_max(train_df, test_df, max_components, pca_path):\n",
    "    # Remove 'Label' column before applying PCA\n",
    "    train_labels = train_df['Label']\n",
    "    test_labels = test_df['Label']\n",
    "    train_features = train_df.drop(columns=['Label'])\n",
    "    test_features = test_df.drop(columns=['Label'])\n",
    "\n",
    "    # Initialize PCA with the maximum number of components\n",
    "    pca = PCA(n_components=max_components)\n",
    "    \n",
    "    # Fit PCA on the training set and transform\n",
    "    train_pca_full = pca.fit_transform(train_features)\n",
    "\n",
    "    ##Ensure direcoty exists. if not it will create one\n",
    "    ensure_directory_exists(pca_path)\n",
    "    \n",
    "    # Save the PCA model for future use\n",
    "    with open(pca_path, \"wb\") as f:\n",
    "        pickle.dump(pca, f)\n",
    "    \n",
    "    #print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    \n",
    "    # Transform the test set using the same PCA model\n",
    "    test_pca_full = pca.transform(test_features)\n",
    "    \n",
    "    return train_pca_full, test_pca_full, train_labels, test_labels\n",
    "\n",
    "# Step 2: Function to slice PCA components and add labels back to the dataset\n",
    "def add_labels_to_pca(train_pca_full, test_pca_full, train_labels, test_labels, num_components):\n",
    "    # Slice the top 'num_components' from the full PCA results\n",
    "    train_pca_reduced = train_pca_full[:, :num_components]\n",
    "    test_pca_reduced = test_pca_full[:, :num_components]\n",
    "    \n",
    "    # Convert to DataFrame for easier handling\n",
    "    train_pca_df = pd.DataFrame(train_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "    test_pca_df = pd.DataFrame(test_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "    \n",
    "    # Add the 'Label' column back\n",
    "    train_pca_df['Label'] = train_labels.values\n",
    "    test_pca_df['Label'] = test_labels.values\n",
    "    \n",
    "    return train_pca_df, test_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "Client: client_1 ... Fold: 1\n",
      "Client: client_1 ... Fold: 2\n",
      "Client: client_1 ... Fold: 3\n",
      "Client: client_1 ... Fold: 4\n",
      "Client: client_1 ... Fold: 5\n",
      "-----------------------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Client: client_2 ... Fold: 1\n",
      "Client: client_2 ... Fold: 2\n",
      "Client: client_2 ... Fold: 3\n",
      "Client: client_2 ... Fold: 4\n",
      "Client: client_2 ... Fold: 5\n",
      "-----------------------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Client: client_3 ... Fold: 1\n",
      "Client: client_3 ... Fold: 2\n",
      "Client: client_3 ... Fold: 3\n",
      "Client: client_3 ... Fold: 4\n",
      "Client: client_3 ... Fold: 5\n",
      "-----------------------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Client: client_4 ... Fold: 1\n",
      "Client: client_4 ... Fold: 2\n",
      "Client: client_4 ... Fold: 3\n",
      "Client: client_4 ... Fold: 4\n",
      "Client: client_4 ... Fold: 5\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "max_pca_components = 40 # This number chosen based on pca analysis. check PCA.ipynb\n",
    "\n",
    "for client, info in clients.items():\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    for fold in range(1, 6):\n",
    "        print(f'Client: {client} ... Fold: {fold}')\n",
    "        train_df = pd.read_csv(info.get('scaled_train').format(fold))\n",
    "        test_df = pd.read_csv(info.get('scaled_test').format(fold))\n",
    "    \n",
    "        # Step 3: Perform PCA with the maximum number of components (e.g., 40)\n",
    "        train_pca_full, test_pca_full, train_labels, test_labels = perform_pca_and_save_with_max(train_df, test_df, max_components=max_pca_components, pca_path=info.get(\"pca_path\").format(fold))      \n",
    "        train_pca_data, test_pca_data = add_labels_to_pca(train_pca_full, test_pca_full, train_labels, test_labels, num_components=max_pca_components)\n",
    "        \n",
    "        train_pca_data.to_csv(info.get('pca_train').format(fold),  index=False)\n",
    "        test_pca_data.to_csv(info.get('pca_test').format(fold),  index=False)\n",
    "\n",
    "    print(\"-----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not in USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Iteration Friendly dictionary\n",
    "# clients = {\n",
    " \n",
    "#     'client_1': {\n",
    "#         'scaled_train': './dataset/client_1/client_1_scaled_train_dataset.csv', #unwanted features removed\n",
    "#         'scaled_test': './dataset/client_1/client_1_sclaed_test_dataset.csv', #unwanted features removed\n",
    "#         'pca_path': './dataset/client_1/pca/client_1_pca.pkl', #Saved for later use\n",
    "#         'pca_train': './dataset/client_1/pca/client_1_pca_train_dataset.csv', #unwanted features removed\n",
    "#         'pca_test': './dataset/client_1/pca/client_1_pca_test_dataset.csv', #unwanted features removed\n",
    "#     },\n",
    "#     'client_2': {\n",
    "#         'scaled_train': './dataset/client_2/client_2_scaled_train_dataset.csv', #unwanted features removed\n",
    "#         'scaled_test': './dataset/client_2/client_2_sclaed_test_dataset.csv', #unwanted features removed\n",
    "#         'pca_path': './dataset/client_2/pca/client_2_pca.pkl', #Saved for later use\n",
    "#         'pca_train': './dataset/client_2/pca/client_2_pca_train_dataset.csv', #unwanted features removed\n",
    "#         'pca_test': './dataset/client_2/pca/client_2_pca_test_dataset.csv', #unwanted features removed\n",
    "#     },\n",
    "#     'client_3': {\n",
    "#         'scaled_train': './dataset/client_3/client_3_scaled_train_dataset.csv', #unwanted features removed\n",
    "#         'scaled_test': './dataset/client_3/client_3_sclaed_test_dataset.csv', #unwanted features removed\n",
    "#         'pca_path': './dataset/client_3/pca/client_3_pca.pkl', #Saved for later use\n",
    "#         'pca_train': './dataset/client_3/pca/client_3_pca_train_dataset.csv', #unwanted features removed\n",
    "#         'pca_test': './dataset/client_3/pca/client_3_pca_test_dataset.csv', #unwanted features removed\n",
    "#     },\n",
    "#     'client_4': {\n",
    "#         'scaled_train': './dataset/client_4/client_4_scaled_train_dataset.csv', #unwanted features removed\n",
    "#         'scaled_test': './dataset/client_4/client_4_sclaed_test_dataset.csv', #unwanted features removed\n",
    "#         'pca_path': './dataset/client_4/pca/client_4_pca.pkl', #Saved for later use\n",
    "#         'pca_train': './dataset/client_4/pca/client_4_pca_train_dataset.csv', #unwanted features removed\n",
    "#         'pca_test': './dataset/client_4/pca/client_4_pca_test_dataset.csv', #unwanted features removed\n",
    "#     },\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Function to perform PCA with a maximum number of components and save the PCA object\n",
    "# def perform_pca_and_save_with_max(train_df, test_df, max_components, pca_path):\n",
    "#     # Remove 'Label' column before applying PCA\n",
    "#     train_labels = train_df['Label']\n",
    "#     test_labels = test_df['Label']\n",
    "#     train_features = train_df.drop(columns=['Label'])\n",
    "#     test_features = test_df.drop(columns=['Label'])\n",
    "\n",
    "#     # Initialize PCA with the maximum number of components\n",
    "#     pca = PCA(n_components=max_components)\n",
    "    \n",
    "#     # Fit PCA on the training set and transform\n",
    "#     train_pca_full = pca.fit_transform(train_features)\n",
    "    \n",
    "#     # Save the PCA model for future use\n",
    "#     with open(pca_path, \"wb\") as f:\n",
    "#         pickle.dump(pca, f)\n",
    "    \n",
    "#     print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    \n",
    "#     # Transform the test set using the same PCA model\n",
    "#     test_pca_full = pca.transform(test_features)\n",
    "    \n",
    "#     return train_pca_full, test_pca_full, train_labels, test_labels\n",
    "\n",
    "# # Step 2: Function to slice PCA components and add labels back to the dataset\n",
    "# def add_labels_to_pca(train_pca_full, test_pca_full, train_labels, test_labels, num_components):\n",
    "#     # Slice the top 'num_components' from the full PCA results\n",
    "#     train_pca_reduced = train_pca_full[:, :num_components]\n",
    "#     test_pca_reduced = test_pca_full[:, :num_components]\n",
    "    \n",
    "#     # Convert to DataFrame for easier handling\n",
    "#     train_pca_df = pd.DataFrame(train_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "#     test_pca_df = pd.DataFrame(test_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "    \n",
    "#     # Add the 'Label' column back\n",
    "#     train_pca_df['Label'] = train_labels.values\n",
    "#     test_pca_df['Label'] = test_labels.values\n",
    "    \n",
    "#     return train_pca_df, test_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_configurations = [\n",
    "    {\n",
    "        'n_components': 40,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "    {\n",
    "        'n_components': 35,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "    {\n",
    "        'n_components': 33,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "    {\n",
    "        'n_components': 30,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f{client} Starting....\n",
      "PCA explained variance ratio: [0.17908248 0.15599521 0.10360098 0.06265984 0.05702301 0.04744612\n",
      " 0.03931341 0.03542901 0.0312049  0.02462718 0.02393085 0.02360545\n",
      " 0.02267465 0.02070881 0.01815602 0.01624444 0.01496836 0.01323305\n",
      " 0.01270404 0.01132342 0.01050242 0.01010683 0.00901977 0.00840257\n",
      " 0.00798672 0.00691281 0.00577896 0.0051003  0.0035652  0.00234264\n",
      " 0.00217048 0.00202068 0.00176953 0.00147195 0.00125896 0.00112544\n",
      " 0.00104076 0.00085804 0.00071288 0.00059623]\n",
      "(335999, 41)\n",
      "(335999, 36)\n",
      "(335999, 34)\n",
      "(335999, 31)\n",
      "-----------------------------------------------------------------------\n",
      "f{client} Starting....\n",
      "PCA explained variance ratio: [0.17568192 0.12473658 0.09752492 0.07552202 0.06113587 0.04641158\n",
      " 0.04374342 0.03611841 0.03387959 0.03110569 0.02618503 0.02230072\n",
      " 0.02165586 0.02130531 0.01937251 0.01770442 0.01641674 0.01333521\n",
      " 0.01253844 0.01200517 0.01183246 0.01103053 0.01025419 0.00834167\n",
      " 0.00781151 0.00677549 0.00578028 0.005172   0.00495851 0.00346764\n",
      " 0.00267746 0.00220526 0.00159477 0.00141242 0.00128583 0.00115429\n",
      " 0.00069655 0.00066433 0.00061219 0.00050331]\n",
      "(336000, 41)\n",
      "(336000, 36)\n",
      "(336000, 34)\n",
      "(336000, 31)\n",
      "-----------------------------------------------------------------------\n",
      "f{client} Starting....\n",
      "PCA explained variance ratio: [0.17891667 0.14315922 0.09935361 0.07356371 0.05880952 0.04687952\n",
      " 0.04552997 0.04231053 0.03581232 0.02993947 0.02525125 0.02227518\n",
      " 0.02185979 0.01960318 0.01782566 0.01618198 0.0125651  0.0119683\n",
      " 0.01179574 0.01065376 0.00966354 0.00850855 0.00754503 0.00678815\n",
      " 0.00623966 0.00576979 0.00541763 0.00445798 0.00395761 0.00295314\n",
      " 0.00284247 0.00206224 0.00153527 0.0012264  0.00110188 0.00085907\n",
      " 0.00063567 0.00059057 0.00049992 0.000424  ]\n",
      "(336000, 41)\n",
      "(336000, 36)\n",
      "(336000, 34)\n",
      "(336000, 31)\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# for client, info in clients.items():\n",
    "#     print(\"f{client} Starting....\")\n",
    "#     train_df = pd.read_csv(info.get('scaled_train'))\n",
    "#     test_df = pd.read_csv(info.get('scaled_test'))\n",
    "#     # Step 3: Perform PCA with the maximum number of components (e.g., 40)\n",
    "#     train_pca_full, test_pca_full, train_labels, test_labels = perform_pca_and_save_with_max(train_df, test_df, max_components=40, pca_path=info.get(\"pca_path\"))\n",
    "    \n",
    "    \n",
    "#     # Step 4: Slice the PCA components and add labels back (e.g., for 40, 35, 33, 30 components)\n",
    "#     for p in pca_configurations:        \n",
    "#         train_pca_data, test_pca_data = add_labels_to_pca(train_pca_full, test_pca_full, train_labels, test_labels, num_components=p.get('n_components'))\n",
    "#         train_pca_data.to_csv(p.get('train_path').format(client, p.get('n_components'), p.get('n_components')),  index=False)\n",
    "#         test_pca_data.to_csv(p.get('test_path').format(client, p.get('n_components'), p.get('n_components')),  index=False)\n",
    "#         print(train_pca_data.shape)\n",
    "#     print(\"-----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # This is just the full version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
