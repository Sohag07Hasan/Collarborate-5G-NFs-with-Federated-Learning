{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overall DataSet Preparation from Row data\n",
    "- Read all the row feature files from the directory specified\n",
    "- Sanitize all the inputs\n",
    "- Take sample count based on attack and benign csv files (technically it's class)\n",
    "- Save the dataset locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_csv_files(directory_path, features=None):\n",
    "    # Initialize an empty list to store dataframes from CSV files\n",
    "    dataframes = []\n",
    "\n",
    "    # Get a list of all files in the directory\n",
    "    file_list = os.listdir(directory_path)\n",
    "\n",
    "    # Loop through each file and check if it's a CSV file\n",
    "    for file_number, file_name in enumerate(file_list):\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Get the full file path\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            # Read the CSV file into a pandas DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Remove leading and trailing spaces from column names\n",
    "            df.columns = df.columns.str.strip()\n",
    "            # Append the DataFrame to the list\n",
    "            df['CSV_File_Number'] = file_number\n",
    "            #dataframes.append(df[features])\n",
    "            dataframes.append(df) #as it is feature analysis, we are taking all the features\n",
    "\n",
    "    # Merge all DataFrames into a single DataFrame\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_data_frames_updated(dataframe, remove_infinity=True, remove_null=True):\n",
    "\n",
    "    if remove_infinity:\n",
    "        numeric_cols = dataframe.select_dtypes(include=[np.number]).columns\n",
    "        infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n",
    "        for col, count in infinite_counts.items():\n",
    "            if count != 0:\n",
    "                dataframe = dataframe[~np.isinf(dataframe[col])]\n",
    "\n",
    "    if remove_null:\n",
    "        null_counts = dataframe.isnull().sum()\n",
    "        for col, count in null_counts.items():\n",
    "            if count != 0:\n",
    "                    dataframe = dataframe.dropna(subset=[col])\n",
    "    print(\"Sanitized Row Count:\", dataframe.shape[0])    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all dataset sources to make iterate to read csv files\n",
    "dataset_sources = {\n",
    " \n",
    "    # 'client_1': {\n",
    "    #     'benign': '../row_data/client_1/benign',\n",
    "    #     'attack': '../row_data/client_1/attack',\n",
    "    # },\n",
    "    # 'client_2': {\n",
    "    #     'benign': '../row_data/client_2/benign',\n",
    "    #     'attack': '../row_data/client_2/attack',\n",
    "    # },\n",
    "    # 'client_3': {\n",
    "    #     'benign': '../row_data/client_3/benign',\n",
    "    #     'attack': '../row_data/client_3/attack',\n",
    "    # },\n",
    "    # 'client_4': {\n",
    "    #     'benign': '../row_data/client_4/benign',\n",
    "    #     'attack': '../row_data/client_4/attack',\n",
    "    #}\n",
    "    'client_5': {\n",
    "        'benign': '/home/sharedrive/PythonCodes/DND/DND_DATASET_ELENGEN/CiCDDoS2019/Benign',\n",
    "        'attack': '/home/sharedrive/PythonCodes/DND/DND_DATASET_ELENGEN/CiCDDoS2019/Attack',\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Uniform Sample\n",
    "def get_uniform_sample(df, group_col=[], sample_size=420000):\n",
    "    \n",
    "    #Separate the labels\n",
    "    label_0 = df[df['Label'] == 0]\n",
    "    label_1 = df[df['Label'] == 1]\n",
    "\n",
    "    count_0 = count_1 = 0 # Initialize with 0\n",
    "\n",
    "    if len(label_0) >= sample_size//2 and len(label_1) >= sample_size//2:\n",
    "        count_0 = count_1 = sample_size//2\n",
    "    elif len(label_0) < sample_size//2 and len(label_1) > sample_size//2:\n",
    "        count_0 = len(label_0)\n",
    "        count_1 = sample_size - count_0\n",
    "    elif len(label_0) > sample_size//2 and len(label_1) < sample_size//2:\n",
    "        count_1 = len(label_1)\n",
    "        count_0 = sample_size - count_1\n",
    "    else:\n",
    "        count_0 = len(label_0)\n",
    "        count_1 = len(label_1)\n",
    "\n",
    "    print(f\"count_0: {count_0}; count_1: {count_1}\")\n",
    "    print(f\"label_0: {len(label_0)}; label_1: {len(label_1)}\")\n",
    "\n",
    "    train_size_0 = 1.0 if count_0 >= len(label_0) else float(count_0/len(label_0))\n",
    "    train_size_1 = 1.0 if count_1 >= len(label_1) else float(count_1/len(label_1))\n",
    "\n",
    "    print(f\"train_size_0: {train_size_0}; train_size_1: {train_size_1}\")\n",
    "    \n",
    "# Handle edge cases where train_size is 1.0 and stratification might fail\n",
    "    if train_size_0 == 1.0:\n",
    "        sample_0 = label_0.copy()  # Use the whole subset without train_test_split\n",
    "    else:\n",
    "        sample_0, _ = train_test_split(\n",
    "            label_0,\n",
    "            train_size=train_size_0,\n",
    "            stratify=label_0['CSV_File_Number'] if group_col else None,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if train_size_1 == 1.0:\n",
    "        sample_1 = label_1.copy()  # Use the whole subset without train_test_split\n",
    "    else:\n",
    "        sample_1, _ = train_test_split(\n",
    "            label_1,\n",
    "            train_size=train_size_1,\n",
    "            stratify=label_1['CSV_File_Number'] if group_col else None,\n",
    "            random_state=42\n",
    "        )   \n",
    "    \n",
    "    # Concatenate and shuffle the samples\n",
    "    combined_sample = pd.concat([sample_0, sample_1])\n",
    "    shuffled_sample = combined_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    return shuffled_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory if it doesn't exist\n",
    "# THis will create subdirectoy as well\n",
    "def ensure_directory_exists(filepath):\n",
    "    directory = os.path.dirname(filepath)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 8405.42it/s]\n",
      "  0%|                                                                                                   | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_5, type = benign\n"
     ]
    }
   ],
   "source": [
    "##Reading all the data\n",
    "features = None\n",
    "\n",
    "\n",
    "for client, data in tqdm(dataset_sources.items(), total=len(dataset_sources)):\n",
    "    locals()[client] = {}\n",
    "\n",
    "for client, data in tqdm(dataset_sources.items(), total=len(dataset_sources)):\n",
    "    client_dataframe = []\n",
    "    for type, path in data.items():\n",
    "        print(f' Loading....Clinet = {client}, type = {type}')\n",
    "        dataframe = read_all_csv_files(path, features)\n",
    "        dataframe = sanitize_data_frames_updated(dataframe)\n",
    "        if type == 'benign':\n",
    "            dataframe['Label'] = 0\n",
    "        else:\n",
    "             dataframe['Label'] = 1\n",
    "        client_dataframe.append(dataframe)\n",
    "        #locals()[client][type] = dataframe\n",
    "    client_merged_df = get_uniform_sample(pd.concat(client_dataframe, ignore_index=True), ['Label', 'CSV_File_Number'], 420000 )\n",
    "    client_merged_df.to_csv(ensure_directory_exists(f'./dataset/{client}/{client}_original_dataset.csv'), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Train and Test Set Segregation\n",
    "- Read datasets saved in Step 1\n",
    "- Segregate Train and Test set and save locally. As we need to take decission based on csv file number and label, we will combine both column to stratify\n",
    "- Sacling Training data save the scaled data and save the scalers as well\n",
    "- Use saved scalers to scale the test data and save the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_exclude = ['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Timestamp', 'CSV_File_Number', 'Label', 'Stratify']\n",
    "output_features = ['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iteration Friendly dictionary\n",
    "clients = {\n",
    " \n",
    "    # 'client_1': {\n",
    "    #     'combined': './dataset/client_1/client_1_original_dataset.csv',\n",
    "    #     'train': './dataset/client_1/fold_{}/client_1_train_dataset.csv',\n",
    "    #     'test': './dataset/client_1/fold_{}/client_1_test_dataset.csv',\n",
    "    #     'scaler': './dataset/client_1/fold_{}/client_1_train_scaler.pkl',\n",
    "    #     'scaled_train': './dataset/client_1/fold_{}/client_1_scaled_train_dataset.csv', #unwanted features removed\n",
    "    #     'scaled_test': './dataset/client_1/fold_{}/client_1_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    # },\n",
    "    # 'client_2': {\n",
    "    #     'combined': './dataset/client_2/client_2_original_dataset.csv',\n",
    "    #     'train': './dataset/client_2/fold_{}/client_2_train_dataset.csv',\n",
    "    #     'test': './dataset/client_2/fold_{}/client_2_test_dataset.csv',\n",
    "    #     'scaler': './dataset/client_2/fold_{}/client_2_train_scaler.pkl',\n",
    "    #     'scaled_train': './dataset/client_2/fold_{}/client_2_scaled_train_dataset.csv', #unwanted features removed\n",
    "    #     'scaled_test': './dataset/client_2/fold_{}/client_2_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    # },\n",
    "    # 'client_3': {\n",
    "    #     'combined': './dataset/client_3/client_3_original_dataset.csv',\n",
    "    #     'train': './dataset/client_3/fold_{}/client_3_train_dataset.csv',\n",
    "    #     'test': './dataset/client_3/fold_{}/client_3_test_dataset.csv',\n",
    "    #     'scaler': './dataset/client_3/fold_{}/client_3_train_scaler.pkl',\n",
    "    #     'scaled_train': './dataset/client_3/fold_{}/client_3_scaled_train_dataset.csv', #unwanted features removed\n",
    "    #     'scaled_test': './dataset/client_3/fold_{}/client_3_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    # },\n",
    "    # 'client_4': {\n",
    "    #     'combined': './dataset/client_4/client_4_original_dataset.csv',\n",
    "    #     'train': './dataset/client_4/fold_{}/client_4_train_dataset.csv',\n",
    "    #     'test': './dataset/client_4/fold_{}/client_4_test_dataset.csv',\n",
    "    #     'scaler': './dataset/client_4/fold_{}/client_4_train_scaler.pkl',\n",
    "    #     'scaled_train': './dataset/client_4/fold_{}/client_4_scaled_train_dataset.csv', #unwanted features removed\n",
    "    #     'scaled_test': './dataset/client_4/fold_{}/client_4_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    # },\n",
    "     'client_5': {\n",
    "        'combined': './dataset/client_5/client_5_original_dataset.csv',\n",
    "        'train': './dataset/client_5/fold_{}/client_5_train_dataset.csv',\n",
    "        'test': './dataset/client_5/fold_{}/client_5_test_dataset.csv',\n",
    "        'scaler': './dataset/client_5/fold_{}/client_5_train_scaler.pkl',\n",
    "        'scaled_train': './dataset/client_5/fold_{}/client_5_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_5/fold_{}/client_5_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    }, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_remove_unwanted_features(dataframe, feature_to_exclude, out_features, scaler_path, type='train'):\n",
    "\n",
    "    features = dataframe.columns.values.tolist()\n",
    "    input_features = [feature for feature in features if feature not in feature_to_exclude] \n",
    "    output_df = dataframe[out_features]\n",
    "    input_df = dataframe[input_features]\n",
    "\n",
    "    print(f\"Input features shape: {input_df.shape}\")  # Debugging\n",
    "    print(f\"Output (label) features shape: {output_df.shape}\")  # Debugging\n",
    "\n",
    "    # Reset index for both input and output DataFrames to ensure correct alignment\n",
    "    input_df = input_df.reset_index(drop=True)\n",
    "    output_df = output_df.reset_index(drop=True)\n",
    "    \n",
    "    if type == 'train':\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(input_df)\n",
    "        scaled_df = pd.DataFrame(scaled_data, columns=input_df.columns)\n",
    "        print(f\"Scaled features shape: {scaled_df.shape}\")  # Debugging\n",
    "        \n",
    "        merged_df = pd.concat([scaled_df, output_df], axis=1)\n",
    "        print(f\"Merged DataFrame shape: {merged_df.shape}\")  # Debugging\n",
    "\n",
    "        # Save the scaler to a file to be used for test set\n",
    "        with open(scaler_path, \"wb\") as file:\n",
    "            pickle.dump(scaler, file)\n",
    "    else:\n",
    "        with open(scaler_path, \"rb\") as file:\n",
    "            scaler = pickle.load(file)\n",
    "            scaled_data = scaler.transform(input_df)  # Changed from fit_transform to transform\n",
    "            scaled_df = pd.DataFrame(scaled_data, columns=input_df.columns)\n",
    "            print(f\"Scaled features shape: {scaled_df.shape}\")  # Debugging\n",
    "            merged_df = pd.concat([scaled_df, output_df], axis=1)\n",
    "            print(f\"Merged DataFrame shape: {merged_df.shape}\")  # Debugging\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for client, info in tqdm(clients.items(), total=len(clients)):\n",
    "    print('------------------------------------------------------------------------------')\n",
    "    print(f\"Started for Client: {client}\")\n",
    "    df = pd.read_csv(info.get('combined'))\n",
    "    #combinding label and csv file number to stratify\n",
    "    df['Stratify'] = df['Label'].astype(str) + '_' + df['CSV_File_Number'].astype(str)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    #Loop through the splitted items:\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(df, df['Stratify']), 1):\n",
    "        print(f\"Started for Split: {fold}\")\n",
    "        # Create training and test sets for the current fold\n",
    "        train_df = df.iloc[train_index].reset_index(drop=True)\n",
    "        test_df = df.iloc[test_index].reset_index(drop=True)    \n",
    "\n",
    "        #checking if fold path exists. if not it will create one\n",
    "        #ensure_directory_exists(info.get('train').format(fold))\n",
    "        \n",
    "        train_df.to_csv(ensure_directory_exists(info.get('train').format(fold)), index=False) ##It will contain all column with additional column\n",
    "        test_df.to_csv(ensure_directory_exists(info.get('test').format(fold)), index=False) ##It will contain all column with additional column\n",
    "    \n",
    "        scaled_train_df = scale_and_remove_unwanted_features(train_df, feature_to_exclude, output_features, info.get('scaler').format(fold), 'train')\n",
    "        #scaled_train_df.describe()\n",
    "        scaled_train_df.to_csv(ensure_directory_exists(info.get('scaled_train')).format(fold), index=False)\n",
    "    \n",
    "        scaled_test_df = scale_and_remove_unwanted_features(test_df, feature_to_exclude, output_features, info.get('scaler').format(fold), 'test')\n",
    "        #scaled_test_df.describe()\n",
    "        scaled_test_df.to_csv(ensure_directory_exists(info.get('scaled_test').format(fold)), index=False)\n",
    "        print(f\"End for Client: {client}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Analysis of Prepared Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.concat(client_dataframe, ignore_index=True)\n",
    "intersted_features = ['Label', 'CSV_File_Number', 'Stratify']\n",
    "train_df_all_clients = []\n",
    "test_df_all_clients = []\n",
    "for client, info in clients.items():\n",
    "    print(f'Client: {client}')\n",
    "    for fold in range(1, 6):\n",
    "        train_df = pd.read_csv(info.get('train').format(fold))\n",
    "        test_df = pd.read_csv(info.get('test').format(fold))\n",
    "\n",
    "        train_df = train_df[intersted_features]\n",
    "        test_df = test_df[intersted_features]\n",
    "        \n",
    "        train_df['client'] = client\n",
    "        train_df['fold'] = fold\n",
    "        \n",
    "        test_df['client'] = client\n",
    "        test_df['fold'] = fold\n",
    "\n",
    "        train_df_all_clients.append(train_df)\n",
    "        test_df_all_clients.append(test_df)\n",
    "        \n",
    "merged_train_df = pd.concat(train_df_all_clients, ignore_index=True)\n",
    "merged_test_df = pd.concat(test_df_all_clients, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(merged_train_df.groupby(['client', 'fold', 'Stratify']).count())\n",
    "grouped_counts = merged_train_df.groupby(['client', 'fold', 'Stratify']).size().reset_index(name='count')\n",
    "# Pivoting the 'fold' column to make it into a separate column\n",
    "pivot_df = grouped_counts.pivot_table(index=['client', 'Stratify'], columns='fold', values='count', fill_value=0).reset_index()\n",
    "pivot_df.columns = ['Client', 'Stratify'] + [f'Fold_{int(col)}' for col in pivot_df.columns if isinstance(col, int)]\n",
    "pivot_df.to_csv(ensure_directory_exists(\"./dataset/dataset_summary.csv\"), index=False)\n",
    "print(pivot_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA dataset Preparation\n",
    "- Choose component of 30, 33, 35 and generate datasets accordingly for training dataset and store locally\n",
    "- Use PCA matrix to convert test dataset and store locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iteration Friendly dictionary\n",
    "clients = {\n",
    " \n",
    "    'client_1': {\n",
    "        'scaled_train': './dataset/client_1/fold_{}/client_1_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_1/fold_{}/client_1_sclaed_test_dataset.csv', #unwanted features removed\n",
    "        'pca_path': './dataset/client_1/fold_{}/pca/client_1_pca.pkl', #Saved for later use\n",
    "        'pca_train': './dataset/client_1/fold_{}/pca/client_1_pca_train_dataset.csv', #unwanted features removed\n",
    "        'pca_test': './dataset/client_1/fold_{}/pca/client_1_pca_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_2': {\n",
    "        'scaled_train': './dataset/client_2/fold_{}/client_2_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_2/fold_{}/client_2_sclaed_test_dataset.csv', #unwanted features removed\n",
    "        'pca_path': './dataset/client_2/fold_{}/pca/client_2_pca.pkl', #Saved for later use\n",
    "        'pca_train': './dataset/client_2/fold_{}/pca/client_2_pca_train_dataset.csv', #unwanted features removed\n",
    "        'pca_test': './dataset/client_2/fold_{}/pca/client_2_pca_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_3': {\n",
    "        'scaled_train': './dataset/client_3/fold_{}/client_3_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_3/fold_{}/client_3_sclaed_test_dataset.csv', #unwanted features removed\n",
    "        'pca_path': './dataset/client_3/fold_{}/pca/client_3_pca.pkl', #Saved for later use\n",
    "        'pca_train': './dataset/client_3/fold_{}/pca/client_3_pca_train_dataset.csv', #unwanted features removed\n",
    "        'pca_test': './dataset/client_3/fold_{}/pca/client_3_pca_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_4': {\n",
    "        'scaled_train': './dataset/client_4/fold_{}/client_4_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_4/fold_{}/client_4_sclaed_test_dataset.csv', #unwanted features removed\n",
    "        'pca_path': './dataset/client_4/fold_{}/pca/client_4_pca.pkl', #Saved for later use\n",
    "        'pca_train': './dataset/client_4/fold_{}/pca/client_4_pca_train_dataset.csv', #unwanted features removed\n",
    "        'pca_test': './dataset/client_4/fold_{}/pca/client_4_pca_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Function to perform PCA with a maximum number of components and save the PCA object\n",
    "def perform_pca_and_save_with_max(train_df, test_df, max_components, pca_path):\n",
    "    # Remove 'Label' column before applying PCA\n",
    "    train_labels = train_df['Label']\n",
    "    test_labels = test_df['Label']\n",
    "    train_features = train_df.drop(columns=['Label'])\n",
    "    test_features = test_df.drop(columns=['Label'])\n",
    "\n",
    "    # Initialize PCA with the maximum number of components\n",
    "    pca = PCA(n_components=max_components)\n",
    "    \n",
    "    # Fit PCA on the training set and transform\n",
    "    train_pca_full = pca.fit_transform(train_features)\n",
    "\n",
    "    ##Ensure direcoty exists. if not it will create one\n",
    "    ensure_directory_exists(pca_path)\n",
    "    \n",
    "    # Save the PCA model for future use\n",
    "    with open(pca_path, \"wb\") as f:\n",
    "        pickle.dump(pca, f)\n",
    "    \n",
    "    #print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    \n",
    "    # Transform the test set using the same PCA model\n",
    "    test_pca_full = pca.transform(test_features)\n",
    "    \n",
    "    return train_pca_full, test_pca_full, train_labels, test_labels\n",
    "\n",
    "# Step 2: Function to slice PCA components and add labels back to the dataset\n",
    "def add_labels_to_pca(train_pca_full, test_pca_full, train_labels, test_labels, num_components):\n",
    "    # Slice the top 'num_components' from the full PCA results\n",
    "    train_pca_reduced = train_pca_full[:, :num_components]\n",
    "    test_pca_reduced = test_pca_full[:, :num_components]\n",
    "    \n",
    "    # Convert to DataFrame for easier handling\n",
    "    train_pca_df = pd.DataFrame(train_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "    test_pca_df = pd.DataFrame(test_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "    \n",
    "    # Add the 'Label' column back\n",
    "    train_pca_df['Label'] = train_labels.values\n",
    "    test_pca_df['Label'] = test_labels.values\n",
    "    \n",
    "    return train_pca_df, test_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pca_components = 40 # This number chosen based on pca analysis. check PCA.ipynb\n",
    "\n",
    "for client, info in clients.items():\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    for fold in range(1, 6):\n",
    "        print(f'Client: {client} ... Fold: {fold}')\n",
    "        train_df = pd.read_csv(info.get('scaled_train').format(fold))\n",
    "        test_df = pd.read_csv(info.get('scaled_test').format(fold))\n",
    "    \n",
    "        # Step 3: Perform PCA with the maximum number of components (e.g., 40)\n",
    "        train_pca_full, test_pca_full, train_labels, test_labels = perform_pca_and_save_with_max(train_df, test_df, max_components=max_pca_components, pca_path=info.get(\"pca_path\").format(fold))      \n",
    "        train_pca_data, test_pca_data = add_labels_to_pca(train_pca_full, test_pca_full, train_labels, test_labels, num_components=max_pca_components)\n",
    "        \n",
    "        train_pca_data.to_csv(info.get('pca_train').format(fold),  index=False)\n",
    "        test_pca_data.to_csv(info.get('pca_test').format(fold),  index=False)\n",
    "\n",
    "    print(\"-----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not in USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Iteration Friendly dictionary\n",
    "# clients = {\n",
    " \n",
    "#     'client_1': {\n",
    "#         'scaled_train': './dataset/client_1/client_1_scaled_train_dataset.csv', #unwanted features removed\n",
    "#         'scaled_test': './dataset/client_1/client_1_sclaed_test_dataset.csv', #unwanted features removed\n",
    "#         'pca_path': './dataset/client_1/pca/client_1_pca.pkl', #Saved for later use\n",
    "#         'pca_train': './dataset/client_1/pca/client_1_pca_train_dataset.csv', #unwanted features removed\n",
    "#         'pca_test': './dataset/client_1/pca/client_1_pca_test_dataset.csv', #unwanted features removed\n",
    "#     },\n",
    "#     'client_2': {\n",
    "#         'scaled_train': './dataset/client_2/client_2_scaled_train_dataset.csv', #unwanted features removed\n",
    "#         'scaled_test': './dataset/client_2/client_2_sclaed_test_dataset.csv', #unwanted features removed\n",
    "#         'pca_path': './dataset/client_2/pca/client_2_pca.pkl', #Saved for later use\n",
    "#         'pca_train': './dataset/client_2/pca/client_2_pca_train_dataset.csv', #unwanted features removed\n",
    "#         'pca_test': './dataset/client_2/pca/client_2_pca_test_dataset.csv', #unwanted features removed\n",
    "#     },\n",
    "#     'client_3': {\n",
    "#         'scaled_train': './dataset/client_3/client_3_scaled_train_dataset.csv', #unwanted features removed\n",
    "#         'scaled_test': './dataset/client_3/client_3_sclaed_test_dataset.csv', #unwanted features removed\n",
    "#         'pca_path': './dataset/client_3/pca/client_3_pca.pkl', #Saved for later use\n",
    "#         'pca_train': './dataset/client_3/pca/client_3_pca_train_dataset.csv', #unwanted features removed\n",
    "#         'pca_test': './dataset/client_3/pca/client_3_pca_test_dataset.csv', #unwanted features removed\n",
    "#     },\n",
    "#     'client_4': {\n",
    "#         'scaled_train': './dataset/client_4/client_4_scaled_train_dataset.csv', #unwanted features removed\n",
    "#         'scaled_test': './dataset/client_4/client_4_sclaed_test_dataset.csv', #unwanted features removed\n",
    "#         'pca_path': './dataset/client_4/pca/client_4_pca.pkl', #Saved for later use\n",
    "#         'pca_train': './dataset/client_4/pca/client_4_pca_train_dataset.csv', #unwanted features removed\n",
    "#         'pca_test': './dataset/client_4/pca/client_4_pca_test_dataset.csv', #unwanted features removed\n",
    "#     },\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Function to perform PCA with a maximum number of components and save the PCA object\n",
    "# def perform_pca_and_save_with_max(train_df, test_df, max_components, pca_path):\n",
    "#     # Remove 'Label' column before applying PCA\n",
    "#     train_labels = train_df['Label']\n",
    "#     test_labels = test_df['Label']\n",
    "#     train_features = train_df.drop(columns=['Label'])\n",
    "#     test_features = test_df.drop(columns=['Label'])\n",
    "\n",
    "#     # Initialize PCA with the maximum number of components\n",
    "#     pca = PCA(n_components=max_components)\n",
    "    \n",
    "#     # Fit PCA on the training set and transform\n",
    "#     train_pca_full = pca.fit_transform(train_features)\n",
    "    \n",
    "#     # Save the PCA model for future use\n",
    "#     with open(pca_path, \"wb\") as f:\n",
    "#         pickle.dump(pca, f)\n",
    "    \n",
    "#     print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    \n",
    "#     # Transform the test set using the same PCA model\n",
    "#     test_pca_full = pca.transform(test_features)\n",
    "    \n",
    "#     return train_pca_full, test_pca_full, train_labels, test_labels\n",
    "\n",
    "# # Step 2: Function to slice PCA components and add labels back to the dataset\n",
    "# def add_labels_to_pca(train_pca_full, test_pca_full, train_labels, test_labels, num_components):\n",
    "#     # Slice the top 'num_components' from the full PCA results\n",
    "#     train_pca_reduced = train_pca_full[:, :num_components]\n",
    "#     test_pca_reduced = test_pca_full[:, :num_components]\n",
    "    \n",
    "#     # Convert to DataFrame for easier handling\n",
    "#     train_pca_df = pd.DataFrame(train_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "#     test_pca_df = pd.DataFrame(test_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "    \n",
    "#     # Add the 'Label' column back\n",
    "#     train_pca_df['Label'] = train_labels.values\n",
    "#     test_pca_df['Label'] = test_labels.values\n",
    "    \n",
    "#     return train_pca_df, test_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_configurations = [\n",
    "    {\n",
    "        'n_components': 40,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "    {\n",
    "        'n_components': 35,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "    {\n",
    "        'n_components': 33,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "    {\n",
    "        'n_components': 30,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for client, info in clients.items():\n",
    "#     print(\"f{client} Starting....\")\n",
    "#     train_df = pd.read_csv(info.get('scaled_train'))\n",
    "#     test_df = pd.read_csv(info.get('scaled_test'))\n",
    "#     # Step 3: Perform PCA with the maximum number of components (e.g., 40)\n",
    "#     train_pca_full, test_pca_full, train_labels, test_labels = perform_pca_and_save_with_max(train_df, test_df, max_components=40, pca_path=info.get(\"pca_path\"))\n",
    "    \n",
    "    \n",
    "#     # Step 4: Slice the PCA components and add labels back (e.g., for 40, 35, 33, 30 components)\n",
    "#     for p in pca_configurations:        \n",
    "#         train_pca_data, test_pca_data = add_labels_to_pca(train_pca_full, test_pca_full, train_labels, test_labels, num_components=p.get('n_components'))\n",
    "#         train_pca_data.to_csv(p.get('train_path').format(client, p.get('n_components'), p.get('n_components')),  index=False)\n",
    "#         test_pca_data.to_csv(p.get('test_path').format(client, p.get('n_components'), p.get('n_components')),  index=False)\n",
    "#         print(train_pca_data.shape)\n",
    "#     print(\"-----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # This is just the full version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
