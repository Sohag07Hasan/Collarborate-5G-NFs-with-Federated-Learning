{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overall DataSet Preparation from Row data\n",
    "- Read all the row feature files from the directory specified\n",
    "- Sanitize all the inputs\n",
    "- Take sample count based on attack and benign csv files (technically it's class)\n",
    "- Save the dataset locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_csv_files(directory_path, features=None):\n",
    "    # Initialize an empty list to store dataframes from CSV files\n",
    "    dataframes = []\n",
    "\n",
    "    # Get a list of all files in the directory\n",
    "    file_list = os.listdir(directory_path)\n",
    "\n",
    "    # Loop through each file and check if it's a CSV file\n",
    "    for file_number, file_name in enumerate(file_list):\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Get the full file path\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            # Read the CSV file into a pandas DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Remove leading and trailing spaces from column names\n",
    "            df.columns = df.columns.str.strip()\n",
    "            # Append the DataFrame to the list\n",
    "            df['CSV_File_Number'] = file_number\n",
    "            #dataframes.append(df[features])\n",
    "            dataframes.append(df) #as it is feature analysis, we are taking all the features\n",
    "\n",
    "    # Merge all DataFrames into a single DataFrame\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_data_frames_updated(dataframe, remove_infinity=True, remove_null=True):\n",
    "\n",
    "    if remove_infinity:\n",
    "        numeric_cols = dataframe.select_dtypes(include=[np.number]).columns\n",
    "        infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n",
    "        for col, count in infinite_counts.items():\n",
    "            if count != 0:\n",
    "                dataframe = dataframe[~np.isinf(dataframe[col])]\n",
    "\n",
    "    if remove_null:\n",
    "        null_counts = dataframe.isnull().sum()\n",
    "        for col, count in null_counts.items():\n",
    "            if count != 0:\n",
    "                    dataframe = dataframe.dropna(subset=[col])\n",
    "    print(\"Sanitized Row Count:\", dataframe.shape[0])    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all dataset sources to make iterate to read csv files\n",
    "dataset_sources = {\n",
    " \n",
    "    'client_1': {\n",
    "        'benign': '../row_data/client_1/benign',\n",
    "        'attack': '../row_data/client_1/attack',\n",
    "    },\n",
    "    'client_2': {\n",
    "        'benign': '../row_data/client_2/benign',\n",
    "        'attack': '../row_data/client_2/attack',\n",
    "    },\n",
    "    'client_3': {\n",
    "        'benign': '../row_data/client_3/benign',\n",
    "        'attack': '../row_data/client_3/attack',\n",
    "    },\n",
    "    'client_4': {\n",
    "        'benign': '../row_data/client_4/benign',\n",
    "        'attack': '../row_data/client_4/attack',\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Uniform Sample\n",
    "def get_uniform_sample(df, group_col=[], sample_size=420000):\n",
    "    \n",
    "    #Separate the labels\n",
    "    label_0 = df[df['Label'] == 0]\n",
    "    label_1 = df[df['Label'] == 1]\n",
    "\n",
    "    count_0 = count_1 = 0 # Initialize with 0\n",
    "\n",
    "    if len(label_0) >= sample_size//2 and len(label_1) >= sample_size//2:\n",
    "        count_0 = count_1 = sample_size//2\n",
    "    elif len(label_0) < sample_size//2 and len(label_1) > sample_size//2:\n",
    "        count_0 = len(label_0)\n",
    "        count_1 = sample_size - count_0\n",
    "    elif len(label_0) > sample_size//2 and len(label_1) < sample_size//2:\n",
    "        count_1 = len(label_1)\n",
    "        count_0 = sample_size - count_1\n",
    "    else:\n",
    "        count_0 = len(label_0)\n",
    "        count_1 = len(label_1)\n",
    "\n",
    "    print(f\"count_0: {count_0}; count_1: {count_1}\")\n",
    "    print(f\"label_0: {len(label_0)}; label_1: {len(label_1)}\")\n",
    "\n",
    "    train_size_0 = 1.0 if count_0 >= len(label_0) else float(count_0/len(label_0))\n",
    "    train_size_1 = 1.0 if count_1 >= len(label_1) else float(count_1/len(label_1))\n",
    "\n",
    "    print(f\"train_size_0: {train_size_0}; train_size_1: {train_size_1}\")\n",
    "    \n",
    "# Handle edge cases where train_size is 1.0 and stratification might fail\n",
    "    if train_size_0 == 1.0:\n",
    "        sample_0 = label_0.copy()  # Use the whole subset without train_test_split\n",
    "    else:\n",
    "        sample_0, _ = train_test_split(\n",
    "            label_0,\n",
    "            train_size=train_size_0,\n",
    "            stratify=label_0['CSV_File_Number'] if group_col else None,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if train_size_1 == 1.0:\n",
    "        sample_1 = label_1.copy()  # Use the whole subset without train_test_split\n",
    "    else:\n",
    "        sample_1, _ = train_test_split(\n",
    "            label_1,\n",
    "            train_size=train_size_1,\n",
    "            stratify=label_1['CSV_File_Number'] if group_col else None,\n",
    "            random_state=42\n",
    "        )   \n",
    "    \n",
    "    # Concatenate and shuffle the samples\n",
    "    combined_sample = pd.concat([sample_0, sample_1])\n",
    "    shuffled_sample = combined_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    return shuffled_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 54648.91it/s]\n",
      "  0%|                                                                                                   | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_1, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_953695/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 202821\n",
      " Loading....Clinet = client_1, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_953695/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 2671528\n",
      "count_0: 202821; count_1: 217179\n",
      "label_0: 202821; label_1: 2671528\n",
      "train_size_0: 1.0; train_size_1: 0.08129392617258738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████████████▌                                                                   | 1/4 [03:47<11:21, 227.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_2, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_953695/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 185943\n",
      " Loading....Clinet = client_2, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_953695/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 3346296\n",
      "count_0: 185943; count_1: 234057\n",
      "label_0: 185943; label_1: 3346296\n",
      "train_size_0: 1.0; train_size_1: 0.06994509750482324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████                                             | 2/4 [08:17<08:24, 252.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_3, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_953695/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 223200\n",
      " Loading....Clinet = client_3, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_953695/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 2348465\n",
      "count_0: 210000; count_1: 210000\n",
      "label_0: 223200; label_1: 2348465\n",
      "train_size_0: 0.9408602150537635; train_size_1: 0.08942011058287008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████████▌                      | 3/4 [11:37<03:48, 228.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_4, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_953695/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 246797\n",
      " Loading....Clinet = client_4, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_953695/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 2253978\n",
      "count_0: 210000; count_1: 210000\n",
      "label_0: 246797; label_1: 2253978\n",
      "train_size_0: 0.850901753262803; train_size_1: 0.09316861122868103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 4/4 [14:53<00:00, 223.42s/it]\n"
     ]
    }
   ],
   "source": [
    "##Reading all the data\n",
    "features = None\n",
    "\n",
    "\n",
    "for client, data in tqdm(dataset_sources.items(), total=len(dataset_sources)):\n",
    "    locals()[client] = {}\n",
    "\n",
    "for client, data in tqdm(dataset_sources.items(), total=len(dataset_sources)):\n",
    "    client_dataframe = []\n",
    "    for type, path in data.items():\n",
    "        print(f' Loading....Clinet = {client}, type = {type}')\n",
    "        dataframe = read_all_csv_files(path, features)\n",
    "        dataframe = sanitize_data_frames_updated(dataframe)\n",
    "        if type == 'benign':\n",
    "            dataframe['Label'] = 0\n",
    "        else:\n",
    "             dataframe['Label'] = 1\n",
    "        client_dataframe.append(dataframe)\n",
    "        #locals()[client][type] = dataframe\n",
    "    client_merged_df = get_uniform_sample(pd.concat(client_dataframe, ignore_index=True), ['Label', 'CSV_File_Number'], 420000 )\n",
    "    client_merged_df.to_csv(f'./dataset/{client}/{client}_original_dataset.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Train and Test Set Segregation\n",
    "- Read datasets saved in Step 1\n",
    "- Segregate Train and Test set and save locally. As we need to take decission based on csv file number and label, we will combine both column to stratify\n",
    "- Sacling Training data save the scaled data and save the scalers as well\n",
    "- Use saved scalers to scale the test data and save the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_exclude = ['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Timestamp', 'CSV_File_Number', 'Label', 'Stratify']\n",
    "output_features = ['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iteration Friendly dictionary\n",
    "clients = {\n",
    " \n",
    "    'client_1': {\n",
    "        'combined': './dataset/client_1/client_1_original_dataset.csv',\n",
    "        'train': './dataset/client_1/fold_{}/client_1_train_dataset.csv',\n",
    "        'test': './dataset/client_1/fold_{}/client_1_test_dataset.csv',\n",
    "        'scaler': './dataset/client_1/fold_{}/client_1_train_scaler.pkl',\n",
    "        'scaled_train': './dataset/client_1/fold_{}/client_1_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_1/fold_{}/client_1_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_2': {\n",
    "        'combined': './dataset/client_2/client_2_original_dataset.csv',\n",
    "        'train': './dataset/client_2/fold_{}/client_2_train_dataset.csv',\n",
    "        'test': './dataset/client_2/fold_{}/client_2_test_dataset.csv',\n",
    "        'scaler': './dataset/client_2/fold_{}/client_2_train_scaler.pkl',\n",
    "        'scaled_train': './dataset/client_2/fold_{}/client_2_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_2/fold_{}/client_2_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_3': {\n",
    "        'combined': './dataset/client_3/client_3_original_dataset.csv',\n",
    "        'train': './dataset/client_3/fold_{}/client_3_train_dataset.csv',\n",
    "        'test': './dataset/client_3/fold_{}/client_3_test_dataset.csv',\n",
    "        'scaler': './dataset/client_3/fold_{}/client_3_train_scaler.pkl',\n",
    "        'scaled_train': './dataset/client_3/fold_{}/client_3_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_3/fold_{}/client_3_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_4': {\n",
    "        'combined': './dataset/client_4/client_4_original_dataset.csv',\n",
    "        'train': './dataset/client_4/fold_{}/client_4_train_dataset.csv',\n",
    "        'test': './dataset/client_4/fold_{}/client_4_test_dataset.csv',\n",
    "        'scaler': './dataset/client_4/fold_{}/client_4_train_scaler.pkl',\n",
    "        'scaled_train': './dataset/client_4/fold_{}/client_4_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_4/fold_{}/client_4_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    },    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_remove_unwanted_features(dataframe, feature_to_exclude, out_features, scaler_path, type='train'):\n",
    "\n",
    "    features = dataframe.columns.values.tolist()\n",
    "    input_features = [feature for feature in features if feature not in feature_to_exclude] \n",
    "    output_df = dataframe[out_features]\n",
    "    input_df = dataframe[input_features]\n",
    "\n",
    "    print(f\"Input features shape: {input_df.shape}\")  # Debugging\n",
    "    print(f\"Output (label) features shape: {output_df.shape}\")  # Debugging\n",
    "\n",
    "    # Reset index for both input and output DataFrames to ensure correct alignment\n",
    "    input_df = input_df.reset_index(drop=True)\n",
    "    output_df = output_df.reset_index(drop=True)\n",
    "    \n",
    "    if type == 'train':\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(input_df)\n",
    "        scaled_df = pd.DataFrame(scaled_data, columns=input_df.columns)\n",
    "        print(f\"Scaled features shape: {scaled_df.shape}\")  # Debugging\n",
    "        \n",
    "        merged_df = pd.concat([scaled_df, output_df], axis=1)\n",
    "        print(f\"Merged DataFrame shape: {merged_df.shape}\")  # Debugging\n",
    "\n",
    "        # Save the scaler to a file to be used for test set\n",
    "        with open(scaler_path, \"wb\") as file:\n",
    "            pickle.dump(scaler, file)\n",
    "    else:\n",
    "        with open(scaler_path, \"rb\") as file:\n",
    "            scaler = pickle.load(file)\n",
    "            scaled_data = scaler.transform(input_df)  # Changed from fit_transform to transform\n",
    "            scaled_df = pd.DataFrame(scaled_data, columns=input_df.columns)\n",
    "            print(f\"Scaled features shape: {scaled_df.shape}\")  # Debugging\n",
    "            merged_df = pd.concat([scaled_df, output_df], axis=1)\n",
    "            print(f\"Merged DataFrame shape: {merged_df.shape}\")  # Debugging\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory if it doesn't exist\n",
    "def ensure_directory_exists(filepath):\n",
    "    directory = os.path.dirname(filepath)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                   | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "Started for Client: client_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started for Split: 1\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_1\n",
      "Started for Split: 2\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_1\n",
      "Started for Split: 3\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_1\n",
      "Started for Split: 4\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_1\n",
      "Started for Split: 5\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████▌                                           | 1/4 [03:57<11:52, 237.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End for Client: client_1\n",
      "------------------------------------------------------------------------------\n",
      "Started for Client: client_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started for Split: 1\n",
      "Input features shape: (335999, 85)\n",
      "Output (label) features shape: (335999, 1)\n",
      "Scaled features shape: (335999, 85)\n",
      "Merged DataFrame shape: (335999, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_2\n",
      "Started for Split: 2\n",
      "Input features shape: (335999, 85)\n",
      "Output (label) features shape: (335999, 1)\n",
      "Scaled features shape: (335999, 85)\n",
      "Merged DataFrame shape: (335999, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_2\n",
      "Started for Split: 3\n",
      "Input features shape: (335999, 85)\n",
      "Output (label) features shape: (335999, 1)\n",
      "Scaled features shape: (335999, 85)\n",
      "Merged DataFrame shape: (335999, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_2\n",
      "Started for Split: 4\n",
      "Input features shape: (335999, 85)\n",
      "Output (label) features shape: (335999, 1)\n",
      "Scaled features shape: (335999, 85)\n",
      "Merged DataFrame shape: (335999, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_2\n",
      "Started for Split: 5\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (83999, 85)\n",
      "Output (label) features shape: (83999, 1)\n",
      "Scaled features shape: (83999, 85)\n",
      "Merged DataFrame shape: (83999, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████                             | 2/4 [07:59<07:59, 239.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End for Client: client_2\n",
      "------------------------------------------------------------------------------\n",
      "Started for Client: client_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started for Split: 1\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_3\n",
      "Started for Split: 2\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_3\n",
      "Started for Split: 3\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_3\n",
      "Started for Split: 4\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_3\n",
      "Started for Split: 5\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████▌              | 3/4 [12:03<04:02, 242.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End for Client: client_3\n",
      "------------------------------------------------------------------------------\n",
      "Started for Client: client_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started for Split: 1\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_4\n",
      "Started for Split: 2\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_4\n",
      "Started for Split: 3\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_4\n",
      "Started for Split: 4\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_4\n",
      "Started for Split: 5\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 4/4 [16:07<00:00, 241.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End for Client: client_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for client, info in tqdm(clients.items(), total=len(clients)):\n",
    "    print('------------------------------------------------------------------------------')\n",
    "    print(f\"Started for Client: {client}\")\n",
    "    df = pd.read_csv(info.get('combined'))\n",
    "    #combinding label and csv file number to stratify\n",
    "    df['Stratify'] = df['Label'].astype(str) + '_' + df['CSV_File_Number'].astype(str)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    #Loop through the splitted items:\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(df, df['Stratify']), 1):\n",
    "        print(f\"Started for Split: {fold}\")\n",
    "        # Create training and test sets for the current fold\n",
    "        train_df = df.iloc[train_index].reset_index(drop=True)\n",
    "        test_df = df.iloc[test_index].reset_index(drop=True)    \n",
    "\n",
    "        #checking if fold path exists. if not it will create one\n",
    "        ensure_directory_exists(info.get('train').format(fold))\n",
    "        \n",
    "        train_df.to_csv(info.get('train').format(fold), index=False) ##It will contain all column with additional column\n",
    "        test_df.to_csv(info.get('test').format(fold), index=False)  ##It will contain all column with additional column\n",
    "    \n",
    "        scaled_train_df = scale_and_remove_unwanted_features(train_df, feature_to_exclude, output_features, info.get('scaler').format(fold), 'train')\n",
    "        #scaled_train_df.describe()\n",
    "        scaled_train_df.to_csv(info.get('scaled_train').format(fold), index=False)\n",
    "    \n",
    "        scaled_test_df = scale_and_remove_unwanted_features(test_df, feature_to_exclude, output_features, info.get('scaler').format(fold), 'test')\n",
    "        #scaled_test_df.describe()\n",
    "        scaled_test_df.to_csv(info.get('scaled_test').format(fold), index=False)\n",
    "        print(f\"End for Client: {client}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Analysis of Prepared Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Client: client_1\n",
      "-----------------------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Client: client_2\n",
      "-----------------------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Client: client_3\n",
      "-----------------------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Client: client_4\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#pd.concat(client_dataframe, ignore_index=True)\n",
    "intersted_features = ['Label', 'CSV_File_Number', 'Stratify']\n",
    "train_df_all_clients = []\n",
    "test_df_all_clients = []\n",
    "for client, info in clients.items():\n",
    "    print(f'Client: {client}')\n",
    "    for fold in range(1, 6):\n",
    "        train_df = pd.read_csv(info.get('train').format(fold))\n",
    "        test_df = pd.read_csv(info.get('test').format(fold))\n",
    "\n",
    "        train_df = train_df[intersted_features]\n",
    "        test_df = test_df[intersted_features]\n",
    "        \n",
    "        train_df['client'] = client\n",
    "        train_df['fold'] = fold\n",
    "        \n",
    "        test_df['client'] = client\n",
    "        test_df['fold'] = fold\n",
    "\n",
    "        train_df_all_clients.append(train_df)\n",
    "        test_df_all_clients.append(test_df)\n",
    "        \n",
    "merged_train_df = pd.concat(train_df_all_clients, ignore_index=True)\n",
    "merged_test_df = pd.concat(test_df_all_clients, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Client Stratify   Fold_1   Fold_2   Fold_3   Fold_4   Fold_5\n",
      "0   client_1      0_0  82051.0  82051.0  82051.0  82051.0  82052.0\n",
      "1   client_1      0_1  41618.0  41618.0  41618.0  41619.0  41619.0\n",
      "2   client_1      0_2      3.0      3.0      2.0      2.0      2.0\n",
      "3   client_1      0_3  38585.0  38585.0  38585.0  38584.0  38585.0\n",
      "4   client_1      1_1  17049.0  17049.0  17049.0  17049.0  17048.0\n",
      "5   client_1     1_10  17048.0  17048.0  17048.0  17048.0  17048.0\n",
      "6   client_1      1_2  17048.0  17049.0  17049.0  17049.0  17049.0\n",
      "7   client_1      1_4  17049.0  17049.0  17049.0  17049.0  17048.0\n",
      "8   client_1      1_5  17049.0  17048.0  17049.0  17049.0  17049.0\n",
      "9   client_1      1_6  37354.0  37354.0  37354.0  37355.0  37355.0\n",
      "10  client_1      1_7  17048.0  17048.0  17048.0  17048.0  17048.0\n",
      "11  client_1      1_8  17049.0  17049.0  17049.0  17048.0  17049.0\n",
      "12  client_1      1_9  17049.0  17049.0  17049.0  17049.0  17048.0\n",
      "13  client_2      0_0  46683.0  46683.0  46682.0  46682.0  46682.0\n",
      "14  client_2      0_1      3.0      2.0      2.0      2.0      3.0\n",
      "15  client_2      0_2  83134.0  83134.0  83134.0  83135.0  83135.0\n",
      "16  client_2      0_3  18935.0  18936.0  18935.0  18935.0  18935.0\n",
      "17  client_2      1_0  47547.0  47547.0  47548.0  47547.0  47547.0\n",
      "18  client_2      1_1  14669.0  14669.0  14668.0  14669.0  14669.0\n",
      "19  client_2      1_2   4915.0   4915.0   4916.0   4915.0   4915.0\n",
      "20  client_2      1_3  14669.0  14669.0  14669.0  14668.0  14669.0\n",
      "21  client_2      1_4  14668.0  14669.0  14669.0  14669.0  14669.0\n",
      "22  client_2      1_5  14668.0  14668.0  14668.0  14668.0  14668.0\n",
      "23  client_2      1_6  14668.0  14668.0  14668.0  14668.0  14668.0\n",
      "24  client_2      1_7  14669.0  14669.0  14668.0  14669.0  14669.0\n",
      "25  client_2      1_8  14669.0  14668.0  14669.0  14669.0  14669.0\n",
      "26  client_2      1_9  32102.0  32102.0  32103.0  32103.0  32102.0\n",
      "27  client_3      0_0  37895.0  37895.0  37895.0  37895.0  37896.0\n",
      "28  client_3      0_1  41041.0  41041.0  41040.0  41041.0  41041.0\n",
      "29  client_3      0_2     17.0     17.0     18.0     18.0     18.0\n",
      "30  client_3      0_3      9.0      9.0      9.0      9.0      8.0\n",
      "31  client_3      0_4  14656.0  14657.0  14657.0  14657.0  14657.0\n",
      "32  client_3      0_5     19.0     19.0     18.0     18.0     18.0\n",
      "33  client_3      0_6   3043.0   3043.0   3042.0   3042.0   3042.0\n",
      "34  client_3      0_7      3.0      3.0      3.0      3.0      4.0\n",
      "35  client_3      0_8  71317.0  71317.0  71317.0  71317.0  71316.0\n",
      "36  client_3      1_0  36731.0  36730.0  36730.0  36730.0  36731.0\n",
      "37  client_3      1_1  18752.0  18753.0  18753.0  18753.0  18753.0\n",
      "38  client_3      1_2  18753.0  18752.0  18753.0  18753.0  18753.0\n",
      "39  client_3      1_3  18753.0  18753.0  18753.0  18752.0  18753.0\n",
      "40  client_3      1_4  18753.0  18752.0  18753.0  18753.0  18753.0\n",
      "41  client_3      1_5  18753.0  18753.0  18753.0  18753.0  18752.0\n",
      "42  client_3      1_6  18753.0  18753.0  18753.0  18753.0  18752.0\n",
      "43  client_3      1_7  18752.0  18753.0  18753.0  18753.0  18753.0\n",
      "44  client_4      0_0     11.0     12.0     11.0     11.0     11.0\n",
      "45  client_4      0_1  53876.0  53876.0  53876.0  53876.0  53876.0\n",
      "46  client_4      0_2      2.0      2.0      3.0      3.0      2.0\n",
      "47  client_4      0_3  23495.0  23495.0  23494.0  23494.0  23494.0\n",
      "48  client_4      0_4      9.0      9.0      9.0      8.0      9.0\n",
      "49  client_4      0_5      4.0      3.0      3.0      3.0      3.0\n",
      "50  client_4      0_6  24611.0  24611.0  24611.0  24612.0  24611.0\n",
      "51  client_4      0_7  63914.0  63914.0  63914.0  63915.0  63915.0\n",
      "52  client_4      0_8   2078.0   2078.0   2078.0   2079.0   2079.0\n",
      "53  client_4      1_0  19539.0  19539.0  19540.0  19539.0  19539.0\n",
      "54  client_4      1_1  19539.0  19540.0  19539.0  19539.0  19539.0\n",
      "55  client_4      1_2  19539.0  19539.0  19540.0  19539.0  19539.0\n",
      "56  client_4      1_3  19538.0  19539.0  19539.0  19538.0  19538.0\n",
      "57  client_4      1_4  19539.0  19538.0  19538.0  19538.0  19539.0\n",
      "58  client_4      1_5  19540.0  19539.0  19539.0  19539.0  19539.0\n",
      "59  client_4      1_6  19538.0  19538.0  19538.0  19539.0  19539.0\n",
      "60  client_4      1_7  31228.0  31228.0  31228.0  31228.0  31228.0\n"
     ]
    }
   ],
   "source": [
    "#print(merged_train_df.groupby(['client', 'fold', 'Stratify']).count())\n",
    "grouped_counts = merged_train_df.groupby(['client', 'fold', 'Stratify']).size().reset_index(name='count')\n",
    "# Pivoting the 'fold' column to make it into a separate column\n",
    "pivot_df = grouped_counts.pivot_table(index=['client', 'Stratify'], columns='fold', values='count', fill_value=0).reset_index()\n",
    "pivot_df.columns = ['Client', 'Stratify'] + [f'Fold_{int(col)}' for col in pivot_df.columns if isinstance(col, int)]\n",
    "print(pivot_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>CSV_File_Number</th>\n",
       "      <th>Stratify</th>\n",
       "      <th>client</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0</td>\n",
       "      <td>client_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1_9</td>\n",
       "      <td>client_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0_1</td>\n",
       "      <td>client_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1_8</td>\n",
       "      <td>client_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>client_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  CSV_File_Number Stratify    client  fold\n",
       "0      0                0      0_0  client_1     1\n",
       "1      1                9      1_9  client_1     1\n",
       "2      0                1      0_1  client_1     1\n",
       "3      1                8      1_8  client_1     1\n",
       "4      1                1      1_1  client_1     1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA dataset Preparation\n",
    "- Choose component of 30, 33, 35 and generate datasets accordingly for training dataset and store locally\n",
    "- Use PCA matrix to convert test dataset and store locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iteration Friendly dictionary\n",
    "clients = {\n",
    " \n",
    "    'client_1': {\n",
    "        'scaled_train': './dataset/client_1/fold_{}/client_1_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_1/fold_{}/client_1_sclaed_test_dataset.csv', #unwanted features removed\n",
    "        'pca_path': './dataset/client_1/fold_{}/pca/client_1_pca.pkl', #Saved for later use\n",
    "        'pca_train': './dataset/client_1/fold_{}/pca/client_1_pca_train_dataset.csv', #unwanted features removed\n",
    "        'pca_test': './dataset/client_1/fold_{}/pca/client_1_pca_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_2': {\n",
    "        'scaled_train': './dataset/client_2/fold_{}/client_2_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_2/fold_{}/client_2_sclaed_test_dataset.csv', #unwanted features removed\n",
    "        'pca_path': './dataset/client_2/fold_{}/pca/client_2_pca.pkl', #Saved for later use\n",
    "        'pca_train': './dataset/client_2/fold_{}/pca/client_2_pca_train_dataset.csv', #unwanted features removed\n",
    "        'pca_test': './dataset/client_2/fold_{}/pca/client_2_pca_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_3': {\n",
    "        'scaled_train': './dataset/client_3/fold_{}/client_3_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_3/fold_{}/client_3_sclaed_test_dataset.csv', #unwanted features removed\n",
    "        'pca_path': './dataset/client_3/fold_{}/pca/client_3_pca.pkl', #Saved for later use\n",
    "        'pca_train': './dataset/client_3/fold_{}/pca/client_3_pca_train_dataset.csv', #unwanted features removed\n",
    "        'pca_test': './dataset/client_3/fold_{}/pca/client_3_pca_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_4': {\n",
    "        'scaled_train': './dataset/client_4/fold_{}/client_4_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_4/fold_{}/client_4_sclaed_test_dataset.csv', #unwanted features removed\n",
    "        'pca_path': './dataset/client_4/fold_{}/pca/client_4_pca.pkl', #Saved for later use\n",
    "        'pca_train': './dataset/client_4/fold_{}/pca/client_4_pca_train_dataset.csv', #unwanted features removed\n",
    "        'pca_test': './dataset/client_4/fold_{}/pca/client_4_pca_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Function to perform PCA with a maximum number of components and save the PCA object\n",
    "def perform_pca_and_save_with_max(train_df, test_df, max_components, pca_path):\n",
    "    # Remove 'Label' column before applying PCA\n",
    "    train_labels = train_df['Label']\n",
    "    test_labels = test_df['Label']\n",
    "    train_features = train_df.drop(columns=['Label'])\n",
    "    test_features = test_df.drop(columns=['Label'])\n",
    "\n",
    "    # Initialize PCA with the maximum number of components\n",
    "    pca = PCA(n_components=max_components)\n",
    "    \n",
    "    # Fit PCA on the training set and transform\n",
    "    train_pca_full = pca.fit_transform(train_features)\n",
    "\n",
    "    ##Ensure direcoty exists. if not it will create one\n",
    "    ensure_directory_exists(pca_path)\n",
    "    \n",
    "    # Save the PCA model for future use\n",
    "    with open(pca_path, \"wb\") as f:\n",
    "        pickle.dump(pca, f)\n",
    "    \n",
    "    #print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    \n",
    "    # Transform the test set using the same PCA model\n",
    "    test_pca_full = pca.transform(test_features)\n",
    "    \n",
    "    return train_pca_full, test_pca_full, train_labels, test_labels\n",
    "\n",
    "# Step 2: Function to slice PCA components and add labels back to the dataset\n",
    "def add_labels_to_pca(train_pca_full, test_pca_full, train_labels, test_labels, num_components):\n",
    "    # Slice the top 'num_components' from the full PCA results\n",
    "    train_pca_reduced = train_pca_full[:, :num_components]\n",
    "    test_pca_reduced = test_pca_full[:, :num_components]\n",
    "    \n",
    "    # Convert to DataFrame for easier handling\n",
    "    train_pca_df = pd.DataFrame(train_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "    test_pca_df = pd.DataFrame(test_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "    \n",
    "    # Add the 'Label' column back\n",
    "    train_pca_df['Label'] = train_labels.values\n",
    "    test_pca_df['Label'] = test_labels.values\n",
    "    \n",
    "    return train_pca_df, test_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "Client: client_1 ... Fold: 1\n",
      "PCA explained variance ratio: [0.20081671 0.15398025 0.09394277 0.06085124 0.05687424 0.04529652\n",
      " 0.04113341 0.03271981 0.03135701 0.02656243 0.02385512 0.02273589\n",
      " 0.02187935 0.02066497 0.01811118 0.01678768 0.01400486 0.01312202\n",
      " 0.01156849 0.01136188 0.01035583 0.00960412 0.0084761  0.00787751\n",
      " 0.00703679 0.00623424 0.00558939 0.00524207 0.00366018 0.00268547\n",
      " 0.00211178 0.0018516  0.00176168 0.001474   0.0012837  0.00100835\n",
      " 0.00087394 0.00070049 0.00069053 0.00067499]\n",
      "Client: client_1 ... Fold: 2\n",
      "PCA explained variance ratio: [0.19868034 0.15398062 0.09438549 0.0601555  0.05711432 0.04481375\n",
      " 0.04085115 0.03262334 0.03135939 0.02662367 0.02375492 0.02252034\n",
      " 0.02191523 0.02088597 0.01838132 0.0168474  0.01497177 0.01314771\n",
      " 0.01149077 0.0113627  0.01114287 0.01011363 0.00858271 0.00791039\n",
      " 0.00697385 0.00637965 0.00563803 0.00526585 0.00366378 0.00268467\n",
      " 0.00207022 0.00188463 0.00177346 0.0014748  0.00126837 0.00101203\n",
      " 0.0008799  0.00070846 0.00068962 0.00068468]\n",
      "Client: client_1 ... Fold: 3\n",
      "PCA explained variance ratio: [0.20098324 0.15447195 0.09273023 0.06108751 0.05829216 0.04982589\n",
      " 0.04182068 0.03306097 0.03157416 0.0240333  0.02350468 0.02219536\n",
      " 0.02085376 0.01863672 0.01723179 0.01593017 0.01422165 0.01261614\n",
      " 0.01133351 0.01106165 0.01043184 0.00990212 0.00887363 0.00801539\n",
      " 0.00744331 0.00674054 0.00633215 0.00519571 0.00359104 0.00276564\n",
      " 0.00212276 0.00199763 0.00173674 0.00147911 0.0012607  0.00101482\n",
      " 0.00094068 0.0007039  0.00065964 0.00052357]\n",
      "Client: client_1 ... Fold: 4\n",
      "PCA explained variance ratio: [0.19788962 0.15450717 0.09489114 0.06023085 0.05728538 0.04517499\n",
      " 0.0416966  0.0328861  0.03148874 0.02669088 0.02386087 0.02275856\n",
      " 0.02183342 0.02082519 0.01830775 0.01672267 0.01456776 0.01295868\n",
      " 0.0114608  0.01138368 0.01111659 0.0100947  0.00863537 0.00787952\n",
      " 0.00674429 0.00636064 0.00549651 0.005065   0.00354169 0.00267738\n",
      " 0.00207405 0.0018395  0.00174917 0.00146952 0.00126304 0.00100954\n",
      " 0.00087582 0.00075062 0.00064647 0.00053381]\n",
      "Client: client_1 ... Fold: 5\n",
      "PCA explained variance ratio: [0.19948782 0.15398227 0.09349374 0.06079936 0.05702218 0.04455473\n",
      " 0.0410487  0.03274467 0.03155143 0.02817435 0.02387853 0.02267573\n",
      " 0.0221262  0.02104162 0.01840289 0.01691291 0.01472447 0.0133136\n",
      " 0.01156947 0.01137612 0.0110147  0.01008568 0.0085216  0.00785581\n",
      " 0.00689324 0.00634481 0.00533721 0.00375081 0.00344919 0.00268057\n",
      " 0.00200925 0.00170721 0.00159098 0.00143993 0.00122954 0.00100038\n",
      " 0.0008737  0.00077688 0.0006961  0.00068691]\n",
      "-----------------------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Client: client_2 ... Fold: 1\n",
      "PCA explained variance ratio: [0.17857561 0.15598467 0.10322798 0.06290016 0.05693154 0.04544293\n",
      " 0.03873726 0.03591063 0.03093108 0.02654059 0.02413599 0.02401741\n",
      " 0.02279223 0.02165998 0.01808085 0.01628078 0.01500395 0.01345361\n",
      " 0.01288911 0.01142557 0.01057466 0.01016725 0.00907663 0.00821699\n",
      " 0.00738126 0.0069933  0.00599577 0.00514283 0.0035487  0.00234336\n",
      " 0.00201909 0.00183244 0.00174525 0.00146907 0.00126637 0.00103742\n",
      " 0.0009173  0.00078774 0.00071314 0.00064466]\n",
      "Client: client_2 ... Fold: 2\n",
      "PCA explained variance ratio: [0.17762308 0.15596793 0.1047856  0.06285338 0.05658304 0.04526376\n",
      " 0.03874291 0.03571494 0.03071808 0.02678446 0.02434223 0.02405641\n",
      " 0.02274606 0.02175539 0.0184027  0.01632394 0.01501017 0.01336453\n",
      " 0.01283186 0.0110932  0.01060764 0.01014378 0.00906717 0.00820398\n",
      " 0.0075733  0.0071757  0.00585147 0.00514823 0.00345659 0.0023476\n",
      " 0.00217885 0.00201572 0.00155522 0.00143682 0.00118897 0.00100942\n",
      " 0.00089821 0.00077958 0.00063298 0.00057838]\n",
      "Client: client_2 ... Fold: 3\n",
      "PCA explained variance ratio: [0.1778837  0.15632214 0.10458531 0.06273214 0.05654955 0.04547499\n",
      " 0.03931272 0.03564691 0.03114956 0.02623726 0.02409612 0.02382102\n",
      " 0.02288278 0.02151281 0.01810354 0.01622806 0.01522838 0.01338229\n",
      " 0.01276607 0.01084106 0.01056405 0.01018312 0.0090305  0.00842356\n",
      " 0.0080545  0.00703422 0.00568236 0.00506771 0.00343788 0.00234079\n",
      " 0.00222057 0.00198959 0.00177056 0.00146056 0.00124608 0.00103833\n",
      " 0.00094485 0.00083121 0.00068209 0.00064163]\n",
      "Client: client_2 ... Fold: 4\n",
      "PCA explained variance ratio: [0.18135131 0.15613844 0.10393364 0.06369919 0.05685008 0.04778105\n",
      " 0.03955063 0.03634533 0.03100613 0.02525631 0.02404312 0.02389472\n",
      " 0.02284392 0.02127915 0.01807487 0.01637725 0.01457835 0.01330737\n",
      " 0.01236637 0.01116898 0.01033457 0.00915928 0.0087976  0.00820422\n",
      " 0.00739009 0.00668123 0.00556463 0.00359322 0.00265136 0.00237098\n",
      " 0.00209501 0.00181476 0.00178403 0.00146618 0.00126338 0.0010372\n",
      " 0.00092633 0.00077916 0.00069772 0.00068775]\n",
      "Client: client_2 ... Fold: 5\n",
      "PCA explained variance ratio: [0.17860527 0.1559773  0.10403225 0.06286734 0.05664211 0.04525519\n",
      " 0.03935506 0.03554141 0.03111621 0.0266494  0.02410886 0.02371855\n",
      " 0.0228543  0.02146577 0.01797722 0.01616245 0.01464677 0.01323403\n",
      " 0.01260085 0.01117976 0.01052734 0.01016399 0.00908961 0.00819687\n",
      " 0.00795698 0.00716979 0.00579115 0.00508514 0.00351589 0.00235226\n",
      " 0.00230435 0.00201892 0.0018072  0.00146559 0.00125336 0.00103718\n",
      " 0.00094146 0.00082777 0.00068034 0.00062849]\n",
      "-----------------------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Client: client_3 ... Fold: 1\n",
      "PCA explained variance ratio: [0.17549892 0.12289912 0.09771355 0.07659883 0.06172051 0.04640066\n",
      " 0.0433348  0.03636986 0.03381966 0.03161559 0.02629104 0.02239896\n",
      " 0.02164746 0.02138641 0.01914539 0.01756311 0.01658392 0.01331675\n",
      " 0.0125388  0.01186502 0.01175073 0.0108667  0.01026758 0.00832086\n",
      " 0.00780656 0.00675866 0.00578106 0.00523994 0.00468381 0.00343472\n",
      " 0.00254634 0.00236197 0.0017493  0.00144238 0.00136517 0.00116441\n",
      " 0.00067059 0.00066336 0.00060571 0.00051162]\n",
      "Client: client_3 ... Fold: 2\n",
      "PCA explained variance ratio: [0.17499205 0.12311916 0.09687181 0.07514744 0.06076157 0.0461813\n",
      " 0.0443621  0.03682068 0.03364864 0.0317043  0.02628199 0.02238766\n",
      " 0.02161348 0.02137982 0.01921791 0.01754459 0.01657593 0.01440642\n",
      " 0.0125385  0.0120721  0.01186866 0.01048893 0.01018953 0.00841796\n",
      " 0.00785105 0.00675441 0.00645481 0.00567196 0.00514715 0.00350132\n",
      " 0.00251877 0.00209585 0.00167951 0.00140835 0.00132469 0.00114326\n",
      " 0.00076457 0.00066298 0.00062251 0.00050895]\n",
      "Client: client_3 ... Fold: 3\n",
      "PCA explained variance ratio: [0.17543981 0.12294162 0.09736222 0.07609813 0.06154901 0.04616148\n",
      " 0.04293064 0.03652039 0.03395765 0.03144361 0.02647901 0.02232191\n",
      " 0.02169323 0.0214432  0.01929575 0.01789258 0.01681    0.01401394\n",
      " 0.01253882 0.01190278 0.0117595  0.01082301 0.01025663 0.00838616\n",
      " 0.00798374 0.00674476 0.00581075 0.00508959 0.00480937 0.00344663\n",
      " 0.00266689 0.00219437 0.00173821 0.00139807 0.0013202  0.00112215\n",
      " 0.00066488 0.00064962 0.00059281 0.00049744]\n",
      "Client: client_3 ... Fold: 4\n",
      "PCA explained variance ratio: [0.17612218 0.12382112 0.09881267 0.07630507 0.06241519 0.04634748\n",
      " 0.04493732 0.04187208 0.03627397 0.03280449 0.02709886 0.02205456\n",
      " 0.02172239 0.02035288 0.01762656 0.01724907 0.01470071 0.01253841\n",
      " 0.01210332 0.01184183 0.01105217 0.01025482 0.00847989 0.00796634\n",
      " 0.0067548  0.0060339  0.00556732 0.00523325 0.0040274  0.00264252\n",
      " 0.00225546 0.00210937 0.00174522 0.00131143 0.0011902  0.00084375\n",
      " 0.00076653 0.00062394 0.00055479 0.00048516]\n",
      "Client: client_3 ... Fold: 5\n",
      "PCA explained variance ratio: [0.17502701 0.12336874 0.09741194 0.07524112 0.06086926 0.04631679\n",
      " 0.04424384 0.03572867 0.03395072 0.03138377 0.02635615 0.02244169\n",
      " 0.02182013 0.02155088 0.01932944 0.01801625 0.01682262 0.01403192\n",
      " 0.01253809 0.01228499 0.0118667  0.01113682 0.01028577 0.00837751\n",
      " 0.00779433 0.00676873 0.00576783 0.00496415 0.004887   0.00342864\n",
      " 0.00289954 0.00221301 0.00158447 0.00138389 0.00127722 0.00111127\n",
      " 0.00074185 0.00065838 0.00062446 0.00047535]\n",
      "-----------------------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Client: client_4 ... Fold: 1\n",
      "PCA explained variance ratio: [0.17857966 0.13854516 0.0996306  0.07441958 0.05974714 0.04662075\n",
      " 0.04571013 0.0417938  0.03643168 0.03029224 0.02531136 0.02230178\n",
      " 0.02189802 0.01962004 0.01779129 0.01639092 0.01256506 0.01191087\n",
      " 0.01136542 0.01062971 0.00973832 0.00851991 0.00771074 0.00682356\n",
      " 0.00640384 0.00605258 0.00578602 0.00513769 0.00411413 0.00321995\n",
      " 0.00308032 0.00205778 0.00158785 0.00127923 0.00113639 0.00085295\n",
      " 0.00071084 0.00060181 0.00050808 0.00043871]\n",
      "Client: client_4 ... Fold: 2\n",
      "PCA explained variance ratio: [0.18041584 0.14229773 0.09888797 0.07334015 0.0587874  0.04657147\n",
      " 0.04512965 0.04103179 0.03618518 0.02938961 0.0245718  0.02226945\n",
      " 0.02189725 0.01972776 0.01790109 0.01672368 0.01256497 0.01198725\n",
      " 0.01165559 0.01066254 0.0091466  0.0085269  0.00767218 0.00698108\n",
      " 0.00657574 0.00626786 0.00582271 0.00524326 0.00406507 0.00326064\n",
      " 0.00288425 0.00202026 0.00156393 0.00123301 0.00108909 0.00083958\n",
      " 0.00065558 0.00059374 0.00049535 0.0004285 ]\n",
      "Client: client_4 ... Fold: 3\n",
      "PCA explained variance ratio: [0.17824307 0.13920753 0.09967962 0.07438823 0.05963864 0.04644539\n",
      " 0.04570252 0.04177178 0.03635865 0.03013283 0.02528369 0.02225977\n",
      " 0.02186751 0.01945161 0.0179336  0.01669704 0.0125648  0.01190776\n",
      " 0.01149826 0.01060986 0.00979158 0.00852581 0.00774621 0.00678651\n",
      " 0.00622982 0.00605815 0.00576819 0.00502867 0.00414607 0.00322888\n",
      " 0.00279358 0.00209306 0.00160335 0.00136048 0.00113652 0.00086817\n",
      " 0.0007177  0.00062234 0.00055499 0.00051744]\n",
      "Client: client_4 ... Fold: 4\n",
      "PCA explained variance ratio: [0.1793051  0.13978357 0.09845433 0.07373622 0.05898115 0.04657371\n",
      " 0.04541143 0.04124368 0.0358836  0.02978718 0.02501702 0.02232801\n",
      " 0.02200073 0.01967848 0.01783082 0.01674067 0.01256496 0.01202926\n",
      " 0.01179207 0.01065748 0.00980011 0.00858346 0.00765842 0.00734124\n",
      " 0.00671539 0.00623315 0.00584145 0.00519037 0.00426155 0.00345647\n",
      " 0.00308418 0.00202911 0.0016141  0.00139984 0.00109571 0.00086558\n",
      " 0.00076055 0.00061144 0.00051347 0.00043796]\n",
      "Client: client_4 ... Fold: 5\n",
      "PCA explained variance ratio: [0.17873747 0.13947133 0.09938774 0.07415506 0.05931152 0.04671577\n",
      " 0.04579846 0.04250188 0.03640494 0.03011102 0.02535605 0.02225814\n",
      " 0.02187022 0.01952865 0.01784626 0.0163981  0.01256504 0.01189647\n",
      " 0.0114149  0.01062743 0.00966235 0.00847021 0.00766143 0.00681451\n",
      " 0.00622658 0.00578982 0.0053356  0.00503025 0.00407643 0.00328406\n",
      " 0.00299741 0.00211508 0.00161161 0.00139199 0.00116061 0.00087562\n",
      " 0.00077029 0.00058515 0.00053078 0.00050769]\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "max_pca_components = 40 # This number chosen based on pca analysis. check PCA.ipynb\n",
    "\n",
    "for client, info in clients.items():\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    for fold in range(1, 6):\n",
    "        print(f'Client: {client} ... Fold: {fold}')\n",
    "        train_df = pd.read_csv(info.get('scaled_train').format(fold))\n",
    "        test_df = pd.read_csv(info.get('scaled_test').format(fold))\n",
    "    \n",
    "        # Step 3: Perform PCA with the maximum number of components (e.g., 40)\n",
    "        train_pca_full, test_pca_full, train_labels, test_labels = perform_pca_and_save_with_max(train_df, test_df, max_components=max_pca_components, pca_path=info.get(\"pca_path\").format(fold))      \n",
    "        train_pca_data, test_pca_data = add_labels_to_pca(train_pca_full, test_pca_full, train_labels, test_labels, num_components=max_pca_components)\n",
    "        \n",
    "        train_pca_data.to_csv(info.get('pca_train').format(fold),  index=False)\n",
    "        test_pca_data.to_csv(info.get('pca_test').format(fold),  index=False)\n",
    "\n",
    "    print(\"-----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not in USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Iteration Friendly dictionary\n",
    "# clients = {\n",
    " \n",
    "#     'client_1': {\n",
    "#         'scaled_train': './dataset/client_1/client_1_scaled_train_dataset.csv', #unwanted features removed\n",
    "#         'scaled_test': './dataset/client_1/client_1_sclaed_test_dataset.csv', #unwanted features removed\n",
    "#         'pca_path': './dataset/client_1/pca/client_1_pca.pkl', #Saved for later use\n",
    "#         'pca_train': './dataset/client_1/pca/client_1_pca_train_dataset.csv', #unwanted features removed\n",
    "#         'pca_test': './dataset/client_1/pca/client_1_pca_test_dataset.csv', #unwanted features removed\n",
    "#     },\n",
    "#     'client_2': {\n",
    "#         'scaled_train': './dataset/client_2/client_2_scaled_train_dataset.csv', #unwanted features removed\n",
    "#         'scaled_test': './dataset/client_2/client_2_sclaed_test_dataset.csv', #unwanted features removed\n",
    "#         'pca_path': './dataset/client_2/pca/client_2_pca.pkl', #Saved for later use\n",
    "#         'pca_train': './dataset/client_2/pca/client_2_pca_train_dataset.csv', #unwanted features removed\n",
    "#         'pca_test': './dataset/client_2/pca/client_2_pca_test_dataset.csv', #unwanted features removed\n",
    "#     },\n",
    "#     'client_3': {\n",
    "#         'scaled_train': './dataset/client_3/client_3_scaled_train_dataset.csv', #unwanted features removed\n",
    "#         'scaled_test': './dataset/client_3/client_3_sclaed_test_dataset.csv', #unwanted features removed\n",
    "#         'pca_path': './dataset/client_3/pca/client_3_pca.pkl', #Saved for later use\n",
    "#         'pca_train': './dataset/client_3/pca/client_3_pca_train_dataset.csv', #unwanted features removed\n",
    "#         'pca_test': './dataset/client_3/pca/client_3_pca_test_dataset.csv', #unwanted features removed\n",
    "#     },\n",
    "#     'client_4': {\n",
    "#         'scaled_train': './dataset/client_4/client_4_scaled_train_dataset.csv', #unwanted features removed\n",
    "#         'scaled_test': './dataset/client_4/client_4_sclaed_test_dataset.csv', #unwanted features removed\n",
    "#         'pca_path': './dataset/client_4/pca/client_4_pca.pkl', #Saved for later use\n",
    "#         'pca_train': './dataset/client_4/pca/client_4_pca_train_dataset.csv', #unwanted features removed\n",
    "#         'pca_test': './dataset/client_4/pca/client_4_pca_test_dataset.csv', #unwanted features removed\n",
    "#     },\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Function to perform PCA with a maximum number of components and save the PCA object\n",
    "# def perform_pca_and_save_with_max(train_df, test_df, max_components, pca_path):\n",
    "#     # Remove 'Label' column before applying PCA\n",
    "#     train_labels = train_df['Label']\n",
    "#     test_labels = test_df['Label']\n",
    "#     train_features = train_df.drop(columns=['Label'])\n",
    "#     test_features = test_df.drop(columns=['Label'])\n",
    "\n",
    "#     # Initialize PCA with the maximum number of components\n",
    "#     pca = PCA(n_components=max_components)\n",
    "    \n",
    "#     # Fit PCA on the training set and transform\n",
    "#     train_pca_full = pca.fit_transform(train_features)\n",
    "    \n",
    "#     # Save the PCA model for future use\n",
    "#     with open(pca_path, \"wb\") as f:\n",
    "#         pickle.dump(pca, f)\n",
    "    \n",
    "#     print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    \n",
    "#     # Transform the test set using the same PCA model\n",
    "#     test_pca_full = pca.transform(test_features)\n",
    "    \n",
    "#     return train_pca_full, test_pca_full, train_labels, test_labels\n",
    "\n",
    "# # Step 2: Function to slice PCA components and add labels back to the dataset\n",
    "# def add_labels_to_pca(train_pca_full, test_pca_full, train_labels, test_labels, num_components):\n",
    "#     # Slice the top 'num_components' from the full PCA results\n",
    "#     train_pca_reduced = train_pca_full[:, :num_components]\n",
    "#     test_pca_reduced = test_pca_full[:, :num_components]\n",
    "    \n",
    "#     # Convert to DataFrame for easier handling\n",
    "#     train_pca_df = pd.DataFrame(train_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "#     test_pca_df = pd.DataFrame(test_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "    \n",
    "#     # Add the 'Label' column back\n",
    "#     train_pca_df['Label'] = train_labels.values\n",
    "#     test_pca_df['Label'] = test_labels.values\n",
    "    \n",
    "#     return train_pca_df, test_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_configurations = [\n",
    "    {\n",
    "        'n_components': 40,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "    {\n",
    "        'n_components': 35,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "    {\n",
    "        'n_components': 33,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "    {\n",
    "        'n_components': 30,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f{client} Starting....\n",
      "PCA explained variance ratio: [0.17908248 0.15599521 0.10360098 0.06265984 0.05702301 0.04744612\n",
      " 0.03931341 0.03542901 0.0312049  0.02462718 0.02393085 0.02360545\n",
      " 0.02267465 0.02070881 0.01815602 0.01624444 0.01496836 0.01323305\n",
      " 0.01270404 0.01132342 0.01050242 0.01010683 0.00901977 0.00840257\n",
      " 0.00798672 0.00691281 0.00577896 0.0051003  0.0035652  0.00234264\n",
      " 0.00217048 0.00202068 0.00176953 0.00147195 0.00125896 0.00112544\n",
      " 0.00104076 0.00085804 0.00071288 0.00059623]\n",
      "(335999, 41)\n",
      "(335999, 36)\n",
      "(335999, 34)\n",
      "(335999, 31)\n",
      "-----------------------------------------------------------------------\n",
      "f{client} Starting....\n",
      "PCA explained variance ratio: [0.17568192 0.12473658 0.09752492 0.07552202 0.06113587 0.04641158\n",
      " 0.04374342 0.03611841 0.03387959 0.03110569 0.02618503 0.02230072\n",
      " 0.02165586 0.02130531 0.01937251 0.01770442 0.01641674 0.01333521\n",
      " 0.01253844 0.01200517 0.01183246 0.01103053 0.01025419 0.00834167\n",
      " 0.00781151 0.00677549 0.00578028 0.005172   0.00495851 0.00346764\n",
      " 0.00267746 0.00220526 0.00159477 0.00141242 0.00128583 0.00115429\n",
      " 0.00069655 0.00066433 0.00061219 0.00050331]\n",
      "(336000, 41)\n",
      "(336000, 36)\n",
      "(336000, 34)\n",
      "(336000, 31)\n",
      "-----------------------------------------------------------------------\n",
      "f{client} Starting....\n",
      "PCA explained variance ratio: [0.17891667 0.14315922 0.09935361 0.07356371 0.05880952 0.04687952\n",
      " 0.04552997 0.04231053 0.03581232 0.02993947 0.02525125 0.02227518\n",
      " 0.02185979 0.01960318 0.01782566 0.01618198 0.0125651  0.0119683\n",
      " 0.01179574 0.01065376 0.00966354 0.00850855 0.00754503 0.00678815\n",
      " 0.00623966 0.00576979 0.00541763 0.00445798 0.00395761 0.00295314\n",
      " 0.00284247 0.00206224 0.00153527 0.0012264  0.00110188 0.00085907\n",
      " 0.00063567 0.00059057 0.00049992 0.000424  ]\n",
      "(336000, 41)\n",
      "(336000, 36)\n",
      "(336000, 34)\n",
      "(336000, 31)\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# for client, info in clients.items():\n",
    "#     print(\"f{client} Starting....\")\n",
    "#     train_df = pd.read_csv(info.get('scaled_train'))\n",
    "#     test_df = pd.read_csv(info.get('scaled_test'))\n",
    "#     # Step 3: Perform PCA with the maximum number of components (e.g., 40)\n",
    "#     train_pca_full, test_pca_full, train_labels, test_labels = perform_pca_and_save_with_max(train_df, test_df, max_components=40, pca_path=info.get(\"pca_path\"))\n",
    "    \n",
    "    \n",
    "#     # Step 4: Slice the PCA components and add labels back (e.g., for 40, 35, 33, 30 components)\n",
    "#     for p in pca_configurations:        \n",
    "#         train_pca_data, test_pca_data = add_labels_to_pca(train_pca_full, test_pca_full, train_labels, test_labels, num_components=p.get('n_components'))\n",
    "#         train_pca_data.to_csv(p.get('train_path').format(client, p.get('n_components'), p.get('n_components')),  index=False)\n",
    "#         test_pca_data.to_csv(p.get('test_path').format(client, p.get('n_components'), p.get('n_components')),  index=False)\n",
    "#         print(train_pca_data.shape)\n",
    "#     print(\"-----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # This is just the full version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
