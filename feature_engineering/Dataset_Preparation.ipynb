{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overall DataSet Preparation from Row data\n",
    "- Read all the row feature files from the directory specified\n",
    "- Sanitize all the inputs\n",
    "- Take sample count based on attack and benign csv files (technically it's class)\n",
    "- Save the dataset locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_csv_files(directory_path, features=None):\n",
    "    # Initialize an empty list to store dataframes from CSV files\n",
    "    dataframes = []\n",
    "\n",
    "    # Get a list of all files in the directory\n",
    "    file_list = os.listdir(directory_path)\n",
    "\n",
    "    # Loop through each file and check if it's a CSV file\n",
    "    for file_number, file_name in enumerate(file_list):\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Get the full file path\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            # Read the CSV file into a pandas DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Remove leading and trailing spaces from column names\n",
    "            df.columns = df.columns.str.strip()\n",
    "            # Append the DataFrame to the list\n",
    "            df['CSV_File_Number'] = file_number\n",
    "            #dataframes.append(df[features])\n",
    "            dataframes.append(df) #as it is feature analysis, we are taking all the features\n",
    "\n",
    "    # Merge all DataFrames into a single DataFrame\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_data_frames_updated(dataframe, remove_infinity=True, remove_null=True):\n",
    "\n",
    "    if remove_infinity:\n",
    "        numeric_cols = dataframe.select_dtypes(include=[np.number]).columns\n",
    "        infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n",
    "        for col, count in infinite_counts.items():\n",
    "            if count != 0:\n",
    "                dataframe = dataframe[~np.isinf(dataframe[col])]\n",
    "\n",
    "    if remove_null:\n",
    "        null_counts = dataframe.isnull().sum()\n",
    "        for col, count in null_counts.items():\n",
    "            if count != 0:\n",
    "                    dataframe = dataframe.dropna(subset=[col])\n",
    "    print(\"Sanitized Row Count:\", dataframe.shape[0])    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all dataset sources to make iterate to read csv files\n",
    "dataset_sources = {\n",
    " \n",
    "    'client_1': {\n",
    "        'benign': '../row_data/client_1/benign',\n",
    "        'attack': '../row_data/client_1/attack',\n",
    "    },\n",
    "    'client_2': {\n",
    "        'benign': '../row_data/client_2/benign',\n",
    "        'attack': '../row_data/client_2/attack',\n",
    "    },\n",
    "    'client_3': {\n",
    "        'benign': '../row_data/client_3/benign',\n",
    "        'attack': '../row_data/client_3/attack',\n",
    "    },\n",
    "    'client_4': {\n",
    "        'benign': '../row_data/client_4/benign',\n",
    "        'attack': '../row_data/client_4/attack',\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Uniform Sample\n",
    "def get_uniform_sample(df, group_col=[], sample_size=420000):\n",
    "    \n",
    "    #Separate the labels\n",
    "    label_0 = df[df['Label'] == 0]\n",
    "    label_1 = df[df['Label'] == 1]\n",
    "\n",
    "    count_0 = count_1 = 0 # Initialize with 0\n",
    "\n",
    "    if len(label_0) >= sample_size//2 and len(label_1) >= sample_size//2:\n",
    "        count_0 = count_1 = sample_size//2\n",
    "    elif len(label_0) < sample_size//2 and len(label_1) > sample_size//2:\n",
    "        count_0 = len(label_0)\n",
    "        count_1 = sample_size - count_0\n",
    "    elif len(label_0) > sample_size//2 and len(label_1) < sample_size//2:\n",
    "        count_1 = len(label_1)\n",
    "        count_0 = sample_size - count_1\n",
    "    else:\n",
    "        count_0 = len(label_0)\n",
    "        count_1 = len(label_1)\n",
    "\n",
    "    print(f\"count_0: {count_0}; count_1: {count_1}\")\n",
    "    print(f\"label_0: {len(label_0)}; label_1: {len(label_1)}\")\n",
    "\n",
    "    train_size_0 = 1.0 if count_0 >= len(label_0) else float(count_0/len(label_0))\n",
    "    train_size_1 = 1.0 if count_1 >= len(label_1) else float(count_1/len(label_1))\n",
    "\n",
    "    print(f\"train_size_0: {train_size_0}; train_size_1: {train_size_1}\")\n",
    "    \n",
    "# Handle edge cases where train_size is 1.0 and stratification might fail\n",
    "    if train_size_0 == 1.0:\n",
    "        sample_0 = label_0.copy()  # Use the whole subset without train_test_split\n",
    "    else:\n",
    "        sample_0, _ = train_test_split(\n",
    "            label_0,\n",
    "            train_size=train_size_0,\n",
    "            stratify=label_0['CSV_File_Number'] if group_col else None,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if train_size_1 == 1.0:\n",
    "        sample_1 = label_1.copy()  # Use the whole subset without train_test_split\n",
    "    else:\n",
    "        sample_1, _ = train_test_split(\n",
    "            label_1,\n",
    "            train_size=train_size_1,\n",
    "            stratify=label_1['CSV_File_Number'] if group_col else None,\n",
    "            random_state=42\n",
    "        )   \n",
    "    \n",
    "    # Concatenate and shuffle the samples\n",
    "    combined_sample = pd.concat([sample_0, sample_1])\n",
    "    shuffled_sample = combined_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    return shuffled_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 54648.91it/s]\n",
      "  0%|                                                                                                   | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_1, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_953695/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 202821\n",
      " Loading....Clinet = client_1, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_953695/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 2671528\n",
      "count_0: 202821; count_1: 217179\n",
      "label_0: 202821; label_1: 2671528\n",
      "train_size_0: 1.0; train_size_1: 0.08129392617258738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████████████▌                                                                   | 1/4 [03:47<11:21, 227.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_2, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_953695/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 185943\n",
      " Loading....Clinet = client_2, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_953695/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 3346296\n",
      "count_0: 185943; count_1: 234057\n",
      "label_0: 185943; label_1: 3346296\n",
      "train_size_0: 1.0; train_size_1: 0.06994509750482324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████                                             | 2/4 [08:17<08:24, 252.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_3, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_953695/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 223200\n",
      " Loading....Clinet = client_3, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_953695/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 2348465\n",
      "count_0: 210000; count_1: 210000\n",
      "label_0: 223200; label_1: 2348465\n",
      "train_size_0: 0.9408602150537635; train_size_1: 0.08942011058287008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████████▌                      | 3/4 [11:37<03:48, 228.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_4, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_953695/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 246797\n",
      " Loading....Clinet = client_4, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_953695/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 2253978\n",
      "count_0: 210000; count_1: 210000\n",
      "label_0: 246797; label_1: 2253978\n",
      "train_size_0: 0.850901753262803; train_size_1: 0.09316861122868103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 4/4 [14:53<00:00, 223.42s/it]\n"
     ]
    }
   ],
   "source": [
    "##Reading all the data\n",
    "features = None\n",
    "\n",
    "\n",
    "for client, data in tqdm(dataset_sources.items(), total=len(dataset_sources)):\n",
    "    locals()[client] = {}\n",
    "\n",
    "for client, data in tqdm(dataset_sources.items(), total=len(dataset_sources)):\n",
    "    client_dataframe = []\n",
    "    for type, path in data.items():\n",
    "        print(f' Loading....Clinet = {client}, type = {type}')\n",
    "        dataframe = read_all_csv_files(path, features)\n",
    "        dataframe = sanitize_data_frames_updated(dataframe)\n",
    "        if type == 'benign':\n",
    "            dataframe['Label'] = 0\n",
    "        else:\n",
    "             dataframe['Label'] = 1\n",
    "        client_dataframe.append(dataframe)\n",
    "        #locals()[client][type] = dataframe\n",
    "    client_merged_df = get_uniform_sample(pd.concat(client_dataframe, ignore_index=True), ['Label', 'CSV_File_Number'], 420000 )\n",
    "    client_merged_df.to_csv(f'./dataset/{client}/{client}_original_dataset.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train and Test Set Segregation\n",
    "- Read datasets saved in Step 1\n",
    "- Segregate Train and Test set and save locally. As we need to take decission based on csv file number and label, we will combine both column to stratify\n",
    "- Sacling Training data save the scaled data and save the scalers as well\n",
    "- Use saved scalers to scale the test data and save the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_exclude = ['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Timestamp', 'CSV_File_Number', 'Label', 'Stratify']\n",
    "output_features = ['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iteration Friendly dictionary\n",
    "clients = {\n",
    " \n",
    "    'client_1': {\n",
    "        'combined': './dataset/client_1/client_1_original_dataset.csv',\n",
    "        'train': './dataset/client_1/client_1_train_dataset.csv',\n",
    "        'test': './dataset/client_1/client_1_test_dataset.csv',\n",
    "        'scaler': './dataset/client_1/client_1_train_scaler.pkl',\n",
    "        'scaled_train': './dataset/client_1/client_1_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_1/client_1_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_2': {\n",
    "        'combined': './dataset/client_2/client_2_original_dataset.csv',\n",
    "        'train': './dataset/client_2/client_2_train_dataset.csv',\n",
    "        'test': './dataset/client_2/client_2_test_dataset.csv',\n",
    "        'scaler': './dataset/client_2/client_2_train_scaler.pkl',\n",
    "        'scaled_train': './dataset/client_2/client_2_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_2/client_2_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_3': {\n",
    "        'combined': './dataset/client_3/client_3_original_dataset.csv',\n",
    "        'train': './dataset/client_3/client_3_train_dataset.csv',\n",
    "        'test': './dataset/client_3/client_3_test_dataset.csv',\n",
    "        'scaler': './dataset/client_3/client_3_train_scaler.pkl',\n",
    "        'scaled_train': './dataset/client_3/client_3_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_3/client_3_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_4': {\n",
    "        'combined': './dataset/client_4/client_4_original_dataset.csv',\n",
    "        'train': './dataset/client_4/client_4_train_dataset.csv',\n",
    "        'test': './dataset/client_4/client_4_test_dataset.csv',\n",
    "        'scaler': './dataset/client_4/client_4_train_scaler.pkl',\n",
    "        'scaled_train': './dataset/client_4/client_4_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_4/client_4_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    },    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_remove_unwanted_features(dataframe, feature_to_exclude, out_features, scaler_path, type='train'):\n",
    "\n",
    "    features = dataframe.columns.values.tolist()\n",
    "    input_features = [feature for feature in features if feature not in feature_to_exclude] \n",
    "    output_df = dataframe[out_features]\n",
    "    input_df = dataframe[input_features]\n",
    "\n",
    "    print(f\"Input features shape: {input_df.shape}\")  # Debugging\n",
    "    print(f\"Output (label) features shape: {output_df.shape}\")  # Debugging\n",
    "\n",
    "    # Reset index for both input and output DataFrames to ensure correct alignment\n",
    "    input_df = input_df.reset_index(drop=True)\n",
    "    output_df = output_df.reset_index(drop=True)\n",
    "    \n",
    "    if type == 'train':\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(input_df)\n",
    "        scaled_df = pd.DataFrame(scaled_data, columns=input_df.columns)\n",
    "        print(f\"Scaled features shape: {scaled_df.shape}\")  # Debugging\n",
    "        \n",
    "        merged_df = pd.concat([scaled_df, output_df], axis=1)\n",
    "        print(f\"Merged DataFrame shape: {merged_df.shape}\")  # Debugging\n",
    "\n",
    "        # Save the scaler to a file to be used for test set\n",
    "        with open(scaler_path, \"wb\") as file:\n",
    "            pickle.dump(scaler, file)\n",
    "    else:\n",
    "        with open(scaler_path, \"rb\") as file:\n",
    "            scaler = pickle.load(file)\n",
    "            scaled_data = scaler.transform(input_df)  # Changed from fit_transform to transform\n",
    "            scaled_df = pd.DataFrame(scaled_data, columns=input_df.columns)\n",
    "            print(f\"Scaled features shape: {scaled_df.shape}\")  # Debugging\n",
    "            merged_df = pd.concat([scaled_df, output_df], axis=1)\n",
    "            print(f\"Merged DataFrame shape: {merged_df.shape}\")  # Debugging\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                   | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Started for client_1\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████████████▊                                                                    | 1/4 [00:52<02:36, 52.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing End for client_1\n",
      "Processing Started for client_2\n",
      "Input features shape: (335999, 85)\n",
      "Output (label) features shape: (335999, 1)\n",
      "Scaled features shape: (335999, 85)\n",
      "Merged DataFrame shape: (335999, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████▌                                             | 2/4 [01:48<01:49, 54.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing End for client_2\n",
      "Processing Started for client_3\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████████████████████████▎                      | 3/4 [02:41<00:53, 53.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing End for client_3\n",
      "Processing Started for client_4\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 4/4 [03:38<00:00, 54.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing End for client_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for client, info in tqdm(clients.items(), total=len(clients)):\n",
    "    print(f\"Processing Started for {client}\")\n",
    "    df = pd.read_csv(info.get('combined'))\n",
    "    #combinding label and csv file number to stratify\n",
    "    df['Stratify'] = df['Label'].astype(str) + '_' + df['CSV_File_Number'].astype(str)\n",
    "    \n",
    "    train_size = 0.8  # 80% for training, 20% for testing\n",
    "    train_df, test_df = train_test_split(df, train_size=train_size, random_state=42, stratify=df['Stratify'])\n",
    "    \n",
    "    train_df.to_csv(info.get('train'), index=False) ##It will contain all column with additional column\n",
    "    test_df.to_csv(info.get('test'), index=False)  ##It will contain all column with additional column\n",
    "\n",
    "    scaled_train_df = scale_and_remove_unwanted_features(train_df, feature_to_exclude, output_features, info.get('scaler'), 'train')\n",
    "    #scaled_train_df.describe()\n",
    "    scaled_train_df.to_csv(info.get('scaled_train'), index=False)\n",
    "\n",
    "    scaled_test_df = scale_and_remove_unwanted_features(test_df, feature_to_exclude, output_features, info.get('scaler'), 'test')\n",
    "    #scaled_test_df.describe()\n",
    "    scaled_test_df.to_csv(info.get('scaled_test'), index=False)\n",
    "    print(f\"Processing End for {client}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA dataset Preparation\n",
    "- Choose component of 30, 33, 35 and generate datasets accordingly for training dataset and store locally\n",
    "- Use PCA matrix to convert test dataset and store locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iteration Friendly dictionary\n",
    "clients = {\n",
    " \n",
    "    # 'client_1': {\n",
    "    #     'scaled_train': './dataset/client_1/client_1_scaled_train_dataset.csv', #unwanted features removed\n",
    "    #     'scaled_test': './dataset/client_1/client_1_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    #     'pca_path': './dataset/client_1/pca/client_1_pca.pkl', #Saved for later use\n",
    "    #     'pca_train': './dataset/client_1/pca/client_1_pca_train_dataset.csv', #unwanted features removed\n",
    "    #     'pca_test': './dataset/client_1/pca/client_1_pca_test_dataset.csv', #unwanted features removed\n",
    "    # },\n",
    "    'client_2': {\n",
    "        'scaled_train': './dataset/client_2/client_2_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_2/client_2_sclaed_test_dataset.csv', #unwanted features removed\n",
    "        'pca_path': './dataset/client_2/pca/client_2_pca.pkl', #Saved for later use\n",
    "        'pca_train': './dataset/client_2/pca/client_2_pca_train_dataset.csv', #unwanted features removed\n",
    "        'pca_test': './dataset/client_2/pca/client_2_pca_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_3': {\n",
    "        'scaled_train': './dataset/client_3/client_3_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_3/client_3_sclaed_test_dataset.csv', #unwanted features removed\n",
    "        'pca_path': './dataset/client_3/pca/client_3_pca.pkl', #Saved for later use\n",
    "        'pca_train': './dataset/client_3/pca/client_3_pca_train_dataset.csv', #unwanted features removed\n",
    "        'pca_test': './dataset/client_3/pca/client_3_pca_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_4': {\n",
    "        'scaled_train': './dataset/client_4/client_4_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_4/client_4_sclaed_test_dataset.csv', #unwanted features removed\n",
    "        'pca_path': './dataset/client_4/pca/client_4_pca.pkl', #Saved for later use\n",
    "        'pca_train': './dataset/client_4/pca/client_4_pca_train_dataset.csv', #unwanted features removed\n",
    "        'pca_test': './dataset/client_4/pca/client_4_pca_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Function to perform PCA with a maximum number of components and save the PCA object\n",
    "def perform_pca_and_save_with_max(train_df, test_df, max_components, pca_path):\n",
    "    # Remove 'Label' column before applying PCA\n",
    "    train_labels = train_df['Label']\n",
    "    test_labels = test_df['Label']\n",
    "    train_features = train_df.drop(columns=['Label'])\n",
    "    test_features = test_df.drop(columns=['Label'])\n",
    "\n",
    "    # Initialize PCA with the maximum number of components\n",
    "    pca = PCA(n_components=max_components)\n",
    "    \n",
    "    # Fit PCA on the training set and transform\n",
    "    train_pca_full = pca.fit_transform(train_features)\n",
    "    \n",
    "    # Save the PCA model for future use\n",
    "    with open(pca_path, \"wb\") as f:\n",
    "        pickle.dump(pca, f)\n",
    "    \n",
    "    print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    \n",
    "    # Transform the test set using the same PCA model\n",
    "    test_pca_full = pca.transform(test_features)\n",
    "    \n",
    "    return train_pca_full, test_pca_full, train_labels, test_labels\n",
    "\n",
    "# Step 2: Function to slice PCA components and add labels back to the dataset\n",
    "def add_labels_to_pca(train_pca_full, test_pca_full, train_labels, test_labels, num_components):\n",
    "    # Slice the top 'num_components' from the full PCA results\n",
    "    train_pca_reduced = train_pca_full[:, :num_components]\n",
    "    test_pca_reduced = test_pca_full[:, :num_components]\n",
    "    \n",
    "    # Convert to DataFrame for easier handling\n",
    "    train_pca_df = pd.DataFrame(train_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "    test_pca_df = pd.DataFrame(test_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "    \n",
    "    # Add the 'Label' column back\n",
    "    train_pca_df['Label'] = train_labels.values\n",
    "    test_pca_df['Label'] = test_labels.values\n",
    "    \n",
    "    return train_pca_df, test_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_configurations = [\n",
    "    {\n",
    "        'n_components': 40,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "    {\n",
    "        'n_components': 35,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "    {\n",
    "        'n_components': 33,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "    {\n",
    "        'n_components': 30,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f{client} Starting....\n",
      "PCA explained variance ratio: [0.17908248 0.15599521 0.10360098 0.06265984 0.05702301 0.04744612\n",
      " 0.03931341 0.03542901 0.0312049  0.02462718 0.02393085 0.02360545\n",
      " 0.02267465 0.02070881 0.01815602 0.01624444 0.01496836 0.01323305\n",
      " 0.01270404 0.01132342 0.01050242 0.01010683 0.00901977 0.00840257\n",
      " 0.00798672 0.00691281 0.00577896 0.0051003  0.0035652  0.00234264\n",
      " 0.00217048 0.00202068 0.00176953 0.00147195 0.00125896 0.00112544\n",
      " 0.00104076 0.00085804 0.00071288 0.00059623]\n",
      "(335999, 41)\n",
      "(335999, 36)\n",
      "(335999, 34)\n",
      "(335999, 31)\n",
      "-----------------------------------------------------------------------\n",
      "f{client} Starting....\n",
      "PCA explained variance ratio: [0.17568192 0.12473658 0.09752492 0.07552202 0.06113587 0.04641158\n",
      " 0.04374342 0.03611841 0.03387959 0.03110569 0.02618503 0.02230072\n",
      " 0.02165586 0.02130531 0.01937251 0.01770442 0.01641674 0.01333521\n",
      " 0.01253844 0.01200517 0.01183246 0.01103053 0.01025419 0.00834167\n",
      " 0.00781151 0.00677549 0.00578028 0.005172   0.00495851 0.00346764\n",
      " 0.00267746 0.00220526 0.00159477 0.00141242 0.00128583 0.00115429\n",
      " 0.00069655 0.00066433 0.00061219 0.00050331]\n",
      "(336000, 41)\n",
      "(336000, 36)\n",
      "(336000, 34)\n",
      "(336000, 31)\n",
      "-----------------------------------------------------------------------\n",
      "f{client} Starting....\n",
      "PCA explained variance ratio: [0.17891667 0.14315922 0.09935361 0.07356371 0.05880952 0.04687952\n",
      " 0.04552997 0.04231053 0.03581232 0.02993947 0.02525125 0.02227518\n",
      " 0.02185979 0.01960318 0.01782566 0.01618198 0.0125651  0.0119683\n",
      " 0.01179574 0.01065376 0.00966354 0.00850855 0.00754503 0.00678815\n",
      " 0.00623966 0.00576979 0.00541763 0.00445798 0.00395761 0.00295314\n",
      " 0.00284247 0.00206224 0.00153527 0.0012264  0.00110188 0.00085907\n",
      " 0.00063567 0.00059057 0.00049992 0.000424  ]\n",
      "(336000, 41)\n",
      "(336000, 36)\n",
      "(336000, 34)\n",
      "(336000, 31)\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for client, info in clients.items():\n",
    "    print(\"f{client} Starting....\")\n",
    "    train_df = pd.read_csv(info.get('scaled_train'))\n",
    "    test_df = pd.read_csv(info.get('scaled_test'))\n",
    "    # Step 3: Perform PCA with the maximum number of components (e.g., 40)\n",
    "    train_pca_full, test_pca_full, train_labels, test_labels = perform_pca_and_save_with_max(train_df, test_df, max_components=40, pca_path=info.get(\"pca_path\"))\n",
    "    \n",
    "    \n",
    "    # Step 4: Slice the PCA components and add labels back (e.g., for 40, 35, 33, 30 components)\n",
    "    for p in pca_configurations:        \n",
    "        train_pca_data, test_pca_data = add_labels_to_pca(train_pca_full, test_pca_full, train_labels, test_labels, num_components=p.get('n_components'))\n",
    "        train_pca_data.to_csv(p.get('train_path').format(client, p.get('n_components'), p.get('n_components')),  index=False)\n",
    "        test_pca_data.to_csv(p.get('test_path').format(client, p.get('n_components'), p.get('n_components')),  index=False)\n",
    "        print(train_pca_data.shape)\n",
    "    print(\"-----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # This is just the full version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
