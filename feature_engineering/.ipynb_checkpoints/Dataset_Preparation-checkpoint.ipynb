{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_csv_files(directory_path, features=None):\n",
    "    # Initialize an empty list to store dataframes from CSV files\n",
    "    dataframes = []\n",
    "\n",
    "    # Get a list of all files in the directory\n",
    "    file_list = os.listdir(directory_path)\n",
    "\n",
    "    # Loop through each file and check if it's a CSV file\n",
    "    for file_number, file_name in enumerate(file_list):\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Get the full file path\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            # Read the CSV file into a pandas DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Remove leading and trailing spaces from column names\n",
    "            df.columns = df.columns.str.strip()\n",
    "            # Append the DataFrame to the list\n",
    "            df['CSV_File_Number'] = file_number\n",
    "            #dataframes.append(df[features])\n",
    "            dataframes.append(df) #as it is feature analysis, we are taking all the features\n",
    "\n",
    "    # Merge all DataFrames into a single DataFrame\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_data_frames_updated(dataframe, remove_infinity=True, remove_null=True):\n",
    "\n",
    "    if remove_infinity:\n",
    "        numeric_cols = dataframe.select_dtypes(include=[np.number]).columns\n",
    "        infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n",
    "        for col, count in infinite_counts.items():\n",
    "            if count != 0:\n",
    "                dataframe = dataframe[~np.isinf(dataframe[col])]\n",
    "\n",
    "    if remove_null:\n",
    "        null_counts = dataframe.isnull().sum()\n",
    "        for col, count in null_counts.items():\n",
    "            if count != 0:\n",
    "                    dataframe = dataframe.dropna(subset=[col])\n",
    "    print(\"Sanitized Row Count:\", dataframe.shape[0])    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all dataset sources to make iterate to read csv files\n",
    "dataset_sources = {\n",
    "\n",
    "    ## Benign Traffic      \n",
    "    'client_1': {\n",
    "        'benign': '../row_data/client_1/benign',\n",
    "        'attack': '../row_data/client_1/attack',\n",
    "    },\n",
    "    'client_2': {\n",
    "        'benign': '../row_data/client_2/benign',\n",
    "        'attack': '../row_data/client_2/attack',\n",
    "    },\n",
    "    'client_3': {\n",
    "        'benign': '../row_data/client_3/benign',\n",
    "        'attack': '../row_data/client_3/attack',\n",
    "    },\n",
    "    'client_4': {\n",
    "        'benign': '../row_data/client_4/benign',\n",
    "        'attack': '../row_data/client_4/attack',\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Uniform Sample\n",
    "def get_uniform_sample(df, group_col=[], sample_size=42000):\n",
    "    \n",
    "    #Separate the labels\n",
    "    label_0 = df[df['Label'] == 0]\n",
    "    label_1 = df[df['Label'] == 1]\n",
    "\n",
    "    count_0 = count_1 = 0 # Initialize with 0\n",
    "\n",
    "    if len(label_0) >= sample_size//2 and len(label_1) >= sample_size//2:\n",
    "        count_0 = count_1 = sample_size//2,\n",
    "    elif len(label_0) < sample_size//2 and len(label_1) > sample_size//2:\n",
    "        count_0 = len(label_0)\n",
    "        count_1 = sample_size - count_0\n",
    "    elif len(label_0) > sample_size//2 and len(label_1) < sample_size//2:\n",
    "        count_1 = len(label_1)\n",
    "        count_0 = sample_size - count_1\n",
    "    else:\n",
    "        count_0 = len(label_0)\n",
    "        count_1 = len(label_1)\n",
    "\n",
    "    sample_0, _ = train_test_split(\n",
    "        label_0,\n",
    "        train_size=count_0,\n",
    "        stratify=df['CSV_File_Number'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    sample_1, _ = train_test_split(\n",
    "        label_1,\n",
    "        train_size=count_1,\n",
    "        stratify=df['CSV_File_Number'],\n",
    "        random_state=42\n",
    "    )        \n",
    "    \n",
    "    # Concatenate and shuffle the samples\n",
    "    combined_sample = pd.concat([sample_0, sample_1])\n",
    "    shuffled_sample = combined_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    return shuffled_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 23464.64it/s]\n",
      "  0%|                                                                                                   | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_1, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_573820/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 202821\n",
      " Loading....Clinet = client_1, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_573820/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 2671528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_573820/725653109.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group.groupby('CSV_File_Number').apply(\n",
      "/tmp/ipykernel_573820/725653109.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group.groupby('CSV_File_Number').apply(\n",
      " 25%|██████████████████████▊                                                                    | 1/4 [01:35<04:45, 95.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_2, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_573820/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 185943\n",
      " Loading....Clinet = client_2, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_573820/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 3346296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_573820/725653109.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group.groupby('CSV_File_Number').apply(\n",
      "/tmp/ipykernel_573820/725653109.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group.groupby('CSV_File_Number').apply(\n",
      " 50%|█████████████████████████████████████████████                                             | 2/4 [03:38<03:43, 111.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_3, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_573820/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 223200\n",
      " Loading....Clinet = client_3, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_573820/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 2348465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_573820/725653109.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group.groupby('CSV_File_Number').apply(\n",
      "/tmp/ipykernel_573820/725653109.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group.groupby('CSV_File_Number').apply(\n",
      " 75%|███████████████████████████████████████████████████████████████████▌                      | 3/4 [05:15<01:44, 104.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_4, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_573820/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 246797\n",
      " Loading....Clinet = client_4, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_573820/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 2253978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_573820/725653109.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group.groupby('CSV_File_Number').apply(\n",
      "/tmp/ipykernel_573820/725653109.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group.groupby('CSV_File_Number').apply(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 4/4 [06:46<00:00, 101.52s/it]\n"
     ]
    }
   ],
   "source": [
    "##Reading all the data\n",
    "features = None\n",
    "\n",
    "\n",
    "for client, data in tqdm(dataset_sources.items(), total=len(dataset_sources)):\n",
    "    locals()[client] = {}\n",
    "\n",
    "for client, data in tqdm(dataset_sources.items(), total=len(dataset_sources)):\n",
    "    client_dataframe = []\n",
    "    for type, path in data.items():\n",
    "        print(f' Loading....Clinet = {client}, type = {type}')\n",
    "        dataframe = read_all_csv_files(path, features)\n",
    "        dataframe = sanitize_data_frames_updated(dataframe)\n",
    "        if type == 'benign':\n",
    "            dataframe['Label'] = 0\n",
    "        else:\n",
    "             dataframe['Label'] = 1\n",
    "        client_dataframe.append(dataframe)\n",
    "        #locals()[client][type] = dataframe\n",
    "    client_merged_df = get_uniform_sample(pd.concat(client_dataframe, ignore_index=True), ['Label', 'CSV_File_Number'], 420000 )\n",
    "    client_merged_df.to_csv(f'./dataset/{client}/dataset.csv', index=False)\n",
    "\n",
    "    #Remove unwanted column\n",
    "    client_merged_df =  client_merged_df.drop(['CSV_File_Number'], axis=1)\n",
    "    train_size = 0.8  # 80% for training, 20% for testing\n",
    "    train_df, test_df = train_test_split(client_merged_df, train_size=train_size, random_state=42, stratify=client_merged_df['Label'])\n",
    "   \n",
    "\n",
    "\n",
    "    scaler_path = './scalers/{client}.pkl'\n",
    "\n",
    "    train_df = scale_dataset(train_df, 'train', scaler_path)    \n",
    "    train_df.to_csv(f'./dataset/{client}/train/{client}_train.csv', index=False)\n",
    "\n",
    "    test_df = scale_dataset(test_df, 'test', scaler_path)\n",
    "    test_df.to_csv(f'./dataset/{client}/test/{client}_test.csv', index=False)    \n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_dataset(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Step 2: Create a pivot table\n",
    "    pivot_table = pd.pivot_table(df, values='PSH Flag Count',  # Replace 'AnyValueColumn' with the column you want to aggregate\n",
    "                                 index='CSV_File_Number',\n",
    "                                 columns='Label',\n",
    "                                 aggfunc='count')  # 'count' or any other function depending on your needs\n",
    "    \n",
    "   # Step 3: Calculate total records based on Label\n",
    "    label_totals = df['Label'].value_counts()\n",
    "    \n",
    "    # Step 4: Display the pivot table and the label totals\n",
    "    print(\"Pivot Table:\")\n",
    "    print(pivot_table)\n",
    "    print(\"\\nTotal Records by Label:\")\n",
    "    print(label_totals)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_1\n",
      "Pivot Table:\n",
      "Label                  0        1\n",
      "CSV_File_Number                  \n",
      "0                50705.0      4.0\n",
      "1                50705.0  20282.0\n",
      "2                    3.0  20282.0\n",
      "3                48231.0      NaN\n",
      "4                    NaN  20282.0\n",
      "5                    NaN  20282.0\n",
      "6                    NaN  20282.0\n",
      "7                    NaN  20282.0\n",
      "8                    NaN  20282.0\n",
      "9                    NaN  20282.0\n",
      "10                   NaN  20282.0\n",
      "\n",
      "Total Records by Label:\n",
      "Label\n",
      "1    182542\n",
      "0    149644\n",
      "Name: count, dtype: int64\n",
      "___________________________________________________________________\n",
      "client_2\n",
      "Pivot Table:\n",
      "Label                  0        1\n",
      "CSV_File_Number                  \n",
      "0                46485.0  18594.0\n",
      "1                    3.0  18594.0\n",
      "2                46485.0  18594.0\n",
      "3                23669.0  18594.0\n",
      "4                    NaN  18594.0\n",
      "5                    NaN  18594.0\n",
      "6                    NaN  18594.0\n",
      "7                    NaN  18594.0\n",
      "8                    NaN  18594.0\n",
      "9                    NaN  18594.0\n",
      "\n",
      "Total Records by Label:\n",
      "Label\n",
      "1    185940\n",
      "0    116642\n",
      "Name: count, dtype: int64\n",
      "___________________________________________________________________\n",
      "client_3\n",
      "Pivot Table:\n",
      "Label                  0        1\n",
      "CSV_File_Number                  \n",
      "0                23333.0  26250.0\n",
      "1                23333.0  26250.0\n",
      "2                   23.0  26250.0\n",
      "3                   12.0  26250.0\n",
      "4                19473.0  26250.0\n",
      "5                   24.0  26250.0\n",
      "6                 4042.0  26250.0\n",
      "7                    4.0  26250.0\n",
      "8                23333.0      NaN\n",
      "\n",
      "Total Records by Label:\n",
      "Label\n",
      "1    210000\n",
      "0     93577\n",
      "Name: count, dtype: int64\n",
      "___________________________________________________________________\n",
      "client_4\n",
      "Pivot Table:\n",
      "Label                  0        1\n",
      "CSV_File_Number                  \n",
      "0                   17.0  26250.0\n",
      "1                23333.0  26250.0\n",
      "2                    4.0  26250.0\n",
      "3                23333.0  26250.0\n",
      "4                   13.0  26250.0\n",
      "5                    5.0  26250.0\n",
      "6                23333.0  26250.0\n",
      "7                23333.0  26250.0\n",
      "8                 3053.0      NaN\n",
      "\n",
      "Total Records by Label:\n",
      "Label\n",
      "1    210000\n",
      "0     96424\n",
      "Name: count, dtype: int64\n",
      "___________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#all dataset sources to make iterate to read csv files\n",
    "dataset_sources = {\n",
    "\n",
    "    ## Benign Traffic      \n",
    "    'client_1': './dataset/client_1/dataset.csv',\n",
    "    'client_2': './dataset/client_2/dataset.csv',\n",
    "    'client_3': './dataset/client_3/dataset.csv',\n",
    "    'client_4': './dataset/client_4/dataset.csv',\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "for key, data_source in dataset_sources.items():\n",
    "    print(key)\n",
    "    inspect_dataset(data_source)\n",
    "    print(\"___________________________________________________________________\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
