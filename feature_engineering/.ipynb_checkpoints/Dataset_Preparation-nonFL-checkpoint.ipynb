{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overall DataSet Preparation from Row data\n",
    "- Read all the row feature files from the directory specified\n",
    "- Sanitize all the inputs\n",
    "- Take sample count based on attack and benign csv files (technically it's class)\n",
    "- Save the dataset locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_csv_files(directory_path, features=None):\n",
    "    # Initialize an empty list to store dataframes from CSV files\n",
    "    dataframes = []\n",
    "\n",
    "    # Get a list of all files in the directory\n",
    "    file_list = os.listdir(directory_path)\n",
    "\n",
    "    # Loop through each file and check if it's a CSV file\n",
    "    for file_number, file_name in enumerate(file_list):\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Get the full file path\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            # Read the CSV file into a pandas DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Remove leading and trailing spaces from column names\n",
    "            df.columns = df.columns.str.strip()\n",
    "            # Append the DataFrame to the list\n",
    "            df['CSV_File_Number'] = file_number\n",
    "            #dataframes.append(df[features])\n",
    "            dataframes.append(df) #as it is feature analysis, we are taking all the features\n",
    "            #print(f\"file number: {file_number}; file name: {file_name}\")\n",
    "\n",
    "    # Merge all DataFrames into a single DataFrame\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_data_frames_updated(dataframe, remove_infinity=True, remove_null=True):\n",
    "\n",
    "    if remove_infinity:\n",
    "        numeric_cols = dataframe.select_dtypes(include=[np.number]).columns\n",
    "        infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n",
    "        for col, count in infinite_counts.items():\n",
    "            if count != 0:\n",
    "                dataframe = dataframe[~np.isinf(dataframe[col])]\n",
    "\n",
    "    if remove_null:\n",
    "        null_counts = dataframe.isnull().sum()\n",
    "        for col, count in null_counts.items():\n",
    "            if count != 0:\n",
    "                    dataframe = dataframe.dropna(subset=[col])\n",
    "    print(\"Sanitized Row Count:\", dataframe.shape[0])    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all dataset sources to make iterate to read csv files\n",
    "dataset_sources = {\n",
    " \n",
    "    'client_1': {\n",
    "        'benign': '../row_data_nonIID/client_1/benign',\n",
    "        'attack': '../row_data_nonIID/client_1/attack',\n",
    "    },\n",
    "    'client_2': {\n",
    "        'benign': '../row_data_nonIID/client_2/benign',\n",
    "        'attack': '../row_data_nonIID/client_2/attack',\n",
    "    },\n",
    "    'client_3': {\n",
    "        'benign': '../row_data_nonIID/client_3/benign',\n",
    "        'attack': '../row_data_nonIID/client_3/attack',\n",
    "    },\n",
    "    'client_4': {\n",
    "        'benign': '../row_data_nonIID/client_4/benign',\n",
    "        'attack': '../row_data_nonIID/client_4/attack',\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Uniform Sample\n",
    "def get_uniform_sample(df, group_col=[], sample_size=420000):\n",
    "    \n",
    "    #Separate the labels\n",
    "    label_0 = df[df['Label'] == 0]\n",
    "    label_1 = df[df['Label'] == 1]\n",
    "\n",
    "    count_0 = count_1 = 0 # Initialize with 0\n",
    "\n",
    "    if len(label_0) >= sample_size//2 and len(label_1) >= sample_size//2:\n",
    "        count_0 = count_1 = sample_size//2\n",
    "    elif len(label_0) < sample_size//2 and len(label_1) > sample_size//2:\n",
    "        count_0 = len(label_0)\n",
    "        count_1 = sample_size - count_0\n",
    "    elif len(label_0) > sample_size//2 and len(label_1) < sample_size//2:\n",
    "        count_1 = len(label_1)\n",
    "        count_0 = sample_size - count_1\n",
    "    else:\n",
    "        count_0 = len(label_0)\n",
    "        count_1 = len(label_1)\n",
    "\n",
    "    print(f\"count_0: {count_0}; count_1: {count_1}\")\n",
    "    print(f\"label_0: {len(label_0)}; label_1: {len(label_1)}\")\n",
    "\n",
    "    train_size_0 = 1.0 if count_0 >= len(label_0) else float(count_0/len(label_0))\n",
    "    train_size_1 = 1.0 if count_1 >= len(label_1) else float(count_1/len(label_1))\n",
    "\n",
    "    print(f\"train_size_0: {train_size_0}; train_size_1: {train_size_1}\")\n",
    "    \n",
    "# Handle edge cases where train_size is 1.0 and stratification might fail\n",
    "    if train_size_0 == 1.0:\n",
    "        sample_0 = label_0.copy()  # Use the whole subset without train_test_split\n",
    "    else:\n",
    "        sample_0, _ = train_test_split(\n",
    "            label_0,\n",
    "            train_size=train_size_0,\n",
    "            stratify=label_0['CSV_File_Number'] if group_col else None,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if train_size_1 == 1.0:\n",
    "        sample_1 = label_1.copy()  # Use the whole subset without train_test_split\n",
    "    else:\n",
    "        sample_1, _ = train_test_split(\n",
    "            label_1,\n",
    "            train_size=train_size_1,\n",
    "            stratify=label_1['CSV_File_Number'] if group_col else None,\n",
    "            random_state=42\n",
    "        )   \n",
    "    \n",
    "    # Concatenate and shuffle the samples\n",
    "    combined_sample = pd.concat([sample_0, sample_1])\n",
    "    shuffled_sample = combined_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    return shuffled_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory if it doesn't exist\n",
    "# THis will create subdirectoy as well\n",
    "def ensure_directory_exists(filepath):\n",
    "    directory = os.path.dirname(filepath)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 67650.06it/s]\n",
      "  0%|                                                                            | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_1, type = benign\n",
      " Loading....Clinet = client_1, type = attack\n",
      "count_0: 202821; count_1: 217179\n",
      "label_0: 202821; label_1: 1048580\n",
      "train_size_0: 1.0; train_size_1: 0.20711724427320757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████                                                   | 1/4 [00:27<01:21, 27.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_2, type = benign\n",
      " Loading....Clinet = client_2, type = attack\n",
      "count_0: 185943; count_1: 234057\n",
      "label_0: 185943; label_1: 1360150\n",
      "train_size_0: 1.0; train_size_1: 0.1720817556887108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████                                  | 2/4 [00:57<00:58, 29.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_3, type = benign\n",
      " Loading....Clinet = client_3, type = attack\n",
      "count_0: 210000; count_1: 210000\n",
      "label_0: 223200; label_1: 1239407\n",
      "train_size_0: 0.9408602150537635; train_size_1: 0.16943586731396548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████                 | 3/4 [01:28<00:30, 30.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_4, type = benign\n",
      " Loading....Clinet = client_4, type = attack\n",
      "count_0: 210000; count_1: 210000\n",
      "label_0: 246797; label_1: 1048576\n",
      "train_size_0: 0.850901753262803; train_size_1: 0.2002716064453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 4/4 [01:57<00:00, 29.34s/it]\n"
     ]
    }
   ],
   "source": [
    "##Reading all the data\n",
    "features = None\n",
    "\n",
    "\n",
    "for client, data in tqdm(dataset_sources.items(), total=len(dataset_sources)):\n",
    "    locals()[client] = {}\n",
    "\n",
    "for client, data in tqdm(dataset_sources.items(), total=len(dataset_sources)):\n",
    "    client_dataframe = []\n",
    "    for type, path in data.items():\n",
    "        print(f' Loading....Clinet = {client}, type = {type}')\n",
    "        dataframe = read_all_csv_files(path, features)\n",
    "        #dataframe = sanitize_data_frames_updated(dataframe)\n",
    "        if type == 'benign':\n",
    "            dataframe['Label'] = 0\n",
    "        else:\n",
    "             dataframe['Label'] = 1\n",
    "        client_dataframe.append(dataframe)\n",
    "        #locals()[client][type] = dataframe\n",
    "    client_merged_df = get_uniform_sample(pd.concat(client_dataframe, ignore_index=True), ['Label', 'CSV_File_Number'], 420000 )\n",
    "    client_merged_df.to_csv(ensure_directory_exists(f'./dataset/{client}/{client}_original_dataset.csv'), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Train and Test Set Segregation\n",
    "- Read datasets saved in Step 1\n",
    "- Segregate Train and Test set and save locally. As we need to take decission based on csv file number and label, we will combine both column to stratify\n",
    "- Sacling Training data save the scaled data and save the scalers as well\n",
    "- Use saved scalers to scale the test data and save the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_exclude = ['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Timestamp', 'CSV_File_Number', 'Label', 'Stratify']\n",
    "output_features = ['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iteration Friendly dictionary\n",
    "clients = {\n",
    " \n",
    "    'client_1': {\n",
    "        'combined': './dataset/client_1/client_1_original_dataset.csv',\n",
    "        'train': './dataset/client_1/fold_{}/client_1_train_dataset.csv',\n",
    "        'test': './dataset/client_1/fold_{}/client_1_test_dataset.csv',\n",
    "        'scaler': './dataset/client_1/fold_{}/client_1_train_scaler.pkl',\n",
    "        'scaled_train': './dataset/client_1/fold_{}/client_1_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_1/fold_{}/client_1_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_2': {\n",
    "        'combined': './dataset/client_2/client_2_original_dataset.csv',\n",
    "        'train': './dataset/client_2/fold_{}/client_2_train_dataset.csv',\n",
    "        'test': './dataset/client_2/fold_{}/client_2_test_dataset.csv',\n",
    "        'scaler': './dataset/client_2/fold_{}/client_2_train_scaler.pkl',\n",
    "        'scaled_train': './dataset/client_2/fold_{}/client_2_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_2/fold_{}/client_2_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_3': {\n",
    "        'combined': './dataset/client_3/client_3_original_dataset.csv',\n",
    "        'train': './dataset/client_3/fold_{}/client_3_train_dataset.csv',\n",
    "        'test': './dataset/client_3/fold_{}/client_3_test_dataset.csv',\n",
    "        'scaler': './dataset/client_3/fold_{}/client_3_train_scaler.pkl',\n",
    "        'scaled_train': './dataset/client_3/fold_{}/client_3_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_3/fold_{}/client_3_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_4': {\n",
    "        'combined': './dataset/client_4/client_4_original_dataset.csv',\n",
    "        'train': './dataset/client_4/fold_{}/client_4_train_dataset.csv',\n",
    "        'test': './dataset/client_4/fold_{}/client_4_test_dataset.csv',\n",
    "        'scaler': './dataset/client_4/fold_{}/client_4_train_scaler.pkl',\n",
    "        'scaled_train': './dataset/client_4/fold_{}/client_4_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_4/fold_{}/client_4_sclaed_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_remove_unwanted_features(dataframe, feature_to_exclude, out_features, scaler_path, type='train'):\n",
    "\n",
    "    features = dataframe.columns.values.tolist()\n",
    "    input_features = [feature for feature in features if feature not in feature_to_exclude] \n",
    "    output_df = dataframe[out_features]\n",
    "    input_df = dataframe[input_features]\n",
    "\n",
    "    print(f\"Input features shape: {input_df.shape}\")  # Debugging\n",
    "    print(f\"Output (label) features shape: {output_df.shape}\")  # Debugging\n",
    "\n",
    "    # Reset index for both input and output DataFrames to ensure correct alignment\n",
    "    input_df = input_df.reset_index(drop=True)\n",
    "    output_df = output_df.reset_index(drop=True)\n",
    "    \n",
    "    if type == 'train':\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(input_df)\n",
    "        scaled_df = pd.DataFrame(scaled_data, columns=input_df.columns)\n",
    "        print(f\"Scaled features shape: {scaled_df.shape}\")  # Debugging\n",
    "        \n",
    "        merged_df = pd.concat([scaled_df, output_df], axis=1)\n",
    "        print(f\"Merged DataFrame shape: {merged_df.shape}\")  # Debugging\n",
    "\n",
    "        # Save the scaler to a file to be used for test set\n",
    "        with open(scaler_path, \"wb\") as file:\n",
    "            pickle.dump(scaler, file)\n",
    "    else:\n",
    "        with open(scaler_path, \"rb\") as file:\n",
    "            scaler = pickle.load(file)\n",
    "            scaled_data = scaler.transform(input_df)  # Changed from fit_transform to transform\n",
    "            scaled_df = pd.DataFrame(scaled_data, columns=input_df.columns)\n",
    "            print(f\"Scaled features shape: {scaled_df.shape}\")  # Debugging\n",
    "            merged_df = pd.concat([scaled_df, output_df], axis=1)\n",
    "            print(f\"Merged DataFrame shape: {merged_df.shape}\")  # Debugging\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "Started for Client: client_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started for Split: 1\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_1\n",
      "Started for Split: 2\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_1\n",
      "Started for Split: 3\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_1\n",
      "Started for Split: 4\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_1\n",
      "Started for Split: 5\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████▊                                                  | 1/4 [03:59<11:58, 239.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End for Client: client_1\n",
      "------------------------------------------------------------------------------\n",
      "Started for Client: client_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started for Split: 1\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_2\n",
      "Started for Split: 2\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_2\n",
      "Started for Split: 3\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_2\n",
      "Started for Split: 4\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_2\n",
      "Started for Split: 5\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████▌                                 | 2/4 [08:15<08:18, 249.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End for Client: client_2\n",
      "------------------------------------------------------------------------------\n",
      "Started for Client: client_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started for Split: 1\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_3\n",
      "Started for Split: 2\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_3\n",
      "Started for Split: 3\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_3\n",
      "Started for Split: 4\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_3\n",
      "Started for Split: 5\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████████████████████████▎                | 3/4 [12:18<04:06, 246.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End for Client: client_3\n",
      "------------------------------------------------------------------------------\n",
      "Started for Client: client_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started for Split: 1\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_4\n",
      "Started for Split: 2\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_4\n",
      "Started for Split: 3\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_4\n",
      "Started for Split: 4\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n",
      "End for Client: client_4\n",
      "Started for Split: 5\n",
      "Input features shape: (336000, 85)\n",
      "Output (label) features shape: (336000, 1)\n",
      "Scaled features shape: (336000, 85)\n",
      "Merged DataFrame shape: (336000, 86)\n",
      "Input features shape: (84000, 85)\n",
      "Output (label) features shape: (84000, 1)\n",
      "Scaled features shape: (84000, 85)\n",
      "Merged DataFrame shape: (84000, 86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 4/4 [16:41<00:00, 250.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End for Client: client_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for client, info in tqdm(clients.items(), total=len(clients)):\n",
    "    print('------------------------------------------------------------------------------')\n",
    "    print(f\"Started for Client: {client}\")\n",
    "    df = pd.read_csv(info.get('combined'))\n",
    "    #combinding label and csv file number to stratify\n",
    "    df['Stratify'] = df['Label'].astype(str) + '_' + df['CSV_File_Number'].astype(str)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    #Loop through the splitted items:\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(df, df['Stratify']), 1):\n",
    "        print(f\"Started for Split: {fold}\")\n",
    "        # Create training and test sets for the current fold\n",
    "        train_df = df.iloc[train_index].reset_index(drop=True)\n",
    "        test_df = df.iloc[test_index].reset_index(drop=True)    \n",
    "\n",
    "        #checking if fold path exists. if not it will create one\n",
    "        #ensure_directory_exists(info.get('train').format(fold))\n",
    "        \n",
    "        train_df.to_csv(ensure_directory_exists(info.get('train').format(fold)), index=False) ##It will contain all column with additional column\n",
    "        test_df.to_csv(ensure_directory_exists(info.get('test').format(fold)), index=False) ##It will contain all column with additional column\n",
    "    \n",
    "        scaled_train_df = scale_and_remove_unwanted_features(train_df, feature_to_exclude, output_features, info.get('scaler').format(fold), 'train')\n",
    "        #scaled_train_df.describe()\n",
    "        scaled_train_df.to_csv(ensure_directory_exists(info.get('scaled_train')).format(fold), index=False)\n",
    "    \n",
    "        scaled_test_df = scale_and_remove_unwanted_features(test_df, feature_to_exclude, output_features, info.get('scaler').format(fold), 'test')\n",
    "        #scaled_test_df.describe()\n",
    "        scaled_test_df.to_csv(ensure_directory_exists(info.get('scaled_test').format(fold)), index=False)\n",
    "        print(f\"End for Client: {client}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Analysis of Prepared Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client: client_1\n",
      "Client: client_2\n",
      "Client: client_3\n",
      "Client: client_4\n"
     ]
    }
   ],
   "source": [
    "#pd.concat(client_dataframe, ignore_index=True)\n",
    "intersted_features = ['Label', 'CSV_File_Number', 'Stratify']\n",
    "train_df_all_clients = []\n",
    "test_df_all_clients = []\n",
    "for client, info in clients.items():\n",
    "    print(f'Client: {client}')\n",
    "    for fold in range(1, 6):\n",
    "        train_df = pd.read_csv(info.get('train').format(fold))\n",
    "        test_df = pd.read_csv(info.get('test').format(fold))\n",
    "\n",
    "        train_df = train_df[intersted_features]\n",
    "        test_df = test_df[intersted_features]\n",
    "        \n",
    "        train_df['client'] = client\n",
    "        train_df['fold'] = fold\n",
    "        \n",
    "        test_df['client'] = client\n",
    "        test_df['fold'] = fold\n",
    "\n",
    "        train_df_all_clients.append(train_df)\n",
    "        test_df_all_clients.append(test_df)\n",
    "        \n",
    "merged_train_df = pd.concat(train_df_all_clients, ignore_index=True)\n",
    "merged_test_df = pd.concat(test_df_all_clients, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Client Stratify   Fold_1   Fold_2   Fold_3   Fold_4   Fold_5\n",
      "0   client_1      0_0  82051.0  82051.0  82051.0  82051.0  82052.0\n",
      "1   client_1      0_1  41618.0  41618.0  41619.0  41619.0  41618.0\n",
      "2   client_1      0_2      3.0      3.0      2.0      2.0      2.0\n",
      "3   client_1      0_3  38585.0  38584.0  38585.0  38585.0  38585.0\n",
      "4   client_1      1_0  43436.0  43436.0  43436.0  43436.0  43436.0\n",
      "5   client_1      1_1  43436.0  43436.0  43436.0  43436.0  43436.0\n",
      "6   client_1      1_2  43436.0  43436.0  43436.0  43436.0  43436.0\n",
      "7   client_1      1_3  43435.0  43436.0  43435.0  43435.0  43435.0\n",
      "8   client_2      0_0  46682.0  46682.0  46682.0  46683.0  46683.0\n",
      "9   client_2      0_1      3.0      3.0      2.0      2.0      2.0\n",
      "10  client_2      0_2  83134.0  83134.0  83134.0  83135.0  83135.0\n",
      "11  client_2      0_3  18935.0  18935.0  18936.0  18935.0  18935.0\n",
      "12  client_2      1_0  36088.0  36088.0  36088.0  36088.0  36088.0\n",
      "13  client_2      1_1  36088.0  36088.0  36088.0  36088.0  36088.0\n",
      "14  client_2      1_2  36088.0  36088.0  36088.0  36088.0  36088.0\n",
      "15  client_2      1_3  78982.0  78982.0  78982.0  78981.0  78981.0\n",
      "16  client_3      0_0  37895.0  37895.0  37896.0  37895.0  37895.0\n",
      "17  client_3      0_1  41041.0  41041.0  41041.0  41041.0  41040.0\n",
      "18  client_3      0_2     17.0     17.0     18.0     18.0     18.0\n",
      "19  client_3      0_3      9.0      9.0      9.0      9.0      8.0\n",
      "20  client_3      0_4  14656.0  14657.0  14657.0  14657.0  14657.0\n",
      "21  client_3      0_5     19.0     19.0     18.0     18.0     18.0\n",
      "22  client_3      0_6   3043.0   3043.0   3042.0   3042.0   3042.0\n",
      "23  client_3      0_7      3.0      3.0      3.0      3.0      4.0\n",
      "24  client_3      0_8  71316.0  71317.0  71317.0  71317.0  71317.0\n",
      "25  client_3      1_0  69599.0  69598.0  69598.0  69598.0  69599.0\n",
      "26  client_3      1_1  35533.0  35533.0  35533.0  35533.0  35532.0\n",
      "27  client_3      1_2  27335.0  27335.0  27335.0  27335.0  27336.0\n",
      "28  client_3      1_3  35534.0  35533.0  35533.0  35534.0  35534.0\n",
      "29  client_4      0_0     11.0     12.0     11.0     11.0     11.0\n",
      "30  client_4      0_1  53876.0  53876.0  53876.0  53876.0  53876.0\n",
      "31  client_4      0_2      2.0      2.0      3.0      3.0      2.0\n",
      "32  client_4      0_3  23495.0  23495.0  23494.0  23494.0  23494.0\n",
      "33  client_4      0_4      9.0      9.0      9.0      8.0      9.0\n",
      "34  client_4      0_5      4.0      3.0      3.0      3.0      3.0\n",
      "35  client_4      0_6  24611.0  24611.0  24612.0  24611.0  24611.0\n",
      "36  client_4      0_7  63914.0  63914.0  63914.0  63915.0  63915.0\n",
      "37  client_4      0_8   2078.0   2078.0   2078.0   2079.0   2079.0\n",
      "38  client_4      1_0  42000.0  42000.0  42000.0  42000.0  42000.0\n",
      "39  client_4      1_1  42000.0  42000.0  42000.0  42000.0  42000.0\n",
      "40  client_4      1_2  42000.0  42000.0  42000.0  42000.0  42000.0\n",
      "41  client_4      1_3  42000.0  42000.0  42000.0  42000.0  42000.0\n"
     ]
    }
   ],
   "source": [
    "#print(merged_train_df.groupby(['client', 'fold', 'Stratify']).count())\n",
    "grouped_counts = merged_train_df.groupby(['client', 'fold', 'Stratify']).size().reset_index(name='count')\n",
    "# Pivoting the 'fold' column to make it into a separate column\n",
    "pivot_df = grouped_counts.pivot_table(index=['client', 'Stratify'], columns='fold', values='count', fill_value=0).reset_index()\n",
    "pivot_df.columns = ['Client', 'Stratify'] + [f'Fold_{int(col)}' for col in pivot_df.columns if isinstance(col, int)]\n",
    "pivot_df.to_csv(ensure_directory_exists(\"./dataset/dataset_summary.csv\"), index=False)\n",
    "print(pivot_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Client Stratify   Fold_1   Fold_2   Fold_3   Fold_4   Fold_5\n",
      "0   client_1      0_0  20513.0  20513.0  20513.0  20513.0  20512.0\n",
      "1   client_1      0_1  10405.0  10405.0  10404.0  10404.0  10405.0\n",
      "2   client_1      0_2      0.0      0.0      1.0      1.0      1.0\n",
      "3   client_1      0_3   9646.0   9647.0   9646.0   9646.0   9646.0\n",
      "4   client_1      1_0  10859.0  10859.0  10859.0  10859.0  10859.0\n",
      "5   client_1      1_1  10859.0  10859.0  10859.0  10859.0  10859.0\n",
      "6   client_1      1_2  10859.0  10859.0  10859.0  10859.0  10859.0\n",
      "7   client_1      1_3  10859.0  10858.0  10859.0  10859.0  10859.0\n",
      "8   client_2      0_0  11671.0  11671.0  11671.0  11670.0  11670.0\n",
      "9   client_2      0_1      0.0      0.0      1.0      1.0      1.0\n",
      "10  client_2      0_2  20784.0  20784.0  20784.0  20783.0  20783.0\n",
      "11  client_2      0_3   4734.0   4734.0   4733.0   4734.0   4734.0\n",
      "12  client_2      1_0   9022.0   9022.0   9022.0   9022.0   9022.0\n",
      "13  client_2      1_1   9022.0   9022.0   9022.0   9022.0   9022.0\n",
      "14  client_2      1_2   9022.0   9022.0   9022.0   9022.0   9022.0\n",
      "15  client_2      1_3  19745.0  19745.0  19745.0  19746.0  19746.0\n",
      "16  client_3      0_0   9474.0   9474.0   9473.0   9474.0   9474.0\n",
      "17  client_3      0_1  10260.0  10260.0  10260.0  10260.0  10261.0\n",
      "18  client_3      0_2      5.0      5.0      4.0      4.0      4.0\n",
      "19  client_3      0_3      2.0      2.0      2.0      2.0      3.0\n",
      "20  client_3      0_4   3665.0   3664.0   3664.0   3664.0   3664.0\n",
      "21  client_3      0_5      4.0      4.0      5.0      5.0      5.0\n",
      "22  client_3      0_6    760.0    760.0    761.0    761.0    761.0\n",
      "23  client_3      0_7      1.0      1.0      1.0      1.0      0.0\n",
      "24  client_3      0_8  17830.0  17829.0  17829.0  17829.0  17829.0\n",
      "25  client_3      1_0  17399.0  17400.0  17400.0  17400.0  17399.0\n",
      "26  client_3      1_1   8883.0   8883.0   8883.0   8883.0   8884.0\n",
      "27  client_3      1_2   6834.0   6834.0   6834.0   6834.0   6833.0\n",
      "28  client_3      1_3   8883.0   8884.0   8884.0   8883.0   8883.0\n",
      "29  client_4      0_0      3.0      2.0      3.0      3.0      3.0\n",
      "30  client_4      0_1  13469.0  13469.0  13469.0  13469.0  13469.0\n",
      "31  client_4      0_2      1.0      1.0      0.0      0.0      1.0\n",
      "32  client_4      0_3   5873.0   5873.0   5874.0   5874.0   5874.0\n",
      "33  client_4      0_4      2.0      2.0      2.0      3.0      2.0\n",
      "34  client_4      0_5      0.0      1.0      1.0      1.0      1.0\n",
      "35  client_4      0_6   6153.0   6153.0   6152.0   6153.0   6153.0\n",
      "36  client_4      0_7  15979.0  15979.0  15979.0  15978.0  15978.0\n",
      "37  client_4      0_8    520.0    520.0    520.0    519.0    519.0\n",
      "38  client_4      1_0  10500.0  10500.0  10500.0  10500.0  10500.0\n",
      "39  client_4      1_1  10500.0  10500.0  10500.0  10500.0  10500.0\n",
      "40  client_4      1_2  10500.0  10500.0  10500.0  10500.0  10500.0\n",
      "41  client_4      1_3  10500.0  10500.0  10500.0  10500.0  10500.0\n"
     ]
    }
   ],
   "source": [
    "#print(merged_train_df.groupby(['client', 'fold', 'Stratify']).count())\n",
    "grouped_counts = merged_test_df.groupby(['client', 'fold', 'Stratify']).size().reset_index(name='count')\n",
    "# Pivoting the 'fold' column to make it into a separate column\n",
    "pivot_df = grouped_counts.pivot_table(index=['client', 'Stratify'], columns='fold', values='count', fill_value=0).reset_index()\n",
    "pivot_df.columns = ['Client', 'Stratify'] + [f'Fold_{int(col)}' for col in pivot_df.columns if isinstance(col, int)]\n",
    "pivot_df.to_csv(ensure_directory_exists(\"./dataset/dataset_summary_test.csv\"), index=False)\n",
    "print(pivot_df.to_string())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA dataset Preparation [Not in Use}\n",
    "- Choose component of 30, 33, 35 and generate datasets accordingly for training dataset and store locally\n",
    "- Use PCA matrix to convert test dataset and store locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iteration Friendly dictionary\n",
    "clients = {\n",
    " \n",
    "    'client_1': {\n",
    "        'scaled_train': './dataset/client_1/fold_{}/client_1_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_1/fold_{}/client_1_sclaed_test_dataset.csv', #unwanted features removed\n",
    "        'pca_path': './dataset/client_1/fold_{}/pca/client_1_pca.pkl', #Saved for later use\n",
    "        'pca_train': './dataset/client_1/fold_{}/pca/client_1_pca_train_dataset.csv', #unwanted features removed\n",
    "        'pca_test': './dataset/client_1/fold_{}/pca/client_1_pca_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_2': {\n",
    "        'scaled_train': './dataset/client_2/fold_{}/client_2_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_2/fold_{}/client_2_sclaed_test_dataset.csv', #unwanted features removed\n",
    "        'pca_path': './dataset/client_2/fold_{}/pca/client_2_pca.pkl', #Saved for later use\n",
    "        'pca_train': './dataset/client_2/fold_{}/pca/client_2_pca_train_dataset.csv', #unwanted features removed\n",
    "        'pca_test': './dataset/client_2/fold_{}/pca/client_2_pca_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_3': {\n",
    "        'scaled_train': './dataset/client_3/fold_{}/client_3_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_3/fold_{}/client_3_sclaed_test_dataset.csv', #unwanted features removed\n",
    "        'pca_path': './dataset/client_3/fold_{}/pca/client_3_pca.pkl', #Saved for later use\n",
    "        'pca_train': './dataset/client_3/fold_{}/pca/client_3_pca_train_dataset.csv', #unwanted features removed\n",
    "        'pca_test': './dataset/client_3/fold_{}/pca/client_3_pca_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    'client_4': {\n",
    "        'scaled_train': './dataset/client_4/fold_{}/client_4_scaled_train_dataset.csv', #unwanted features removed\n",
    "        'scaled_test': './dataset/client_4/fold_{}/client_4_sclaed_test_dataset.csv', #unwanted features removed\n",
    "        'pca_path': './dataset/client_4/fold_{}/pca/client_4_pca.pkl', #Saved for later use\n",
    "        'pca_train': './dataset/client_4/fold_{}/pca/client_4_pca_train_dataset.csv', #unwanted features removed\n",
    "        'pca_test': './dataset/client_4/fold_{}/pca/client_4_pca_test_dataset.csv', #unwanted features removed\n",
    "    },\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Function to perform PCA with a maximum number of components and save the PCA object\n",
    "def perform_pca_and_save_with_max(train_df, test_df, max_components, pca_path):\n",
    "    # Remove 'Label' column before applying PCA\n",
    "    train_labels = train_df['Label']\n",
    "    test_labels = test_df['Label']\n",
    "    train_features = train_df.drop(columns=['Label'])\n",
    "    test_features = test_df.drop(columns=['Label'])\n",
    "\n",
    "    # Initialize PCA with the maximum number of components\n",
    "    pca = PCA(n_components=max_components)\n",
    "    \n",
    "    # Fit PCA on the training set and transform\n",
    "    train_pca_full = pca.fit_transform(train_features)\n",
    "\n",
    "    ##Ensure direcoty exists. if not it will create one\n",
    "    ensure_directory_exists(pca_path)\n",
    "    \n",
    "    # Save the PCA model for future use\n",
    "    with open(pca_path, \"wb\") as f:\n",
    "        pickle.dump(pca, f)\n",
    "    \n",
    "    #print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    \n",
    "    # Transform the test set using the same PCA model\n",
    "    test_pca_full = pca.transform(test_features)\n",
    "    \n",
    "    return train_pca_full, test_pca_full, train_labels, test_labels\n",
    "\n",
    "# Step 2: Function to slice PCA components and add labels back to the dataset\n",
    "def add_labels_to_pca(train_pca_full, test_pca_full, train_labels, test_labels, num_components):\n",
    "    # Slice the top 'num_components' from the full PCA results\n",
    "    train_pca_reduced = train_pca_full[:, :num_components]\n",
    "    test_pca_reduced = test_pca_full[:, :num_components]\n",
    "    \n",
    "    # Convert to DataFrame for easier handling\n",
    "    train_pca_df = pd.DataFrame(train_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "    test_pca_df = pd.DataFrame(test_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "    \n",
    "    # Add the 'Label' column back\n",
    "    train_pca_df['Label'] = train_labels.values\n",
    "    test_pca_df['Label'] = test_labels.values\n",
    "    \n",
    "    return train_pca_df, test_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_pca_components = 40 # This number chosen based on pca analysis. check PCA.ipynb\n",
    "\n",
    "# for client, info in clients.items():\n",
    "#     print(\"----------------------------------------------------------\")\n",
    "#     for fold in range(1, 6):\n",
    "#         print(f'Client: {client} ... Fold: {fold}')\n",
    "#         train_df = pd.read_csv(info.get('scaled_train').format(fold))\n",
    "#         test_df = pd.read_csv(info.get('scaled_test').format(fold))\n",
    "    \n",
    "#         # Step 3: Perform PCA with the maximum number of components (e.g., 40)\n",
    "#         train_pca_full, test_pca_full, train_labels, test_labels = perform_pca_and_save_with_max(train_df, test_df, max_components=max_pca_components, pca_path=info.get(\"pca_path\").format(fold))      \n",
    "#         train_pca_data, test_pca_data = add_labels_to_pca(train_pca_full, test_pca_full, train_labels, test_labels, num_components=max_pca_components)\n",
    "        \n",
    "#         train_pca_data.to_csv(info.get('pca_train').format(fold),  index=False)\n",
    "#         test_pca_data.to_csv(info.get('pca_test').format(fold),  index=False)\n",
    "\n",
    "#     print(\"-----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not in USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Iteration Friendly dictionary\n",
    "# clients = {\n",
    " \n",
    "#     'client_1': {\n",
    "#         'scaled_train': './dataset/client_1/client_1_scaled_train_dataset.csv', #unwanted features removed\n",
    "#         'scaled_test': './dataset/client_1/client_1_sclaed_test_dataset.csv', #unwanted features removed\n",
    "#         'pca_path': './dataset/client_1/pca/client_1_pca.pkl', #Saved for later use\n",
    "#         'pca_train': './dataset/client_1/pca/client_1_pca_train_dataset.csv', #unwanted features removed\n",
    "#         'pca_test': './dataset/client_1/pca/client_1_pca_test_dataset.csv', #unwanted features removed\n",
    "#     },\n",
    "#     'client_2': {\n",
    "#         'scaled_train': './dataset/client_2/client_2_scaled_train_dataset.csv', #unwanted features removed\n",
    "#         'scaled_test': './dataset/client_2/client_2_sclaed_test_dataset.csv', #unwanted features removed\n",
    "#         'pca_path': './dataset/client_2/pca/client_2_pca.pkl', #Saved for later use\n",
    "#         'pca_train': './dataset/client_2/pca/client_2_pca_train_dataset.csv', #unwanted features removed\n",
    "#         'pca_test': './dataset/client_2/pca/client_2_pca_test_dataset.csv', #unwanted features removed\n",
    "#     },\n",
    "#     'client_3': {\n",
    "#         'scaled_train': './dataset/client_3/client_3_scaled_train_dataset.csv', #unwanted features removed\n",
    "#         'scaled_test': './dataset/client_3/client_3_sclaed_test_dataset.csv', #unwanted features removed\n",
    "#         'pca_path': './dataset/client_3/pca/client_3_pca.pkl', #Saved for later use\n",
    "#         'pca_train': './dataset/client_3/pca/client_3_pca_train_dataset.csv', #unwanted features removed\n",
    "#         'pca_test': './dataset/client_3/pca/client_3_pca_test_dataset.csv', #unwanted features removed\n",
    "#     },\n",
    "#     'client_4': {\n",
    "#         'scaled_train': './dataset/client_4/client_4_scaled_train_dataset.csv', #unwanted features removed\n",
    "#         'scaled_test': './dataset/client_4/client_4_sclaed_test_dataset.csv', #unwanted features removed\n",
    "#         'pca_path': './dataset/client_4/pca/client_4_pca.pkl', #Saved for later use\n",
    "#         'pca_train': './dataset/client_4/pca/client_4_pca_train_dataset.csv', #unwanted features removed\n",
    "#         'pca_test': './dataset/client_4/pca/client_4_pca_test_dataset.csv', #unwanted features removed\n",
    "#     },\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Function to perform PCA with a maximum number of components and save the PCA object\n",
    "# def perform_pca_and_save_with_max(train_df, test_df, max_components, pca_path):\n",
    "#     # Remove 'Label' column before applying PCA\n",
    "#     train_labels = train_df['Label']\n",
    "#     test_labels = test_df['Label']\n",
    "#     train_features = train_df.drop(columns=['Label'])\n",
    "#     test_features = test_df.drop(columns=['Label'])\n",
    "\n",
    "#     # Initialize PCA with the maximum number of components\n",
    "#     pca = PCA(n_components=max_components)\n",
    "    \n",
    "#     # Fit PCA on the training set and transform\n",
    "#     train_pca_full = pca.fit_transform(train_features)\n",
    "    \n",
    "#     # Save the PCA model for future use\n",
    "#     with open(pca_path, \"wb\") as f:\n",
    "#         pickle.dump(pca, f)\n",
    "    \n",
    "#     print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    \n",
    "#     # Transform the test set using the same PCA model\n",
    "#     test_pca_full = pca.transform(test_features)\n",
    "    \n",
    "#     return train_pca_full, test_pca_full, train_labels, test_labels\n",
    "\n",
    "# # Step 2: Function to slice PCA components and add labels back to the dataset\n",
    "# def add_labels_to_pca(train_pca_full, test_pca_full, train_labels, test_labels, num_components):\n",
    "#     # Slice the top 'num_components' from the full PCA results\n",
    "#     train_pca_reduced = train_pca_full[:, :num_components]\n",
    "#     test_pca_reduced = test_pca_full[:, :num_components]\n",
    "    \n",
    "#     # Convert to DataFrame for easier handling\n",
    "#     train_pca_df = pd.DataFrame(train_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "#     test_pca_df = pd.DataFrame(test_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components)])\n",
    "    \n",
    "#     # Add the 'Label' column back\n",
    "#     train_pca_df['Label'] = train_labels.values\n",
    "#     test_pca_df['Label'] = test_labels.values\n",
    "    \n",
    "#     return train_pca_df, test_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_configurations = [\n",
    "    {\n",
    "        'n_components': 40,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "    {\n",
    "        'n_components': 35,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "    {\n",
    "        'n_components': 33,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "    {\n",
    "        'n_components': 30,\n",
    "        'train_path': \"./dataset/{}/pca/components_{}/pca_{}_train.csv\",\n",
    "        'test_path': \"./dataset/{}/pca/components_{}/pca_{}_test.csv\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for client, info in clients.items():\n",
    "#     print(\"f{client} Starting....\")\n",
    "#     train_df = pd.read_csv(info.get('scaled_train'))\n",
    "#     test_df = pd.read_csv(info.get('scaled_test'))\n",
    "#     # Step 3: Perform PCA with the maximum number of components (e.g., 40)\n",
    "#     train_pca_full, test_pca_full, train_labels, test_labels = perform_pca_and_save_with_max(train_df, test_df, max_components=40, pca_path=info.get(\"pca_path\"))\n",
    "    \n",
    "    \n",
    "#     # Step 4: Slice the PCA components and add labels back (e.g., for 40, 35, 33, 30 components)\n",
    "#     for p in pca_configurations:        \n",
    "#         train_pca_data, test_pca_data = add_labels_to_pca(train_pca_full, test_pca_full, train_labels, test_labels, num_components=p.get('n_components'))\n",
    "#         train_pca_data.to_csv(p.get('train_path').format(client, p.get('n_components'), p.get('n_components')),  index=False)\n",
    "#         test_pca_data.to_csv(p.get('test_path').format(client, p.get('n_components'), p.get('n_components')),  index=False)\n",
    "#         print(train_pca_data.shape)\n",
    "#     print(\"-----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # This is just the full version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
