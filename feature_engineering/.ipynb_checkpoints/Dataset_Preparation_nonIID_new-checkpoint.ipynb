{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['Flow Duration' ,'Total Fwd Packet' ,'Total Bwd packets' ,'Total Length of Fwd Packet' ,'Total Length of Bwd Packet' ,'Fwd Packet Length Max' ,'Fwd Packet Length Min' ,'Fwd Packet Length Std' ,'Bwd Packet Length Max' ,'Bwd Packet Length Min' ,'Bwd Packet Length Std' ,'Flow IAT Mean' ,'Flow IAT Std' ,'Flow IAT Max' ,'Flow IAT Min' ,'Fwd IAT Total' ,'Fwd IAT Std' ,'Fwd IAT Max' ,'Fwd IAT Min' ,'Bwd IAT Total' ,'Bwd IAT Std' ,'Bwd IAT Max' ,'Bwd IAT Min' ,'Fwd Header Length' ,'Bwd Header Length' ,'Packet Length Min' ,'Packet Length Max' ,'Packet Length Std' ,'Packet Length Variance' ,'FWD Init Win Bytes' ,'Bwd Init Win Bytes' ,'Fwd Act Data Pkts' ,'Fwd Seg Size Min' ,'Active Mean' ,'Active Std' ,'Active Max' ,'Active Min' ,'Idle Mean' ,'Idle Std' ,'Idle Max' ,'Idle Min', 'Label', 'Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_data_frame(dataframe, remove_infinity=True, remove_null=True):\n",
    "    \"\"\"\n",
    "    Cleans a given DataFrame by removing infinite and/or null values without removing columns.\n",
    "    \"\"\"\n",
    "\n",
    "    original_size = dataframe.shape[0]\n",
    "\n",
    "    print(\"Before sanitization\")\n",
    "    print(dataframe.shape)\n",
    "\n",
    "    # Remove infinite values (keep all columns)\n",
    "    if remove_infinity:\n",
    "        numeric_cols = dataframe.select_dtypes(include=[np.number]).columns\n",
    "        dataframe[numeric_cols] = dataframe[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Remove null values (drop rows with NaN but keep columns)\n",
    "    if remove_null:\n",
    "        dataframe = dataframe.dropna()\n",
    "\n",
    "    sanitized_size = dataframe.shape[0]\n",
    "\n",
    "    # print(f\"Original Row Count: {original_size}\")\n",
    "    # print(f\"Sanitized Row Count: {sanitized_size}\")\n",
    "    # print(f\"Rows Removed: {original_size - sanitized_size}\")\n",
    "\n",
    "    # print(\"After sanitization\")\n",
    "    #print(dataframe.shape)  # ðŸ”¹ Now, column count should remain unchanged\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "def read_all_csv_files(directory_path, features=None):\n",
    "    # Initialize an empty list to store dataframes from CSV files\n",
    "    dataframes = []\n",
    "    # Get a list of all files in the directory\n",
    "    file_list = os.listdir(directory_path)\n",
    "\n",
    "    # Loop through each file and check if it's a CSV file\n",
    "    for file_number, file_name in enumerate(file_list):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.columns = df.columns.str.strip()\n",
    "            df['Type'] = f\"benign_{file_number}\"\n",
    "            df['Label'] = 0\n",
    "            dataframes.append(df[FEATURES]) #as it is feature analysis, we are taking all the features\n",
    "\n",
    "    # Merge all DataFrames into a single DataFrame\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def read_attack_csv_files(attacks, path):\n",
    "    dataframes = []\n",
    "    for attack_type, attack in attacks.items():        \n",
    "        file_path = os.path.join(path, attack)      \n",
    "        df = pd.read_csv(file_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        df['Type'] = attack_type\n",
    "        df['Label'] = 1\n",
    "        #print(attack_type)\n",
    "        #print(df.shape)\n",
    "        dataframes.append(df[FEATURES])\n",
    "\n",
    "   # Merge all DataFrames into a single DataFrame\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    #print(merged_df.shape)\n",
    "    return merged_df\n",
    "\n",
    "##Sample recrods\n",
    "def sample_df(df, stratify, train_size, random_state, client, category):\n",
    "    \"\"\"\n",
    "    Samples a dataframe while maintaining stratification and ensuring randomization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure `train_size` does not exceed available data\n",
    "    train_size = min(train_size, df.shape[0])\n",
    "\n",
    "    # If train_size == df.shape[0], return shuffled df directly (to ensure randomness)\n",
    "    if train_size == df.shape[0]:\n",
    "        sampled_df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    else:\n",
    "        sampled_df, _ = train_test_split(\n",
    "            df, train_size=int(train_size), stratify=df[stratify], random_state=random_state\n",
    "        )\n",
    "\n",
    "    # Save the sampled dataset\n",
    "    sampled_df.to_csv(ensure_directory_exists(client['save_path'].format(category)), index=False)\n",
    "    \n",
    "    split_dataframe(sampled_df, split=5, client=client, category=category)\n",
    "\n",
    "    return sampled_df\n",
    "\n",
    "\n",
    "def split_dataframe(df, split, client, category):\n",
    "  \n",
    "    # Split into 5 parts\n",
    "    split_dfs = np.array_split(df, 5)\n",
    "\n",
    "    # Save each split as a separate CSV file\n",
    "    for i, split_df in enumerate(split_dfs):\n",
    "        path = client['split_path'].format(i+1, category)\n",
    "        split_df.to_csv(ensure_directory_exists(path), index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "# THis will create subdirectoy as well\n",
    "def ensure_directory_exists(filepath):\n",
    "    directory = os.path.dirname(filepath)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def shuffle_dataframe_stratified(df, stratify_col, random_state=42):\n",
    "    \"\"\"\n",
    "    Shuffles a DataFrame while maintaining stratification on a given column.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to shuffle.\n",
    "    - stratify_col (str): Column name to stratify by.\n",
    "    - random_state (int): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Shuffled and stratified DataFrame.\n",
    "    \"\"\"\n",
    "    # Use train_test_split with train_size=1.0 to shuffle while keeping stratification\n",
    "    # shuffled_df, _ = train_test_split(\n",
    "    #     df, train_size=None, stratify=df[stratify_col], random_state=random_state\n",
    "    # )\n",
    "\n",
    "    # Step 1: Shuffle inside each group\n",
    "    shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
    "        lambda x: x.sample(frac=1, random_state=random_state)\n",
    "    )\n",
    "\n",
    "    # Step 2: Shuffle the entire dataset\n",
    "    shuffled_df = shuffled_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    return shuffled_df\n",
    "\n",
    "\n",
    "def scale_data(train_df, test_df, feature_columns, label_column, extra_column):\n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the scaler on the training data only (excluding label and extra column)\n",
    "    train_scaled_values = scaler.fit_transform(train_df[feature_columns])\n",
    "\n",
    "    # Transform the test data using the same scaler\n",
    "    test_scaled_values = scaler.transform(test_df[feature_columns])\n",
    "\n",
    "    # Store the scaled values back into DataFrames\n",
    "    train_scaled = train_df.copy()\n",
    "    test_scaled = test_df.copy()\n",
    "\n",
    "    train_scaled[feature_columns] = train_scaled_values\n",
    "    test_scaled[feature_columns] = test_scaled_values\n",
    "\n",
    "    # Ensure label and extra column are preserved without modifications\n",
    "    train_scaled[label_column] = train_df[label_column]\n",
    "    test_scaled[label_column] = test_df[label_column]\n",
    "\n",
    "    train_scaled[extra_column] = train_df[extra_column]\n",
    "    test_scaled[extra_column] = test_df[extra_column]\n",
    "\n",
    "    # # Save the scaler to a file for future use\n",
    "    # with open(scaler_path, \"wb\") as scaler_file:\n",
    "    #     pickle.dump(scaler, scaler_file)\n",
    "\n",
    "    # print(f\"Scaler saved at: {scaler_path}\")\n",
    "\n",
    "    return train_scaled, test_scaled, scaler\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Variables for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data extracted from the file structure\n",
    "file_structure = {\n",
    "    \"client_1\": {\n",
    "        \"syn\": \"tcp_syn.pcap_Flow.csv\",\n",
    "        \"fin\": \"tcp_fin.pcap_Flow.csv\",\n",
    "        \"ack\": \"tcp_ack.pcap_Flow.csv\",\n",
    "        \"xmas\": \"tcp_xmas.pcap_Flow.csv\",\n",
    "        \"ymas\": \"tcp_ymas.pcap_Flow.csv\",\n",
    "        \"push\": \"tcp_push.pcap_Flow.csv\",\n",
    "        \"urg\": \"tcp_urg.pcap_Flow.csv\",\n",
    "        \"udp\": \"udp_flood.pcap_Flow.csv\"\n",
    "    },\n",
    "    \"client_2\": {\n",
    "        \"syn\": \"upf2_tcp_syn.pcap_Flow.csv\",\n",
    "        \"fin\": \"upf2_tcp_fin.pcap_Flow.csv\",\n",
    "        \"ack\": \"upf2_tcp_ack.pcap_Flow.csv\",\n",
    "        \"xmas\": \"upf2_tcp_xmas.pcap_Flow.csv\",\n",
    "        \"ymas\": \"upf2_tcp_ymas.pcap_Flow.csv\",\n",
    "        \"push\": \"upf2_tcp_push.pcap_Flow.csv\",\n",
    "        \"urg\": \"upf2_tcp_urg.pcap_Flow.csv\",\n",
    "        \"udp\": \"upf2_upd_flood.pcap_Flow.csv\"\n",
    "    },\n",
    "    \"client_3\": {\n",
    "        \"syn\": \"syn1.pcap_Flow.csv\",\n",
    "        \"fin\": \"fin1.pcap_Flow.csv\",\n",
    "        \"ack\": \"ack1.pcap_Flow.csv\",\n",
    "        \"xmas\": \"xmas1.pcap_Flow.csv\",\n",
    "        \"ymas\": \"ymas1.pcap_Flow.csv\",\n",
    "        \"push\": \"push1.pcap_Flow.csv\",\n",
    "        \"urg\": \"urg1.pcap_Flow.csv\",\n",
    "        \"udp\": \"12udpfloodupf1.pcap_Flow.csv\"\n",
    "    },\n",
    "    \"client_4\": {\n",
    "        \"syn\": \"syn2.pcap_Flow.csv\",\n",
    "        \"fin\": \"fin2.pcap_Flow.csv\",\n",
    "        \"ack\": \"ack2.pcap_Flow.csv\",\n",
    "        \"xmas\": \"xmas2.pcap_Flow.csv\",\n",
    "        \"ymas\": \"ymas2.pcap_Flow.csv\",\n",
    "        \"push\": \"push2.pcap_Flow.csv\",\n",
    "        \"urg\": \"urg2.pcap_Flow.csv\",\n",
    "        \"udp\": \"12udpfloodupf2.pcap_Flow.csv\"\n",
    "    },\n",
    "}\n",
    "\n",
    "#all dataset sources to make iterate to read csv files\n",
    "dataset_sources = {\n",
    " \n",
    "    'client_1': {\n",
    "        \"path\": {\n",
    "            'benign': '../row_data_IID/{}/benign',\n",
    "            'attack': '../row_data_IID/{}/attack',\n",
    "        },\n",
    "        'known_attacks': ['fin', 'udp'],\n",
    "        'save_path': \"./dataset_nonIID/client_1/client_1_{}.csv\",\n",
    "        'split_path': \"./dataset_nonIID/client_1/split_{0}/client_1_{1}_{0}.csv\"\n",
    "    },\n",
    "    'client_2': {\n",
    "         \"path\": {\n",
    "            'benign': '../row_data_IID/{}/benign',\n",
    "            'attack': '../row_data_IID/{}/attack',\n",
    "        },\n",
    "        'known_attacks': ['ack', 'push'],\n",
    "        'save_path': \"./dataset_nonIID/client_2/client_2_{}.csv\",\n",
    "        'split_path': \"./dataset_nonIID/client_2/split_{0}/client_2_{1}_{0}.csv\"\n",
    "    },\n",
    "    'client_3': {\n",
    "         \"path\": {\n",
    "            'benign': '../row_data_IID/{}/benign',\n",
    "            'attack': '../row_data_IID/{}/attack',\n",
    "        },\n",
    "        'known_attacks': ['syn', 'xmas'],\n",
    "        'save_path': \"./dataset_nonIID/client_3/client_3_{}.csv\",\n",
    "        'split_path': \"./dataset_nonIID/client_3/split_{0}/client_3_{1}_{0}.csv\"\n",
    "    },\n",
    "    'client_4': {\n",
    "         \"path\": {\n",
    "            'benign': '../row_data_IID/{}/benign',\n",
    "            'attack': '../row_data_IID/{}/attack',\n",
    "        },\n",
    "        'known_attacks': ['urg', 'ymas'],\n",
    "        'save_path': \"./dataset_nonIID/client_4/client_4_{}.csv\",\n",
    "        'split_path': \"./dataset_nonIID/client_4/split_{0}/client_4_{1}_{0}.csv\"\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "SEED = 42\n",
    "TOTAL_RECORDS = 400000\n",
    "CLASS_RATIO = 0.5 #50% Benign and 50% Attack\n",
    "SPLIT = 5\n",
    "PER_ATTACK_RATIO_IN_TRAINSET = 0.25\n",
    "PER_ATTACK_RATIO_IN_TESTSET = 0.06"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                   | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before sanitization\n",
      "(202821, 43)\n",
      "Before sanitization\n",
      "(2409383, 43)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                            | 1/4 [00:36<01:49, 36.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before sanitization\n",
      "(185943, 43)\n",
      "Before sanitization\n",
      "(2408730, 43)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                             | 2/4 [01:10<01:10, 35.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before sanitization\n",
      "(223200, 43)\n",
      "Before sanitization\n",
      "(2335440, 43)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                              | 3/4 [01:46<00:35, 35.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before sanitization\n",
      "(246797, 43)\n",
      "Before sanitization\n",
      "(2232941, 43)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/home/sharedrive/PythonCodes/.venv311_new/lib/python3.11/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:21<00:00, 35.28s/it]\n"
     ]
    }
   ],
   "source": [
    "for client, data in tqdm(dataset_sources.items(), total=len(dataset_sources)):\n",
    "    known_attacks = dataset_sources[client]['known_attacks']\n",
    "\n",
    "    benign_df = sanitize_data_frame(read_all_csv_files(data.get('path').get('benign').format(client)))\n",
    "    attack_df = sanitize_data_frame(read_attack_csv_files(file_structure.get(client), path=data.get('path').get('attack').format(client)))\n",
    "   \n",
    "\n",
    "    known_attack_df = attack_df[attack_df['Type'].isin(known_attacks)]\n",
    "    #benign_records = attack_records = TOTAL_RECORDS * CLASS_RATIO\n",
    "\n",
    "    benign_available = benign_df.shape[0]\n",
    "    attack_available = known_attack_df.shape[0]\n",
    "\n",
    "    benign_target = int(TOTAL_RECORDS * CLASS_RATIO)  # Ideally 200,000 from benign\n",
    "    attack_target = int(TOTAL_RECORDS * CLASS_RATIO)  # Ideally 200,000 from attack\n",
    "\n",
    "    # Case 1: Both have enough data\n",
    "    if benign_available >= benign_target and attack_available >= attack_target:\n",
    "        benign_train_size = benign_target\n",
    "        attack_train_size = attack_target\n",
    "\n",
    "    # Case 2: Not enough benign, compensate with attack\n",
    "    elif benign_available < benign_target and attack_available >= attack_target:\n",
    "        benign_train_size = benign_available  # Take all available benign\n",
    "        attack_train_size = TOTAL_RECORDS - benign_train_size  # Fill remaining from attack\n",
    "\n",
    "    # Case 3: Not enough attack, compensate with benign\n",
    "    elif benign_available >= benign_target and attack_available < attack_target:\n",
    "        attack_train_size = attack_available  # Take all available attack\n",
    "        benign_train_size = TOTAL_RECORDS - attack_train_size  # Fill remaining from benign\n",
    "\n",
    "    # Case 4: Neither has enough data, take all available\n",
    "    else:\n",
    "        benign_train_size = benign_available\n",
    "        attack_train_size = attack_available\n",
    "        # Ensure total is still 400,000 (if both are lower, it will be less than 400,000)\n",
    "\n",
    "\n",
    "\n",
    "    benign_df = sample_df(\n",
    "        benign_df, stratify=\"Type\", train_size=benign_train_size, random_state=SEED, \n",
    "        client = dataset_sources[client],\n",
    "        category = \"benign\"\n",
    "    )\n",
    "    \n",
    "    known_attack_df = sample_df(\n",
    "        attack_df[attack_df['Type'].isin(known_attacks)], stratify=\"Type\",\n",
    "        train_size=TOTAL_RECORDS * CLASS_RATIO, random_state=SEED,\n",
    "        client = dataset_sources[client],\n",
    "        category = \"known_attack\"\n",
    "    )\n",
    "    \n",
    "    unknow_recoreds = TOTAL_RECORDS * CLASS_RATIO * 6/8\n",
    "    unknown_attack_df = sample_df(\n",
    "        attack_df[~attack_df['Type'].isin(known_attacks)], stratify=\"Type\", \n",
    "        train_size=unknow_recoreds, random_state=SEED,    \n",
    "        client = dataset_sources[client],\n",
    "        category = \"unknown_attacks\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_splits = './dataset_nonIID/client_{0}/split_{1}/client_{0}_benign_{1}.csv'\n",
    "known_attack_splits = './dataset_nonIID/client_{0}/split_{1}/client_{0}_known_attack_{1}.csv'\n",
    "unknown_attack_splits = './dataset_nonIID/client_{0}/split_{1}/client_{0}_unknown_attacks_{1}.csv'\n",
    "fold_path = './dataset_nonIID/client_{0}/fold_{1}/client_{0}_{2}_dataset.csv'\n",
    "scaler_path = './dataset_nonIID/client_{0}/fold_{1}/client_{0}_train_scaler.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
      "/tmp/ipykernel_4124047/494536845.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = df.groupby(stratify_col, group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "#features = ['Flow Duration' ,'Total Fwd Packet' ,'Total Bwd packets' ,'Total Length of Fwd Packet' ,'Total Length of Bwd Packet' ,'Fwd Packet Length Max' ,'Fwd Packet Length Min' ,'Fwd Packet Length Std' ,'Bwd Packet Length Max' ,'Bwd Packet Length Min' ,'Bwd Packet Length Std' ,'Flow IAT Mean' ,'Flow IAT Std' ,'Flow IAT Max' ,'Flow IAT Min' ,'Fwd IAT Total' ,'Fwd IAT Std' ,'Fwd IAT Max' ,'Fwd IAT Min' ,'Bwd IAT Total' ,'Bwd IAT Std' ,'Bwd IAT Max' ,'Bwd IAT Min' ,'Fwd Header Length' ,'Bwd Header Length' ,'Packet Length Min' ,'Packet Length Max' ,'Packet Length Std' ,'Packet Length Variance' ,'FWD Init Win Bytes' ,'Bwd Init Win Bytes' ,'Fwd Act Data Pkts' ,'Fwd Seg Size Min' ,'Active Mean' ,'Active Std' ,'Active Max' ,'Active Min' ,'Idle Mean' ,'Idle Std' ,'Idle Max' ,'Idle Min', 'Label', 'Type']\n",
    "numeric_features = FEATURES.copy()\n",
    "numeric_features.remove('Label')\n",
    "numeric_features.remove('Type')\n",
    "\n",
    "for client in range(1, 5):\n",
    "    benign= []\n",
    "    known = []\n",
    "    unknown = []\n",
    "    for split in range(1, 6):\n",
    "        benign.append(pd.read_csv(benign_splits.format(client, split)))\n",
    "        known.append(pd.read_csv(known_attack_splits.format(client, split)))\n",
    "        unknown.append(pd.read_csv(unknown_attack_splits.format(client, split)))\n",
    "    \n",
    "    for i in range(5):\n",
    "\n",
    "        #prepare Testset\n",
    "        benign_test, known_test, unknown_test = benign[i], known[i], unknown[i]\n",
    "        test_df = pd.concat([benign_test, known_test, unknown_test], ignore_index=True)\n",
    "\n",
    "        #preparing Training set\n",
    "        benign_train = pd.concat([benign[j] for j in range(5) if j != i], ignore_index=True)\n",
    "        known_train = pd.concat([known[j] for j in range(5) if j != i], ignore_index=True)\n",
    "        train_df = pd.concat([benign_train, known_train], ignore_index=True)\n",
    "\n",
    "        ##Shuffling\n",
    "        train_df = shuffle_dataframe_stratified(train_df, stratify_col=\"Type\", random_state=SEED)\n",
    "        test_df = shuffle_dataframe_stratified(test_df, stratify_col=\"Type\", random_state=SEED)\n",
    "\n",
    "        ##Scaling\n",
    "        #scale_data(train_df, test_df, feature_columns, label_column, extra_column):\n",
    "        sclaed_train_df, scaled_test_df, scaler = scale_data(train_df, test_df, numeric_features, 'Label', 'Type')\n",
    "        #fold_path = './dataset_nonIID/client_{0}/fold_{1}/client_{0}_{2}_dataset.csv'\n",
    "\n",
    "        sclaed_train_df.to_csv(ensure_directory_exists(fold_path.format(client, i+1, 'scaled_train')), index=False)\n",
    "        scaled_test_df.to_csv(ensure_directory_exists(fold_path.format(client, i+1, 'scaled_test')), index=False)\n",
    "        #scaler_path = './dataset_nonIID/client_{0}/fold_{1}/client_{0}_train_scaler.pkl'\n",
    "        # Ensure the directory exists before saving the file\n",
    "        path = ensure_directory_exists(scaler_path.format(client, i+1))\n",
    "        with open(path, \"wb\") as scaler_file:\n",
    "            pickle.dump(scaler, scaler_file)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
