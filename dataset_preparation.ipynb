{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_csv_files(directory_path, features):\n",
    "    # Initialize an empty list to store dataframes from CSV files\n",
    "    dataframes = []\n",
    "\n",
    "    # Get a list of all files in the directory\n",
    "    file_list = os.listdir(directory_path)\n",
    "\n",
    "    # Loop through each file and check if it's a CSV file\n",
    "    for file_number, file_name in enumerate(file_list):\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Get the full file path\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            # Read the CSV file into a pandas DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Remove leading and trailing spaces from column names\n",
    "            df.columns = df.columns.str.strip()\n",
    "            # Append the DataFrame to the list\n",
    "            df['CSV_File_Number'] = file_number\n",
    "            dataframes.append(df[features])\n",
    "\n",
    "    # Merge all DataFrames into a single DataFrame\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_data_frames_updated(dataframe, remove_infinity=True, remove_null=True):\n",
    "\n",
    "    if remove_infinity:\n",
    "        numeric_cols = dataframe.select_dtypes(include=[np.number]).columns\n",
    "        infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n",
    "        for col, count in infinite_counts.items():\n",
    "            if count != 0:\n",
    "                dataframe = dataframe[~np.isinf(dataframe[col])]\n",
    "\n",
    "    if remove_null:\n",
    "        null_counts = dataframe.isnull().sum()\n",
    "        for col, count in null_counts.items():\n",
    "            if count != 0:\n",
    "                    dataframe = dataframe.dropna(subset=[col])\n",
    "    print(\"Sanitized Row Count:\", dataframe.shape[0])    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all dataset sources to make iterate to read csv files\n",
    "dataset_sources = {\n",
    "\n",
    "    ## Benign Traffic      \n",
    "    'client_1': {\n",
    "        'benign': './row_data/client_1/benign',\n",
    "        'attack': './row_data/client_1/attack',\n",
    "    },\n",
    "    'client_2': {\n",
    "        'benign': './row_data/client_2/benign',\n",
    "        'attack': './row_data/client_2/attack',\n",
    "    },\n",
    "    'client_3': {\n",
    "        'benign': './row_data/client_3/benign',\n",
    "        'attack': './row_data/client_3/attack',\n",
    "    },\n",
    "    'client_4': {\n",
    "        'benign': './row_data/client_4/benign',\n",
    "        'attack': './row_data/client_4/attack',\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##\n",
    "# def get_uniform_sample(df, group_col, sample_size):\n",
    "#       # Shuffle the data within each label based on File Number/Index\n",
    "#     df = df.groupby(group_col).apply(lambda x: x.sample(frac=1)).reset_index(drop=True)\n",
    "\n",
    "#     # Randomize the whole data again\n",
    "#     df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#     # Grab a sample ensuring uniform distribution of labels and File Number/Index\n",
    "#     min_count = min(df['Label'].value_counts().min(), sample_size // 2)\n",
    "#     sample_df = pd.concat([df[df['Label'] == 0].groupby('CSV_File_Number').apply(lambda x: x.sample(min_count // len(x['CSV_File_Number'].unique()), random_state=42)).reset_index(drop=True),\n",
    "#                            [df['Label'] == 1].groupby('CSV_File_Number').apply(lambda x: x.sample(min_count // len(x['CSV_File_Number'].unique()), random_state=42)).reset_index(drop=True)])\n",
    "    \n",
    "#     return sample_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "def get_uniform_sample(df, group_col, sample_size):\n",
    "    # Determine the minimum count available in any label group\n",
    "    min_count = min(df['Label'].value_counts().min(), sample_size // 2)\n",
    "\n",
    "    # Sample from each group while respecting the available counts\n",
    "    sampled_dfs = []\n",
    "    for label in df['Label'].unique():\n",
    "        group = df[df['Label'] == label]\n",
    "        sampled_dfs.append(\n",
    "            group.groupby('CSV_File_Number').apply(\n",
    "                lambda x: x.sample(\n",
    "                    min(min_count // len(group['CSV_File_Number'].unique()), len(x)),\n",
    "                    random_state=42\n",
    "                )\n",
    "            ).reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "    sample_df = pd.concat(sampled_dfs, ignore_index=True)\n",
    "    \n",
    "    return sample_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 21426.84it/s]\n",
      "  0%|                                                                                                                       | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_1, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167871/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 202821\n",
      " Loading....Clinet = client_1, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167871/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 2671528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167871/725653109.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group.groupby('CSV_File_Number').apply(\n",
      "/tmp/ipykernel_167871/725653109.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group.groupby('CSV_File_Number').apply(\n",
      " 25%|███████████████████████████▊                                                                                   | 1/4 [01:33<04:39, 93.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_2, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167871/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 185943\n",
      " Loading....Clinet = client_2, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167871/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 3346296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167871/725653109.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group.groupby('CSV_File_Number').apply(\n",
      "/tmp/ipykernel_167871/725653109.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group.groupby('CSV_File_Number').apply(\n",
      " 50%|███████████████████████████████████████████████████████                                                       | 2/4 [03:21<03:24, 102.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_3, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167871/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 223200\n",
      " Loading....Clinet = client_3, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167871/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 2348465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167871/725653109.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group.groupby('CSV_File_Number').apply(\n",
      "/tmp/ipykernel_167871/725653109.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group.groupby('CSV_File_Number').apply(\n",
      " 75%|███████████████████████████████████████████████████████████████████████████████████▎                           | 3/4 [04:43<01:32, 92.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading....Clinet = client_4, type = benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167871/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 246797\n",
      " Loading....Clinet = client_4, type = attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167871/218912892.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  infinite_counts = dataframe[numeric_cols].applymap(np.isinf).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Row Count: 2253978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167871/725653109.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group.groupby('CSV_File_Number').apply(\n",
      "/tmp/ipykernel_167871/725653109.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group.groupby('CSV_File_Number').apply(\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [06:01<00:00, 90.43s/it]\n"
     ]
    }
   ],
   "source": [
    "##Reading all the data\n",
    "features = ['Bwd IAT Std', 'Bwd IAT Max', 'Fwd Packet Length Std', 'Bwd IAT Total', 'Fwd Packet Length Min', 'PSH Flag Count',\n",
    "            'Packet Length Variance', 'Fwd Packet Length Max', 'Down/Up Ratio', 'Bwd IAT Mean', 'FIN Flag Count', 'Packet Length Min', \n",
    "            'Active Std', 'Bwd Packet Length Min', 'Bwd IAT Min', 'FWD Init Win Bytes', 'URG Flag Count', 'Fwd IAT Total', 'Fwd IAT Std', \n",
    "            'ACK Flag Count', 'Flow IAT Mean', 'Flow IAT Min', 'Active Min', 'Bwd Packet Length Std', 'Packet Length Max', 'Active Mean', \n",
    "            'Bwd Packet Length Max', 'Idle Std', 'Active Max', 'Flow IAT Max', 'Label', 'CSV_File_Number']\n",
    "\n",
    "\n",
    "for client, data in tqdm(dataset_sources.items(), total=len(dataset_sources)):\n",
    "    locals()[client] = {}\n",
    "\n",
    "for client, data in tqdm(dataset_sources.items(), total=len(dataset_sources)):\n",
    "    client_dataframe = []\n",
    "    for type, path in data.items():\n",
    "        print(f' Loading....Clinet = {client}, type = {type}')\n",
    "        dataframe = read_all_csv_files(path, features)\n",
    "        dataframe = sanitize_data_frames_updated(dataframe)\n",
    "        if type == 'benign':\n",
    "            dataframe['Label'] = 0\n",
    "        else:\n",
    "             dataframe['Label'] = 1\n",
    "        client_dataframe.append(dataframe)\n",
    "        #locals()[client][type] = dataframe\n",
    "    client_merged_df = get_uniform_sample(pd.concat(client_dataframe, ignore_index=True), ['Label', 'CSV_File_Number'], 420000 )\n",
    "    client_merged_df.to_csv(f'./dataset/{client}/dataset.csv', index=False)\n",
    "\n",
    "    #Remove unwanted column\n",
    "    client_merged_df =  client_merged_df.drop(['CSV_File_Number'], axis=1)\n",
    "    train_size = 0.8  # 80% for training, 20% for testing\n",
    "    train_df, test_df = train_test_split(client_merged_df, train_size=train_size, random_state=42, stratify=client_merged_df['Label'])\n",
    "    train_df.to_csv(f'./dataset/{client}/train/{client}_train.csv', index=False)\n",
    "    test_df.to_csv(f'./dataset/{client}/test/{client}_test.csv', index=False)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
